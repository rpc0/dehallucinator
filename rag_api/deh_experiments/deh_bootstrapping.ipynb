{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for deh experiments specific modules\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "utils_folder = Path(\".\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import deh_globals\n",
    "globals().update(deh_globals.__dict__)\n",
    "import squad_scoring\n",
    "globals().update(deh_globals.__dict__)\n",
    "import deh_prompts\n",
    "import deh_vector_store\n",
    "globals().update(deh_vector_store.__dict__)\n",
    "import deh_squad_data\n",
    "import deh_hyde\n",
    "import deh_experiments_config\n",
    "globals().update(deh_experiments_config.__dict__)\n",
    "import deh_llm\n",
    "globals().update(deh_llm.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Current Experiment\n",
    "\n",
    "Currently, the notebook runs one single  experiment that is selected in the next cell. However, it can easily be adapted to run more than one experiment. All experiments are defined and configured in the module \"deh_experiments_config.py\". Furthermore, in the next cell a series of global parameters are set. These depend on the choice of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment = experiments[\"FINAL_FINAL_RAG_TR_90\"]\n",
    "\n",
    "EXPERIMENT_GLOBAL_ID = current_experiment[\"name\"]\n",
    "DEFAULT_CHUNKING_METHOD = current_experiment[\"chunking_method\"]\n",
    "CHAT_MODEL_NAME = current_experiment[\"llm_model\"]\n",
    "TEMPERATURE = current_experiment[\"temperature\"]\n",
    "VECTOR_STORE_TOP_K = current_experiment[\"vector_store_top_k\"]\n",
    "JUDGES_SUPPRESS_THRESHOLD = current_experiment[\"judges_suppress_threshold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading SQuAD data\n",
    "\n",
    "Next, the SQuAD dataset set (Version v2.0, dev set) is loaded in raw format from a csv file into a dataset. Loading the dataset in its raw format is useful for some of the functions of the notebook. After reading in the raw dataset, several dataframes are created to show statistics about the dataframe.\n",
    "\n",
    "Please note the following two important points:\n",
    "\n",
    "- throughout this notebook the term \"title\" and \"article\" are used as synonyms\n",
    "- the SQuAD dataset contains text passages from Wikipedia articles. Each such segment is called \"context\" and belongs to one of the articles. This is a source of confusion since the items retrieved from a vector store as the result of a similarity search are also called \"contexts\". Wherever possible, we will try to distinguish these two different meanings of the word \"context\" (so ulimtately, the meaning of the word \"context\", depends on the context it is used in :-) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading squad data...\n",
      "Loaded squad data from ../../../deh_data_results/data/squad_raw.csv\n",
      "\n",
      "Number of raw entries in squad_raw:   26232\n",
      "Number of unique titles:                 35\n",
      "Number of unique contexts:             1204\n",
      "Number of unique questions:           11858\n",
      "Number of unique answers:             16209\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading squad data...\")\n",
    "\n",
    "column_names = [\"title\", \"squad_context\", \"qid\", \"question\", \"is_impossible\", \"answer\"]\n",
    "\n",
    "try:\n",
    "    squad_raw = pd.read_csv(f\"{DATA_ROOT}/squad_raw.csv\", names=column_names, skiprows=1)\n",
    "    print(f\"Loaded squad data from {DATA_ROOT}/squad_raw.csv\\n\")\n",
    "except:\n",
    "    print(f\"Failed to load squad data from {DATA_ROOT}/squad_raw.csv\")\n",
    "\n",
    "df_squad_raw = pd.DataFrame(squad_raw)\n",
    "print(f\"Number of raw entries in squad_raw:   {len(df_squad_raw)}\")\n",
    "\n",
    "df_titles = pd.DataFrame(df_squad_raw['title'].unique(), columns=[\"title\"])\n",
    "print(f\"Number of unique titles:                 {len(df_titles)}\")\n",
    "\n",
    "df_contexts = pd.DataFrame(df_squad_raw['squad_context'].unique(), columns=[\"squad_context\"])\n",
    "print(f\"Number of unique contexts:             {len(df_contexts)}\")\n",
    "\n",
    "df_qas = df_squad_raw[['title', 'squad_context', 'qid', 'question', 'is_impossible']].drop_duplicates()\n",
    "df_qas = df_qas.reset_index(drop=True)\n",
    "print(f\"Number of unique questions:           {len(df_qas)}\")\n",
    "\n",
    "df_squad_answers = df_squad_raw[['qid', 'question', 'answer']].drop_duplicates()\n",
    "print(f\"Number of unique answers:             {len(df_squad_answers)}\")           \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus is analysed separately); if configured, re-chunk the data\n",
    "\n",
    "Next, the vector store is initialized. If the global variable CHUNK_SQUAD_DATASET is set to True, the dataset is chunked by calling the method chunk_squad_dataset(). Chunking depends on the current chunking method, the chunk size and the chunk overlap.If no chunking is done, the vector store has to already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking not foreseen. Skipping chunking.\n",
      "Will now get the following vector store: ../../../deh_data_results/chroma/chroma/chroma_deh_rag_db_k5_cs1000_per_article\n"
     ]
    }
   ],
   "source": [
    "contexts = list(df_contexts[\"squad_context\"].values)\n",
    "\n",
    "if CHUNK_SQUAD_DATASET:    \n",
    "    deh_vector_store.chunk_squad_dataset(contexts, dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "else:\n",
    "    print(\"Chunking not foreseen. Skipping chunking.\")\n",
    "\n",
    "# Intiialize the Chroma vector store\n",
    "vector_store = deh_vector_store.get_vector_store(DEFAULT_CHROMA_PREFIX, DEFAULT_CHUNKING_METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading qas with contexts data (if data is not to be restored)\n",
    "\n",
    "All experiments are carried out using a dataframe called \"df_qas_with_contexts\". This file contains the following columns:\n",
    "title\tsquad_context\tqid\tquestion\tis_impossible\thyde_article\tquestion_context\thyde_based_context\n",
    "\n",
    "- title: the title of the corrsponding Wikipedia article (e.g. Normans, Computational_complexity_theory, etc.)\n",
    "- squad_context: the text passage from the Wikipedia article to which the question relates\n",
    "- qid: the id of the question\n",
    "- question: the question itself\n",
    "- is_impossible: False, if the question can be answered from the squad_context, True if not (i.e. the article does not contain the necessary information to answer the question)\n",
    "- hyde_article: if not empty, the corresponding Hyde article that has been generated by an LLM for the question\n",
    "- question_context: the contexts retrieved from the vector store based on the question (depends on k, the chunking method and the chunking size)\n",
    "- hyde_based_context: the contexts retrieved from the vector store based on the Hyde article (depends on k, the chunking method and the chunking size)\n",
    "\n",
    "An important aspect to keep in mind here is that, for reasons of efficiency, all contexts are pre-created and stored in corresponding csv files. This accelartates running experiments, since if several experiments need to be run in succession or an experiment needs to be repeatedly run with different paramteres, then contexts do not need to be retrieved on the fly again and again for each experiment. It is important to note that these contexts depend on the number k of contexts that the vector store must  return and also on the chunking strategy that is being used, the chunking size and the chunking overlap. Furthermore, it is important to note that retrieving chunks from a vector store is a deterministic process (at least in our experiments settings) and that therefore, chunks can be created ahead of running the experiments.\n",
    "\n",
    "Another important point to note is that not all contexts based on Hyde have been created. The reason for that is that creating Hyde articles is very time consuming and therefore, the number of available Hyde articles is currently limited to approx. 4000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method gets the qas_with_contexts data from a csv file. Depending on the chunking \n",
    "# method (naive, per_context, per_article, pseudo_semantic), the chunk size and the paramter k,\n",
    "# it gets the data from a different csv file. The csv file contains all the necessary context \n",
    "# data and must already exist. If it does not, or if contexts need to be created for the \n",
    "# current experiment configuration, then this can be done by setting the global parameter\n",
    "# RESTORE_QAS_WITH_CONTEXTS and/or REFRESH_QUESION_CONTEXTS and REFRESH_HYDE_CONTEXTS.\n",
    "# Normally, creating contexts should not be a frequently used operation and once created,\n",
    "# reading qas_with_contexts data from a csv file should be the normal mode of operation.\n",
    "def load_qas_with_contexts_from_file(chunking_method):\n",
    "    \n",
    "    file_path = f\"{DATA_ROOT}/qas_with_contexts_k{VECTOR_STORE_TOP_K}_cs{CHUNK_SIZE}_{chunking_method}.csv\"\n",
    "    print(f\"Loading qas_with_contexts from the CSV file: {file_path}\\n...\")\n",
    "    \n",
    "    try:\n",
    "        df_qas_with_contexts = pd.read_csv(file_path) #, names=column_names)\n",
    "        print(f\"Loaded qas_with_contexts from {file_path}\\n...\")\n",
    "    except Exception as e:  # Catch all exceptions\n",
    "        print(f\"An error has occurred: {e}\")\n",
    "        print(f\"Failed to load qas_with_contexts from {file_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # TODO: the followin if statement can probably be removed. Check if \n",
    "    # is still needed and if not, remove it.\n",
    "    # drop the answer column if it exists, since it leads to duplicates\n",
    "    if 'answer' in df_qas_with_contexts.columns:\n",
    "        df_qas_with_contexts = df_qas_with_contexts.drop(columns=['answer'])\n",
    "\n",
    "    df_qas_with_contexts = df_qas_with_contexts.drop_duplicates()\n",
    "\n",
    "    print(f\"Rows in dataframe df_qas_with_contexts: {len(df_qas_with_contexts)}\")\n",
    "    \n",
    "    hyde_articles_cnt = df_qas_with_contexts['hyde_article'].notna().sum()\n",
    "    hyde_based_contexts_cnt = df_qas_with_contexts['hyde_based_context'].notna().sum()\n",
    "    print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "    print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n",
    "\n",
    "    return df_qas_with_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading qas_with_contexts from the CSV file: ../../../deh_data_results/data/qas_with_contexts_k5_cs1000_per_article.csv\n",
      "...\n",
      "Loaded qas_with_contexts from ../../../deh_data_results/data/qas_with_contexts_k5_cs1000_per_article.csv\n",
      "...\n",
      "Rows in dataframe df_qas_with_contexts: 11858\n",
      "Number of questions with Hyde articles: 3918\n",
      "Number of questions with Hyde based contexts: 3918\n",
      "Loading qas_with_contexts from the CSV file: ../../../deh_data_results/data/qas_with_contexts_k5_cs1000_naive.csv\n",
      "...\n",
      "Loaded qas_with_contexts from ../../../deh_data_results/data/qas_with_contexts_k5_cs1000_naive.csv\n",
      "...\n",
      "Rows in dataframe df_qas_with_contexts: 11858\n",
      "Number of questions with Hyde articles: 3918\n",
      "Number of questions with Hyde based contexts: 3918\n"
     ]
    }
   ],
   "source": [
    "# If not restoring qas_with_contexts from the CSV file, then read\n",
    "# the data from the csv file (i.e. it exists and is correct).\n",
    "# Else, the contexts will be (re-) created in one of the cells below\n",
    "# in the notebook.\n",
    "if not RESTORE_QAS_WITH_CONTEXTS:\n",
    "    df_qas_with_contexts = load_qas_with_contexts_from_file(DEFAULT_CHUNKING_METHOD)\n",
    "    df_qas_with_contexts_semantic = load_qas_with_contexts_from_file(DEFAULT_SEMANTIC_CHUNKING_METHOD)\n",
    "else:\n",
    "    print(f\"RESTORE_QAS_WITH_CONTEXTS is True. Skipping loading qas_with_contexts from the CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restore qas with contexts (if configured); alternatively refreseh contexts in existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only used for restoring the qas_with_contexts data, should it be missing or corrupted\n",
    "# Normally, this should not be frequently needed. When the function is called,\n",
    "# df_qas contains the initial raw data, as it has been read from the SQuAD raw file.\n",
    "#\n",
    "# Please also note: Hyde data (articles and contexts) is stored in a separate file (Hyde articles\n",
    "# are generated in a separate module).\n",
    "\n",
    "def restore_df_qas_with_contexts_file(file_path, df_qas):\n",
    "\n",
    "    # Add columns hyde_article, question_context, hyde_based_context to df_qas_with_contexts\n",
    "    # since the raw data does not contain these columns\n",
    "    if not 'hyde_article' in df_qas.columns:\n",
    "        df_qas['hyde_article'] = np.nan\n",
    "    if not 'question_context' in df_qas.columns:\n",
    "        df_qas['question_context'] = np.nan\n",
    "    if not 'hyde_based_context' in df_qas.columns:\n",
    "        df_qas['hyde_based_context'] = np.nan\n",
    "\n",
    "    # Get Hyde data\n",
    "    hyde_based_context_path = f\"{HYDE_BASED_CONTEXTS_ROOT}/hyde_based_contexts.csv\"\n",
    "    df_hyde_based_contexts = pd.read_csv(hyde_based_context_path)\n",
    "\n",
    "    print(f\"Rows in dataframe df_hyde_based_contexts: {len(df_hyde_based_contexts)}\")\n",
    "    hyde_articles_cnt = df_hyde_based_contexts['hyde_article'].notna().sum()\n",
    "    hyde_based_contexts_cnt = df_hyde_based_contexts['hyde_based_context'].notna().sum()\n",
    "\n",
    "    # Merge df_qas with df_hyde_based_contexts based on the 'qid' column\n",
    "    merged = df_qas.merge(df_hyde_based_contexts, on='qid', how='left', suffixes=('', '_df_hyde_based_contexts'))\n",
    "    df_qas['hyde_article'] = merged['hyde_article_df_hyde_based_contexts']\n",
    "\n",
    "    print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "    print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n",
    "\n",
    "    df_qas.to_csv(file_path, header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not restoring df_qas_with_contexts or refresh of the contexts...\n"
     ]
    }
   ],
   "source": [
    "# TODO --> add check if qas_with_contexts file already exists\n",
    "\n",
    "# The following is only needed if the qas_with_contexts file is missing or\n",
    "# corrupted or if the contexts need to be refreshed for any reason.\n",
    "\n",
    "if RESTORE_QAS_WITH_CONTEXTS:\n",
    "    restore_file_path = f\"{DATA_ROOT}/qas_with_contexts_k{VECTOR_STORE_TOP_K}_cs{CHUNK_SIZE}_{DEFAULT_CHUNKING_METHOD}.csv\"\n",
    "    restore_df_qas_with_contexts_file(restore_file_path, df_qas)\n",
    "\n",
    "# Refresh normal question contexts and hyde-based contexts\n",
    "# TODO: check if might be useful to first empty the two contexts columns in the dataframe\n",
    "if RESTORE_QAS_WITH_CONTEXTS or REFRESH_QUESTION_CONTEXTS or REFRESH_HYDE_CONTEXTS:\n",
    "    print(f\"Re-Generating contexts for the dataset and persisting the data...\")\n",
    "    list_of_qas = df_qas.to_dict(orient='records')\n",
    "\n",
    "    for i, qa in enumerate(list_of_qas):\n",
    "        # print(i)\n",
    "        if i %100 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        # Generating contexts based on questions (if necessary)\n",
    "        if RESTORE_QAS_WITH_CONTEXTS or REFRESH_QUESTION_CONTEXTS:\n",
    "            question = qa[\"question\"]\n",
    "            \n",
    "            top_docs = vector_store.similarity_search(\n",
    "                query = question,\n",
    "                k = VECTOR_STORE_TOP_K,\n",
    "            )\n",
    "            qa[\"question_context\"] = \"\\n\\n\".join([top_doc.page_content for top_doc in top_docs])\n",
    "\n",
    "        # Generating contexts based on Hyde articles (if necessary)\n",
    "        if RESTORE_QAS_WITH_CONTEXTS or REFRESH_HYDE_CONTEXTS:\n",
    "            hyde_article = qa[\"hyde_article\"]\n",
    "            #print(f\"hyde_article: {hyde_article}\")\n",
    "            if pd.isna(hyde_article):\n",
    "                hyde_article = \"\"\n",
    "            elif len(hyde_article) == 0:\n",
    "                hyde_article = \"\"\n",
    "            else:\n",
    "                top_docs = vector_store.similarity_search(\n",
    "                    query = hyde_article,\n",
    "                    k = VECTOR_STORE_TOP_K,\n",
    "                )\n",
    "\n",
    "                qa[\"hyde_based_context\"] = \"\\n\\n\".join([top_doc.page_content for top_doc in top_docs])\n",
    "\n",
    "    df_qas = pd.DataFrame(list_of_qas)\n",
    "    df_qas_with_contexts = df_qas.copy()\n",
    "    df_qas.to_csv(restore_file_path, header=True, index=False)\n",
    "else:\n",
    "    print(f\"Not restoring df_qas_with_contexts or refresh of the contexts...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show names of all dataframes\n",
    "\n",
    "Just for a general overview, generate all the dataframes that have been created up to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of Dataframes and their lenghts:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df name</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_experiments</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>df_squad_raw</td>\n",
       "      <td>26232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>df_titles</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>df_contexts</td>\n",
       "      <td>1204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>df_qas</td>\n",
       "      <td>11858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>df_squad_answers</td>\n",
       "      <td>16209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>df_qas_with_contexts</td>\n",
       "      <td>11858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>df_qas_with_contexts_semantic</td>\n",
       "      <td>11858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         df name   rows\n",
       "0                 df_experiments     45\n",
       "1                   df_squad_raw  26232\n",
       "2                      df_titles     35\n",
       "3                    df_contexts   1204\n",
       "4                         df_qas  11858\n",
       "5               df_squad_answers  16209\n",
       "6           df_qas_with_contexts  11858\n",
       "7  df_qas_with_contexts_semantic  11858"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "print(\"Names of Dataframes and their lenghts:\\n\")\n",
    "global_keys_copy = copy.deepcopy(list(globals().keys()))\n",
    "\n",
    "my_l = [{n: len(globals()[n])} for n in global_keys_copy if n.startswith(\"df_\")]\n",
    "df_dfs = pd.DataFrame([(k, v) for d in my_l for k, v in d.items()], columns=['df name', 'rows'])\n",
    "df_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions that are needed for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runnable chain\n",
    "def get_runnable_chain(current_query_prompt, llm):\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "    return runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hyde context for a question\n",
    "def get_hyde_based_context(question):\n",
    "    hyde_based_context = df_qas_with_contexts[df_qas_with_contexts[\"question\"] == question]\n",
    "    if hyde_based_context.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return hyde_based_context[\"hyde_based_context\"].values[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# Get the metrics for a set of predictions (preds) that have been generated in a run\n",
    "def get_squad_metrics(dataset, preds, verbose=False):\n",
    "    squad_metrics = squad_scoring.calc_squad_metrics(dataset, preds);\n",
    "    return squad_metrics[\"precision\"], squad_metrics[\"recall\"], squad_metrics[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and confidence interval for a list of scores\n",
    "def calculate_mean_confidence_interval(scores_l):\n",
    "\n",
    "    # Calculate mean\n",
    "    mean = np.mean(scores_l)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    sample_std_dev = np.std(scores_l, ddof=1)\n",
    "    margin_of_error = 1.96 * sample_std_dev\n",
    "    ci = (max(mean - margin_of_error, 0), min(mean + margin_of_error, 100))\n",
    "\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram for a list of scores and persist it\n",
    "def generate_histogram(scores_l, mean, ci, results_folder_name, experiment_name):\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(scores_l, bins=30, density=False, edgecolor='black', alpha=0.6, color = 'lightblue' ) # color='aquamarine')\n",
    "    plt.xlim(0, 100)\n",
    "\n",
    "    plt.title(f\"F1-Scores for {experiment_name} - (Bootstraps: {BOOTSTRAPS_N:,} - Sample Size: {SAMPLE_SIZE:,})\", fontsize=10)\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    max_len = 6\n",
    "    mean_label = f\"{mean: .2f}\".rjust(max_len)\n",
    "    plt.axvline(mean, color='red', linestyle='dotted', linewidth=2, label=f'Mean F1:          {mean_label}')\n",
    "\n",
    "    # Add vertical lines for the 95% confidence interval\n",
    "    lower = f\"{ci[0]: .2f}\".rjust(max_len)\n",
    "    upper = f\"{ci[1]: .2f}\".rjust(max_len)\n",
    "    plt.axvline(ci[0], color='orange', linestyle='dashdot', linewidth=1.5, label=f\"95% CI Lower:  {lower}\")\n",
    "    plt.axvline(ci[1], color='orange', linestyle='dashdot', linewidth=1.5, label=f\"95% CI Upper:  {upper}\")\n",
    "\n",
    "    ax = plt.gca()  # Get current axis\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(10))  # Set major ticks every 10 units\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(5))   # Set minor ticks every 5 units\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=10))\n",
    "    \n",
    "    # Customize grid for major and minor ticks\n",
    "    ax.grid(which='major', color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', color='lightgray', linestyle=':', linewidth=0.5)\n",
    "\n",
    "    # Add a legend\n",
    "\n",
    "    # Add additional text to the legend\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    model_info_line = Line2D([0], [0], color='none', label=f\"LLM Model: {CHAT_MODEL_NAME}\")\n",
    "    labels.append(f\"LLM Model: {CHAT_MODEL_NAME}\")\n",
    "    handles.append(model_info_line)\n",
    "\n",
    "    if experiment_name == \"BASIC_RAG_SEMANTIC_CHUNKGING\":\n",
    "        labels.append(f\"K={VECTOR_STORE_TOP_K} / {DEFAULT_SEMANTIC_CHUNKING_METHOD}\")\n",
    "    else:\n",
    "        labels.append(f\"K={VECTOR_STORE_TOP_K} / {DEFAULT_CHUNKING_METHOD}\")\n",
    "    handles.append(model_info_line)\n",
    "\n",
    "    plt.legend(prop={'family': 'monospace', 'size': 9})\n",
    "    plt.legend(handles, labels, loc='upper right', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_folder_name, f\"{experiment_name}_{BOOTSTRAPS_N}_{SAMPLE_SIZE}\"))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLM as a Judge functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_answer_tuple(input_string):\n",
    "    \n",
    "    # Trim whitespace\n",
    "    parts = input_string.strip()\n",
    "    # Remove parentheses\n",
    "    parts = parts.strip(\"()\")\n",
    "    # Split on comma\n",
    "    parts = parts.split(\", \")\n",
    "    if len(parts) != 2:\n",
    "        return (\"NO\", 0)\n",
    "\n",
    "    answer = parts[0].strip().upper()  # \"Yes\" or \"No\"\n",
    "    if answer not in [\"YES\", \"NO\"]:\n",
    "        return (\"NO\", 0)\n",
    "    try:\n",
    "        score = float(parts[1])  # Convert score to a float\n",
    "    except ValueError:\n",
    "        return (\"NO\", 0)\n",
    "    \n",
    "    return (answer, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_verdict(judge_verdicts):\n",
    "\n",
    "    judge_answers = judge_verdicts[0]\n",
    "    yes_count = judge_answers.count('YES')\n",
    "    no_count = judge_answers.count('NO')\n",
    "\n",
    "    if no_count > yes_count:\n",
    "        return \"NO\", 0.0\n",
    "    \n",
    "    judge_scores = list(judge_verdicts[1])\n",
    "    return \"YES\", sum(judge_scores) / yes_count\n",
    "\n",
    "def get_majority_verditcs(all_judge_answers, all_judge_scores):\n",
    "\n",
    "    majority_verdicts = []\n",
    "    for per_question_judge_verdicts in zip(zip(*all_judge_answers), zip(*all_judge_scores)):\n",
    "        majority_verdict = get_majority_verdict(per_question_judge_verdicts)\n",
    "        majority_verdicts.append(majority_verdict)\n",
    "\n",
    "    return majority_verdicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_judges_verdicts(sample_ldict, experiment_name, hyde_context_needed=False):\n",
    "    judge_current_query_prompt = deh_prompts.query_prompts[9]\n",
    "    #judge_current_query_prompt = deh_prompts.BASIC_RAG_LLM_AS_A_JUDGE_PROMPT\n",
    "    print(f\"judge_current_query_prompt = {judge_current_query_prompt.template}\\n\")\n",
    "\n",
    "\n",
    "    for ele in sample_ldict:\n",
    "        print(ele)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    all_judge_answers = []\n",
    "    all_judge_scores = []\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    for judge_i, judge_llm in enumerate(judge_llms):\n",
    "        judge_chain = judge_current_query_prompt | get_llm(judge_current_query_prompt, True, judge_llm)\n",
    "        judge_answers = []\n",
    "        judge_scores = []\n",
    "\n",
    "        for i, qa in enumerate(sample_ldict):\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processing question {i} for judge {judge_i}...\")\n",
    "\n",
    "            question = qa[\"question\"]\n",
    "            context = qa[\"question_context\"]\n",
    "            if hyde_context_needed:\n",
    "                hyde_based_context = qa[\"hyde_based_context\"]\n",
    "                context = \"\\n\\n\".join([context, hyde_based_context])\n",
    "            answer_key = experiment_name.lower() + \"_llm_answer\"\n",
    "            answer = qa[answer_key]\n",
    "\n",
    "            response = judge_chain.invoke({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "            a, s = convert_string_to_answer_tuple(response.content)\n",
    "            judge_answers.append(a)\n",
    "            judge_scores.append(s)\n",
    "            \n",
    "        all_judge_answers.append(judge_answers)\n",
    "        all_judge_scores.append(judge_scores)\n",
    "\n",
    "    final_judges_verdicts = get_majority_verditcs(all_judge_answers, all_judge_scores)\n",
    "    print(\"Printing final_judges_verdicts:\")\n",
    "    for final_verdict in final_judges_verdicts:\n",
    "        print(final_verdict)\n",
    "\n",
    "    return final_judges_verdicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions for Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp and format the start timestamp as a string\n",
    "def get_timestamp_as_string():\n",
    "    start_timestamp = datetime.now()\n",
    "    start_timestamp_str = start_timestamp.strftime('%Y%m%d_%H%M%S')\n",
    "    return start_timestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(qas, query_prompt_idx, experiment_name, context_needed=False,\n",
    "                     hyde_based_context_needed=False, suppress_answers=False):\n",
    "\n",
    "    # Create the chain\n",
    "    current_query_prompt = deh_prompts.query_prompts[query_prompt_idx]\n",
    "    print(f\"current_query_prompt = {current_query_prompt.template}\\n\")\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = get_runnable_chain(current_query_prompt, llm)\n",
    "\n",
    "    # Generate the LLM answers for all questions and calculate per-answer metrics \n",
    "    # add each answer to the all_preds\n",
    "    preds = {}\n",
    "    all_preds= {}\n",
    "    prefix = experiment_name.lower()\n",
    "\n",
    "    for i, qa in enumerate(qas):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        qid = qa[\"qid\"]\n",
    "        question = qa[\"question\"]\n",
    "\n",
    "        if context_needed or hyde_based_context_needed:\n",
    "            if not hyde_based_context_needed:\n",
    "                context = qa[\"question_context\"]\n",
    "            elif not context_needed:\n",
    "                context = qa[\"hyde_based_context\"]\n",
    "            else:\n",
    "                context = qa[\"question_context\"] + \"\\n\\n\" + qa[\"hyde_based_context\"]\n",
    "\n",
    "            response = runnable_chain.invoke({\"question\": question, \"context\": context})\n",
    "        else:\n",
    "            response = runnable_chain.invoke({\"question\": question})\n",
    "\n",
    "        llm_answer = response.content\n",
    "\n",
    "        if \"DONT KNOW\" in llm_answer.upper() or llm_answer.upper().startswith(\"NO CONTEXT PROVIDED\") or llm_answer.upper().startswith(\"NO INFORMATION GIVEN\"): # or llm_answer.upper().startswith(\"UNFORTUNATELY\"):     \n",
    "            llm_answer = \"\"\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        all_preds[qid] = llm_answer\n",
    "\n",
    "        scores = squad_scoring.calc_squad_metrics(dataset, preds)\n",
    "        f1 = scores[\"f1\"]\n",
    "        precision = scores[\"precision\"]\n",
    "        recall = scores[\"recall\"]\n",
    "        jaccard = scores[\"jaccard\"]\n",
    "\n",
    "        preds = {}\n",
    "        qa[f\"{prefix}_llm_answer\"] = llm_answer\n",
    "        qa[f\"{prefix}_f1\"] = f1\n",
    "        qa[f\"{prefix}_precision\"] = precision\n",
    "        qa[f\"{prefix}_recall\"] = recall\n",
    "        qa[f\"{prefix}_jaccard\"] = jaccard\n",
    "\n",
    "    if suppress_answers:\n",
    "        threw_away_answer = False\n",
    "        threw_away_answers_cnt = 0\n",
    "        preds = {}\n",
    "\n",
    "        final_judges_verdicts = get_final_judges_verdicts(qas, experiment_name, hyde_context_needed)\n",
    "\n",
    "        for i, qa in enumerate(qas):\n",
    "\n",
    "            qid = qa[\"qid\"]\n",
    "            question = qa[\"question\"]\n",
    "\n",
    "            if final_judges_verdicts[i][0] == \"NO\":\n",
    "                if not qa[f\"{prefix}_llm_answer\"] == \"\":\n",
    "                    threw_away_answer = True\n",
    "                    threw_away_answers_cnt += 1\n",
    "                    print(f\"Threw away answer for question {i}: {qa['question']}\")\n",
    "                qa[f\"{prefix}_llm_answer\"] = \"\"\n",
    "            else:\n",
    "                if final_judges_verdicts[i][1] < JUDGES_SUPPRESS_THRESHOLD:\n",
    "                    if not qa[f\"{prefix}_llm_answer\"] == \"\":\n",
    "                        threw_away_answer = True\n",
    "                        threw_away_answers_cnt += 1\n",
    "                        print(f\"Threw away answer for question {i}: {qa['question']}\")\n",
    "                    qa[f\"{prefix}_llm_answer\"] = \"\"\n",
    "\n",
    "            if threw_away_answer:\n",
    "\n",
    "                preds[qid] = \"\"\n",
    "                all_preds[qid] = \"\"\n",
    "                scores = squad_scoring.calc_squad_metrics(dataset, preds)\n",
    "                f1 = scores[\"f1\"]\n",
    "                precision = scores[\"precision\"]\n",
    "                recall = scores[\"recall\"]\n",
    "                jaccard = scores[\"jaccard\"]\n",
    "\n",
    "                preds = {}\n",
    "                qa[f\"{prefix}_llm_answer\"] = \"\"\n",
    "                qa[f\"{prefix}_f1\"] = f1\n",
    "                qa[f\"{prefix}_precision\"] = precision\n",
    "                qa[f\"{prefix}_recall\"] = recall\n",
    "                qa[f\"{prefix}_jaccard\"] = jaccard     \n",
    "\n",
    "            threw_away_answer = False   \n",
    "\n",
    "        print(f\"Threw away {threw_away_answers_cnt} answers.\")\n",
    "\n",
    "    return all_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_results(results_folder_name, experiment_name, df, eval_dict):\n",
    "\n",
    "    df.to_csv(f\"{results_folder_name}/qas_{experiment_name.lower()}_scores.csv\", header=True, index=False)\n",
    "    df[['qid', 'question', f\"{experiment_name.lower()}_llm_answer\"]].to_csv(f\"{results_folder_name}/qas_{experiment_name.lower()}_answers.csv\", header=True, index=False)\n",
    "   \n",
    "    json_file_name = f\"{results_folder_name}/qas_{experiment_name.lower()}_answers.json\"\n",
    "    df_json = df[['qid', f\"{experiment_name.lower()}_llm_answer\"]].rename(columns={f\"{experiment_name.lower()}_llm_answer\": \"answer\"})\n",
    "    answer_data = {row[\"qid\"]: row[\"answer\"] for _, row in df_json.iterrows()}\n",
    "    with open(json_file_name, \"w\") as json_file:\n",
    "        json.dump(answer_data, json_file, indent=4)\n",
    "\n",
    "    eval_dict_file_name = f\"{results_folder_name}/eval_dict_{experiment_name.lower()}.json\"\n",
    "    eval_data = {key: round(value, 2) for key, value in eval_dict.items()}\n",
    "    with open(eval_dict_file_name, \"w\") as json_file:\n",
    "        json.dump(eval_data, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_experiments_config(results_folder_name, experiment_name, query_prompt_idx, \n",
    "                               experiment_mean, experiment_ci, execution_time):\n",
    "\n",
    "    params_str = f\"Experiment: {experiment_name}\\n\"\n",
    "    params_str = params_str + f\"{datetime.now()}\\n\"\n",
    "    params_str = params_str + f\"Execution duration in minutes: {round(execution_time/60,2)}\\n\"\n",
    "    params_str = params_str + \"================================================\\n\\n\"\n",
    "\n",
    "    current_query_prompt_str = deh_prompts.query_prompts[query_prompt_idx].template\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + \"Prompt used:\\n\"\n",
    "    params_str = params_str + current_query_prompt_str + \"\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"LLM: {CHAT_MODEL_NAME}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"F1 mean: {round(experiment_mean, 2)}\\n\"\n",
    "    params_str = params_str + f\"95% CI ({round(experiment_ci[0], 2)}, {round(experiment_ci[1],2)})\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"DATA_ROOT = {DATA_ROOT}\\n\"\n",
    "    params_str = params_str + f\"RESULTS_ROOT = {RESULTS_ROOT}\\n\"\n",
    "    params_str = params_str + f\"HYDE_BASED_CONTEXTS_ROOT  = {HYDE_BASED_CONTEXTS_ROOT}\\n\"\n",
    "    params_str = params_str + f\"data_file = {DATA_ROOT}/dev-v2.0.json\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"CHROMA_ROOT               = {CHROMA_ROOT}\\n\"\n",
    "    params_str = params_str + f\"VECTOR_STORE_TOP_K        = {VECTOR_STORE_TOP_K}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_SIZE                = {CHUNK_SIZE}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_OVERLAP             = {CHUNK_OVERLAP}\\n\"\n",
    "    params_str = params_str + f\"DEFAULT_CHROMA_PREFIX     = {DEFAULT_CHROMA_PREFIX}\\n\"\n",
    "    params_str = params_str + f\"DEFAULT_CHUNKING_METHOD   = {DEFAULT_CHUNKING_METHOD}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_SQUAD_DATASET       = {CHUNK_SQUAD_DATASET}\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"REFRESH_QUESTION_CONTEXTS = {REFRESH_QUESTION_CONTEXTS}\\n\"\n",
    "    params_str = params_str + f\"REFRESH_HYDE_CONTEXTS     = {REFRESH_HYDE_CONTEXTS}\\n\"\n",
    "    params_str = params_str + f\"RESTORE_QAS_WITH_CONTEXTS = {RESTORE_QAS_WITH_CONTEXTS}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"        \n",
    "    params_str = params_str + f\"SAMPLE_SIZE  = {SAMPLE_SIZE}\\n\"\n",
    "    params_str = params_str + f\"BOOTSTRAPS_N = {BOOTSTRAPS_N}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_0  = {LLM_MODEL_NAME_0}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_1  = {LLM_MODEL_NAME_1}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_2  = {LLM_MODEL_NAME_2}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_3  = {LLM_MODEL_NAME_3}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_4  = {LLM_MODEL_NAME_4}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_5  = {LLM_MODEL_NAME_5}\\n\"\n",
    "    params_str = params_str + f\"CHAT_MODEL_NAME   = {CHAT_MODEL_NAME}\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"MAX_TOKENS        = {MAX_TOKENS}\\n\"\n",
    "    params_str = params_str + f\"TEMPERATURE       = {TEMPERATURE}\\n\"\n",
    "    params_str = params_str + f\"TOP_P             = {TOP_P}\\n\"\n",
    "    params_str = params_str + f\"FREQUENCY_PENALTY = {FREQUENCY_PENALTY}\\n\"\n",
    "    params_str = params_str + f\"PRESENCE_PENALTY  = {PRESENCE_PENALTY}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    judges = [llm_models[judge_i] for judge_i in judge_llms]\n",
    "    params_str = params_str + f\"Judge LLMS = {judges}\\n\"\n",
    "    params_str = params_str + f\"JUDGES_SUPPRESS_THRESHOLD   = {JUDGES_SUPPRESS_THRESHOLD}\\n\\n\"\n",
    "\n",
    "    with open(f\"{results_folder_name}/experiments_config_{experiment_name.lower()}.txt\", 'w') as f:\n",
    "        f.write(params_str)\n",
    "\n",
    "    print(\"Persisting the experiment config done...\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_scores(scores_l, clip_perc):\n",
    "    scores_sorted_l = sorted(scores_l)\n",
    "\n",
    "    clip_cnt = int(len(scores_sorted_l) * clip_perc / 100)\n",
    "    print(f\"clip_cnt: {clip_cnt}\")\n",
    "\n",
    "    # Now clip both ends by clip_perc percent\n",
    "    clipped_scores_l = scores_sorted_l[clip_cnt:-clip_cnt] if clip_cnt > 0 else scores_sorted_l\n",
    "    return clipped_scores_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bootstrapping(scores_l, results_folder_name, experiment_name, bootstraps_n = BOOTSTRAPS_N):\n",
    "    \n",
    "    mu_hats = []\n",
    "    n = len(scores_l)\n",
    "    scores_l = clip_scores(scores_l, 2.5)\n",
    "\n",
    "    for i in range(bootstraps_n):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processing sample {i}...\")\n",
    "        bootstrap_sample = random.choices(scores_l, k=n) # sample with replacement\n",
    "        mu_hat = np.mean(bootstrap_sample)\n",
    "        mu_hats.append(mu_hat)\n",
    "\n",
    "    bootstraps_mean, ci = calculate_mean_confidence_interval(mu_hats)\n",
    "    plt = generate_histogram(mu_hats, bootstraps_mean, ci, results_folder_name, experiment_name)\n",
    "    plt.show();\n",
    "    return bootstraps_mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_folder(start_timestamp_str, experiment_name=None):\n",
    "\n",
    "    if experiment_name:\n",
    "        results_folder_name = f\"{RESULTS_ROOT}/{EXPERIMENT_GLOBAL_ID}_{start_timestamp_str}/{experiment_name}\"\n",
    "    else:\n",
    "        results_folder_name = f\"{RESULTS_ROOT}/{EXPERIMENT_GLOBAL_ID}_{start_timestamp_str}\"\n",
    "    if not os.path.exists(results_folder_name):\n",
    "        os.makedirs(results_folder_name, exist_ok=True)\n",
    "    return results_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(qas, start_timestamp_str, experiment_name, query_prompt_idx, \n",
    "                   context_needed=False, hyde_context_needed=False, \n",
    "                   suppress_answers=False):\n",
    "\n",
    "    exp_start_time = time.perf_counter()\n",
    "    results_folder_name = create_results_folder(start_timestamp_str, experiment_name)\n",
    "\n",
    "    print(f\"\\n\\n============= Now running experiment {experiment_name} =============\\n\")\n",
    "    \n",
    "    print(f\"============= Calculating scores for {experiment_name} =============\\n\")\n",
    "    print(f\"SAMPLE_SIZE: {SAMPLE_SIZE}\\n\")\n",
    "    all_preds = calculate_scores(qas, query_prompt_idx, experiment_name, context_needed, \n",
    "                                 hyde_context_needed, suppress_answers)\n",
    "\n",
    "    eval_dict = squad_scoring.calc_squad_metrics(dataset, all_preds)\n",
    "    print(f\"\\n================= SQuAD Scores:==================================\\n\")\n",
    "    for key, value in eval_dict.items():\n",
    "        print(f\"{key}: {round(value,2)}\")\n",
    "        if \"total\" in key:\n",
    "            print(\"\")\n",
    "                \n",
    "    print(f\"============= Persisting results for {experiment_name} =============\\n\")\n",
    "    df = pd.DataFrame(qas)\n",
    "    persist_results(results_folder_name, experiment_name, df, eval_dict)\n",
    "    \n",
    "    print(f\"============= Bootstrapping for {experiment_name} =============\\n\")\n",
    "    print(f\"BOOTSTRAPS_N: {BOOTSTRAPS_N}\")\n",
    "\n",
    "    experiment_mean, experiment_ci = do_bootstrapping(df[f\"{experiment_name.lower()}_f1\"].dropna().tolist(), results_folder_name, experiment_name, BOOTSTRAPS_N)\n",
    "\n",
    "    # print(f\"============= Get RAGAS metrics for {experiment_name} =============\\n\")\n",
    "    \n",
    "    # generate_ragas_metrics(df, experiment_name, query_prompt_idx, results_folder_name)\n",
    "\n",
    "    exp_end_time = time.perf_counter()\n",
    "    execution_time = exp_end_time - exp_start_time\n",
    "    execution_times_entry = {}\n",
    "    execution_times_entry[\"experiment_name\"] = experiment_name\n",
    "    execution_times_entry[\"execution_time\"] = execution_time\n",
    "    execution_times_entry[\"sample_size\"] = SAMPLE_SIZE\n",
    "    execution_times_entry[\"bootstrap_n\"] = BOOTSTRAPS_N\n",
    "    execution_times_l.append(execution_times_entry)\n",
    "\n",
    "    print(f\"Will now persist the experiment configuration for {experiment_name}...\")\n",
    "    persist_experiments_config(results_folder_name, experiment_name, query_prompt_idx, \n",
    "                               experiment_mean, experiment_ci, execution_time)\n",
    "\n",
    "    return execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a sample from df_qas_with_contexts for bootstrapping and bootstrapping with Hyde\n",
    "\n",
    "To do bootstrapping, a random sample of size SAMPLE_SIZE from is drawn from the datarame \"df_qas_with_contexts\". There are three different sub-types of samples, depending on the type of the experiment:\n",
    "\n",
    "- one \"normal\" sample (df_qas_for_bootstrapping_sample)\n",
    "- one specific sample for Hyde (df_qas_for_hyde_bootstrapping_sample)\n",
    "- one specific sample if semantic chunking is used (ldict_qas_for_boostrapping_sample_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: names of samples are far too long; make them much shorter\n",
    "# TODO: check if separate sample for semantic chunking is really necessary\n",
    "\n",
    "df_qas_for_bootstrapping = df_qas_with_contexts[['qid', 'question', 'question_context', 'hyde_based_context']]\n",
    "df_qas_for_bootstrapping_sample = df_qas_for_bootstrapping.sample(n=SAMPLE_SIZE, replace=True)  #, random_state=42)\n",
    "\n",
    "ldict_qas_for_boostrapping_sample = df_qas_for_bootstrapping_sample.to_dict(orient='records')\n",
    "df_qas_for_bootstrapping_semantic = df_qas_with_contexts_semantic[['qid', 'question', 'question_context', 'hyde_based_context']]\n",
    "\n",
    "# TODO: There is surely a better way to implement this with dataframe indexing, but for the moment\n",
    "# I'll just keep it as is...\n",
    "sample_idx = set(df_qas_for_bootstrapping_sample.index)\n",
    "ldict_qas_for_boostrapping_sample_semantic = []\n",
    "for index, row in df_qas_for_bootstrapping_semantic.iterrows():\n",
    "    if index in sample_idx:\n",
    "        ldict_qas_for_boostrapping_sample_semantic.append(row.to_dict())\n",
    "\n",
    "df_qas_for_hyde_bootstrapping = df_qas_for_bootstrapping[df_qas_for_bootstrapping['hyde_based_context'].notna()]\n",
    "df_qas_for_hyde_bootstrapping_sample = df_qas_for_hyde_bootstrapping.sample(n=SAMPLE_SIZE, replace=True) #, random_state=42)\n",
    "ldict_qas_for_hyde_bootstrapping_sample = df_qas_for_hyde_bootstrapping_sample.to_dict(orient='records')   \n",
    "\n",
    "sample_ldicts = [ldict_qas_for_boostrapping_sample, \n",
    "                 ldict_qas_for_hyde_bootstrapping_sample,\n",
    "                 ldict_qas_for_boostrapping_sample_semantic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running the current experiment\n",
    "\n",
    "Currently runs one experiment, but can be easily modified to run a series of experiments in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp_str = get_timestamp_as_string()\n",
    "results_folder_name = create_results_folder(start_timestamp_str)\n",
    "\n",
    "# for row in [current_experiment]:\n",
    "exp = current_experiment\n",
    "\n",
    "sample_ldict = sample_ldicts[exp[\"sample_ldicts_idx\"]]\n",
    "experiment_name = exp[\"name\"]\n",
    "query_prompt_idx = exp[\"query_prompt_idx\"]\n",
    "context_needed = exp[\"context_needed\"]\n",
    "hyde_context_needed = exp[\"hyde_context_needed\"]\n",
    "suppress_answers = exp[\"suppress_answers\"]\n",
    "\n",
    "print(f\"experiment_name: {experiment_name}\")\n",
    "print(f\"query_prompt_idx: {query_prompt_idx}\")\n",
    "print(f\"context_needed: {context_needed}\")\n",
    "print(f\"hyde_context_needed: {hyde_context_needed}\")\n",
    "print(f\"suppress_answers: {suppress_answers}\")\n",
    "print(f\"CHAT_MODEL_NAME: {CHAT_MODEL_NAME}\")\n",
    "print(f\"VECTOR_STORE_TOP_K: {VECTOR_STORE_TOP_K}\")\n",
    "print(f\"TEMPERATURE: {TEMPERATURE}\")\n",
    "print(f\"JUDGES_SUPPRESS_THRESHOLD: {JUDGES_SUPPRESS_THRESHOLD}\")\n",
    "\n",
    "run_experiment(sample_ldict, start_timestamp_str, experiment_name, query_prompt_idx,\n",
    "            context_needed, hyde_context_needed, suppress_answers)\n",
    "\n",
    "print(f\"\\n================== Execution times (in seconds): ==================================\\n\")\n",
    "df_execution_times = pd.DataFrame(execution_times_l, \n",
    "                                columns=['experiment_name', 'execution_time', 'sample_size', 'bootstrap_n'])\n",
    "\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "df_execution_times.head(100)              \n",
    "\n",
    "print(f\"Total execution time in Minutes: {round(sum(df_execution_times['execution_time']) / 60, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the display format for floats\n",
    "pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total execution time in Minutes: {round(sum(df_execution_times['execution_time']) / 60, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
