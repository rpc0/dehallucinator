{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for deh experiments specific modules\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "utils_folder = Path(\".\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import deh_globals\n",
    "globals().update(deh_globals.__dict__)\n",
    "import squad_scoring\n",
    "globals().update(deh_globals.__dict__)\n",
    "import deh_prompts\n",
    "import deh_vector_store\n",
    "globals().update(deh_vector_store.__dict__)\n",
    "import deh_squad_data\n",
    "import deh_hyde\n",
    "import deh_experiments_config\n",
    "globals().update(deh_experiments_config.__dict__)\n",
    "import deh_llm\n",
    "globals().update(deh_llm.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.) Define the current Experiment\n",
    "\n",
    "Currently, the notebook runs one single  experiment that is selected in the next cell. However, it can easily be adapted to run more than one experiment. All experiments are defined and configured in the module \"deh_experiments_config.py\". Furthermore, in the next cell a series of global parameters are set. These depend on the choice of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment = experiments[\"FINAL_FINAL_RAG_TR_90\"]\n",
    "\n",
    "EXPERIMENT_GLOBAL_ID = current_experiment[\"name\"]\n",
    "DEFAULT_CHUNKING_METHOD = current_experiment[\"chunking_method\"]\n",
    "CHAT_MODEL_NAME = current_experiment[\"llm_model\"]\n",
    "TEMPERATURE = current_experiment[\"temperature\"]\n",
    "VECTOR_STORE_TOP_K = current_experiment[\"vector_store_top_k\"]\n",
    "JUDGES_SUPPRESS_THRESHOLD = current_experiment[\"judges_suppress_threshold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.) Functions for loading and initializing Data\n",
    "\n",
    "Next, the SQuAD dataset set (Version v2.0, dev set) is loaded in raw format from a csv file into a dataset. Loading the dataset in its raw format is useful for some of the functions of the notebook. After reading in the raw dataset, several dataframes are created to show statistics about the dataframe.\n",
    "\n",
    "Please note the following two important points:\n",
    "\n",
    "- throughout this notebook the term \"title\" and \"article\" are used as synonyms\n",
    "- the SQuAD dataset contains text passages from Wikipedia articles. Each such segment is called \"context\" and belongs to one of the articles. This is a source of confusion since the items retrieved from a vector store as the result of a similarity search are also called \"contexts\". Wherever possible, we will try to distinguish these two different meanings of the word \"context\" (so ulimtately, the meaning of the word \"context\", depends on the context it is used in :-) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading squad data...\")\n",
    "\n",
    "column_names = [\"title\", \"squad_context\", \"qid\", \"question\", \"is_impossible\", \"answer\"]\n",
    "\n",
    "try:\n",
    "    squad_raw = pd.read_csv(f\"{DATA_ROOT}/squad_raw.csv\", names=column_names, skiprows=1)\n",
    "    print(f\"Loaded squad data from {DATA_ROOT}/squad_raw.csv\\n\")\n",
    "except:\n",
    "    print(f\"Failed to load squad data from {DATA_ROOT}/squad_raw.csv\")\n",
    "\n",
    "df_squad_raw = pd.DataFrame(squad_raw)\n",
    "print(f\"Number of raw entries in squad_raw:   {len(df_squad_raw)}\")\n",
    "\n",
    "df_titles = pd.DataFrame(df_squad_raw['title'].unique(), columns=[\"title\"])\n",
    "print(f\"Number of unique titles:                 {len(df_titles)}\")\n",
    "\n",
    "df_contexts = pd.DataFrame(df_squad_raw['squad_context'].unique(), columns=[\"squad_context\"])\n",
    "print(f\"Number of unique contexts:             {len(df_contexts)}\")\n",
    "\n",
    "df_qas = df_squad_raw[['title', 'squad_context', 'qid', 'question', 'is_impossible']].drop_duplicates()\n",
    "df_qas = df_qas.reset_index(drop=True)\n",
    "print(f\"Number of unique questions:           {len(df_qas)}\")\n",
    "\n",
    "df_squad_answers = df_squad_raw[['qid', 'question', 'answer']].drop_duplicates()\n",
    "print(f\"Number of unique answers:             {len(df_squad_answers)}\")           \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus is analysed separately); if configured, re-chunk the data\n",
    "\n",
    "Next, the vector store is initialized. If the global variable CHUNK_SQUAD_DATASET is set to True, the dataset is chunked by calling the method chunk_squad_dataset(). Chunking depends on the current chunking method, the chunk size and the chunk overlap.If no chunking is done, the vector store has to already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = list(df_contexts[\"squad_context\"].values)\n",
    "\n",
    "if CHUNK_SQUAD_DATASET:    \n",
    "    deh_vector_store.chunk_squad_dataset(contexts, dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "else:\n",
    "    print(\"Chunking not foreseen. Skipping chunking.\")\n",
    "\n",
    "# Intiialize the Chroma vector store\n",
    "vector_store = deh_vector_store.get_vector_store(DEFAULT_CHROMA_PREFIX, DEFAULT_CHUNKING_METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading qas with contexts data (if data is not to be restored)\n",
    "\n",
    "All experiments are carried out using a dataframe called \"df_qas_with_contexts\". This file contains the following columns:\n",
    "title\tsquad_context\tqid\tquestion\tis_impossible\thyde_article\tquestion_context\thyde_based_context\n",
    "\n",
    "- title: the title of the corrsponding Wikipedia article (e.g. Normans, Computational_complexity_theory, etc.)\n",
    "- squad_context: the text passage from the Wikipedia article to which the question relates\n",
    "- qid: the id of the question\n",
    "- question: the question itself\n",
    "- is_impossible: False, if the question can be answered from the squad_context, True if not (i.e. the article does not contain the necessary information to answer the question)\n",
    "- hyde_article: if not empty, the corresponding Hyde article that has been generated by an LLM for the question\n",
    "- question_context: the contexts retrieved from the vector store based on the question (depends on k, the chunking method and the chunking size)\n",
    "- hyde_based_context: the contexts retrieved from the vector store based on the Hyde article (depends on k, the chunking method and the chunking size)\n",
    "\n",
    "An important aspect to keep in mind here is that, for reasons of efficiency, all contexts are pre-created and stored in corresponding csv files. This accelartates running experiments, since if several experiments need to be run in succession or an experiment needs to be repeatedly run with different paramteres, then contexts do not need to be retrieved on the fly again and again for each experiment. It is important to note that these contexts depend on the number k of contexts that the vector store must  return and also on the chunking strategy that is being used, the chunking size and the chunking overlap. Furthermore, it is important to note that retrieving chunks from a vector store is a deterministic process (at least in our experiments settings) and that therefore, chunks can be created ahead of running the experiments.\n",
    "\n",
    "Another important point to note is that not all contexts based on Hyde have been created. The reason for that is that creating Hyde articles is very time consuming and therefore, the number of available Hyde articles is currently limited to approx. 4000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method gets the qas_with_contexts data from a csv file. Depending on the chunking \n",
    "# method (naive, per_context, per_article, pseudo_semantic), the chunk size and the paramter k,\n",
    "# it gets the data from a different csv file. The csv file contains all the necessary context \n",
    "# data and must already exist. If it does not, or if contexts need to be created for the \n",
    "# current experiment configuration, then this can be done by setting the global parameter\n",
    "# RESTORE_QAS_WITH_CONTEXTS and/or REFRESH_QUESION_CONTEXTS and REFRESH_HYDE_CONTEXTS.\n",
    "# Normally, creating contexts should not be a frequently used operation and once created,\n",
    "# reading qas_with_contexts data from a csv file should be the normal mode of operation.\n",
    "def load_qas_with_contexts_from_file(chunking_method):\n",
    "    \n",
    "    file_path = f\"{DATA_ROOT}/qas_with_contexts_k{VECTOR_STORE_TOP_K}_cs{CHUNK_SIZE}_{chunking_method}.csv\"\n",
    "    print(f\"Loading qas_with_contexts from the CSV file: {file_path}\\n...\")\n",
    "    \n",
    "    try:\n",
    "        df_qas_with_contexts = pd.read_csv(file_path) #, names=column_names)\n",
    "        print(f\"Loaded qas_with_contexts from {file_path}\\n...\")\n",
    "    except Exception as e:  # Catch all exceptions\n",
    "        print(f\"An error has occurred: {e}\")\n",
    "        print(f\"Failed to load qas_with_contexts from {file_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # TODO: the followin if statement can probably be removed. Check if \n",
    "    # is still needed and if not, remove it.\n",
    "    # drop the answer column if it exists, since it leads to duplicates\n",
    "    if 'answer' in df_qas_with_contexts.columns:\n",
    "        df_qas_with_contexts = df_qas_with_contexts.drop(columns=['answer'])\n",
    "\n",
    "    df_qas_with_contexts = df_qas_with_contexts.drop_duplicates()\n",
    "\n",
    "    print(f\"Rows in dataframe df_qas_with_contexts: {len(df_qas_with_contexts)}\")\n",
    "    \n",
    "    hyde_articles_cnt = df_qas_with_contexts['hyde_article'].notna().sum()\n",
    "    hyde_based_contexts_cnt = df_qas_with_contexts['hyde_based_context'].notna().sum()\n",
    "    print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "    print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n",
    "\n",
    "    return df_qas_with_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not restoring qas_with_contexts from the CSV file, then read\n",
    "# the data from the csv file (i.e. it exists and is correct).\n",
    "# Else, the contexts will be (re-) created in one of the cells below\n",
    "# in the notebook.\n",
    "if not RESTORE_QAS_WITH_CONTEXTS:\n",
    "    df_qas_with_contexts = load_qas_with_contexts_from_file(DEFAULT_CHUNKING_METHOD)\n",
    "    df_qas_with_contexts_semantic = load_qas_with_contexts_from_file(DEFAULT_SEMANTIC_CHUNKING_METHOD)\n",
    "else:\n",
    "    print(f\"RESTORE_QAS_WITH_CONTEXTS is True. Skipping loading qas_with_contexts from the CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restore qas with contexts (if configured); alternatively refreseh contexts in existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only used for restoring the qas_with_contexts data, should it be missing or corrupted\n",
    "# Normally, this should not be frequently needed. When the function is called,\n",
    "# df_qas contains the initial raw data, as it has been read from the SQuAD raw file.\n",
    "#\n",
    "# Please also note: Hyde data (articles and contexts) is stored in a separate file (Hyde articles\n",
    "# are generated in a separate module).\n",
    "\n",
    "def restore_df_qas_with_contexts_file(file_path, df_qas):\n",
    "\n",
    "    # Add columns hyde_article, question_context, hyde_based_context to df_qas_with_contexts\n",
    "    # since the raw data does not contain these columns\n",
    "    if not 'hyde_article' in df_qas.columns:\n",
    "        df_qas['hyde_article'] = np.nan\n",
    "    if not 'question_context' in df_qas.columns:\n",
    "        df_qas['question_context'] = np.nan\n",
    "    if not 'hyde_based_context' in df_qas.columns:\n",
    "        df_qas['hyde_based_context'] = np.nan\n",
    "\n",
    "    # Get Hyde data\n",
    "    hyde_based_context_path = f\"{HYDE_BASED_CONTEXTS_ROOT}/hyde_based_contexts.csv\"\n",
    "    df_hyde_based_contexts = pd.read_csv(hyde_based_context_path)\n",
    "\n",
    "    print(f\"Rows in dataframe df_hyde_based_contexts: {len(df_hyde_based_contexts)}\")\n",
    "    hyde_articles_cnt = df_hyde_based_contexts['hyde_article'].notna().sum()\n",
    "    hyde_based_contexts_cnt = df_hyde_based_contexts['hyde_based_context'].notna().sum()\n",
    "\n",
    "    # Merge df_qas with df_hyde_based_contexts based on the 'qid' column\n",
    "    merged = df_qas.merge(df_hyde_based_contexts, on='qid', how='left', suffixes=('', '_df_hyde_based_contexts'))\n",
    "    df_qas['hyde_article'] = merged['hyde_article_df_hyde_based_contexts']\n",
    "\n",
    "    print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "    print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n",
    "\n",
    "    df_qas.to_csv(file_path, header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO --> add check if qas_with_contexts file already exists\n",
    "\n",
    "# The following is only needed if the qas_with_contexts file is missing or\n",
    "# corrupted or if the contexts need to be refreshed for any reason.\n",
    "\n",
    "if RESTORE_QAS_WITH_CONTEXTS:\n",
    "    restore_file_path = f\"{DATA_ROOT}/qas_with_contexts_k{VECTOR_STORE_TOP_K}_cs{CHUNK_SIZE}_{DEFAULT_CHUNKING_METHOD}.csv\"\n",
    "    restore_df_qas_with_contexts_file(restore_file_path, df_qas)\n",
    "\n",
    "# Refresh normal question contexts and hyde-based contexts\n",
    "# TODO: check if might be useful to first empty the two contexts columns in the dataframe\n",
    "if RESTORE_QAS_WITH_CONTEXTS or REFRESH_QUESTION_CONTEXTS or REFRESH_HYDE_CONTEXTS:\n",
    "    print(f\"Re-Generating contexts for the dataset and persisting the data...\")\n",
    "    list_of_qas = df_qas.to_dict(orient='records')\n",
    "\n",
    "    for i, qa in enumerate(list_of_qas):\n",
    "        # print(i)\n",
    "        if i %100 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        # Generating contexts based on questions (if necessary)\n",
    "        if RESTORE_QAS_WITH_CONTEXTS or REFRESH_QUESTION_CONTEXTS:\n",
    "            question = qa[\"question\"]\n",
    "            \n",
    "            top_docs = vector_store.similarity_search(\n",
    "                query = question,\n",
    "                k = VECTOR_STORE_TOP_K,\n",
    "            )\n",
    "            qa[\"question_context\"] = \"\\n\\n\".join([top_doc.page_content for top_doc in top_docs])\n",
    "\n",
    "        # Generating contexts based on Hyde articles (if necessary)\n",
    "        if RESTORE_QAS_WITH_CONTEXTS or REFRESH_HYDE_CONTEXTS:\n",
    "            hyde_article = qa[\"hyde_article\"]\n",
    "            #print(f\"hyde_article: {hyde_article}\")\n",
    "            if pd.isna(hyde_article):\n",
    "                hyde_article = \"\"\n",
    "            elif len(hyde_article) == 0:\n",
    "                hyde_article = \"\"\n",
    "            else:\n",
    "                top_docs = vector_store.similarity_search(\n",
    "                    query = hyde_article,\n",
    "                    k = VECTOR_STORE_TOP_K,\n",
    "                )\n",
    "\n",
    "                qa[\"hyde_based_context\"] = \"\\n\\n\".join([top_doc.page_content for top_doc in top_docs])\n",
    "\n",
    "    df_qas = pd.DataFrame(list_of_qas)\n",
    "    df_qas_with_contexts = df_qas.copy()\n",
    "    df_qas.to_csv(restore_file_path, header=True, index=False)\n",
    "else:\n",
    "    print(f\"Not restoring df_qas_with_contexts or refresh of the contexts...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show names of all dataframes\n",
    "\n",
    "Just for a general overview, generate all the dataframes that have been created up to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "print(\"Names of Dataframes and their lenghts:\\n\")\n",
    "global_keys_copy = copy.deepcopy(list(globals().keys()))\n",
    "\n",
    "my_l = [{n: len(globals()[n])} for n in global_keys_copy if n.startswith(\"df_\")]\n",
    "df_dfs = pd.DataFrame([(k, v) for d in my_l for k, v in d.items()], columns=['df name', 'rows'])\n",
    "df_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.) Functions for persisting Results (including histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp and format the start timestamp as a string\n",
    "def get_timestamp_as_string():\n",
    "    start_timestamp = datetime.now()\n",
    "    start_timestamp_str = start_timestamp.strftime('%Y%m%d_%H%M%S')\n",
    "    return start_timestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_folder(start_timestamp_str, experiment_name=None):\n",
    "\n",
    "    if experiment_name:\n",
    "        results_folder_name = f\"{RESULTS_ROOT}/{EXPERIMENT_GLOBAL_ID}_{start_timestamp_str}/{experiment_name}\"\n",
    "    else:\n",
    "        results_folder_name = f\"{RESULTS_ROOT}/{EXPERIMENT_GLOBAL_ID}_{start_timestamp_str}\"\n",
    "    if not os.path.exists(results_folder_name):\n",
    "        os.makedirs(results_folder_name, exist_ok=True)\n",
    "    return results_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_results(results_folder_name, experiment_name, df, eval_dict):\n",
    "\n",
    "    df.to_csv(f\"{results_folder_name}/qas_{experiment_name.lower()}_scores.csv\", header=True, index=False)\n",
    "    df[['qid', 'question', f\"{experiment_name.lower()}_llm_answer\"]].to_csv(f\"{results_folder_name}/qas_{experiment_name.lower()}_answers.csv\", header=True, index=False)\n",
    "   \n",
    "    json_file_name = f\"{results_folder_name}/qas_{experiment_name.lower()}_answers.json\"\n",
    "    df_json = df[['qid', f\"{experiment_name.lower()}_llm_answer\"]].rename(columns={f\"{experiment_name.lower()}_llm_answer\": \"answer\"})\n",
    "    answer_data = {row[\"qid\"]: row[\"answer\"] for _, row in df_json.iterrows()}\n",
    "    with open(json_file_name, \"w\") as json_file:\n",
    "        json.dump(answer_data, json_file, indent=4)\n",
    "\n",
    "    eval_dict_file_name = f\"{results_folder_name}/eval_dict_{experiment_name.lower()}.json\"\n",
    "    eval_data = {key: round(value, 2) for key, value in eval_dict.items()}\n",
    "    with open(eval_dict_file_name, \"w\") as json_file:\n",
    "        json.dump(eval_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a histogram for a list of scores and persist in the directory \"results_folder_name\"\n",
    "def generate_histogram(scores_l, mean, ci, results_folder_name, experiment_name):\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(scores_l, bins=30, density=False, edgecolor='black', alpha=0.6, color = 'lightblue' ) # color='aquamarine')\n",
    "    plt.xlim(0, 100)\n",
    "\n",
    "    plt.title(f\"F1-Scores for {experiment_name} - (Bootstraps: {BOOTSTRAPS_N:,} - Sample Size: {SAMPLE_SIZE:,})\", fontsize=10)\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    max_len = 6\n",
    "    mean_label = f\"{mean: .2f}\".rjust(max_len)\n",
    "    plt.axvline(mean, color='red', linestyle='dotted', linewidth=2, label=f'Mean F1:          {mean_label}')\n",
    "\n",
    "    # Add vertical lines for the 95% confidence interval\n",
    "    lower = f\"{ci[0]: .2f}\".rjust(max_len)\n",
    "    upper = f\"{ci[1]: .2f}\".rjust(max_len)\n",
    "    plt.axvline(ci[0], color='orange', linestyle='dashdot', linewidth=1.5, label=f\"95% CI Lower:  {lower}\")\n",
    "    plt.axvline(ci[1], color='orange', linestyle='dashdot', linewidth=1.5, label=f\"95% CI Upper:  {upper}\")\n",
    "\n",
    "    ax = plt.gca()  # Get current axis\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(10))  # Set major ticks every 10 units\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(5))   # Set minor ticks every 5 units\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=10))\n",
    "    \n",
    "    # Customize grid for major and minor ticks\n",
    "    ax.grid(which='major', color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', color='lightgray', linestyle=':', linewidth=0.5)\n",
    "\n",
    "    # Add a legend\n",
    "\n",
    "    # Add additional text to the legend\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    model_info_line = Line2D([0], [0], color='none', label=f\"LLM Model: {CHAT_MODEL_NAME}\")\n",
    "    labels.append(f\"LLM Model: {CHAT_MODEL_NAME}\")\n",
    "    handles.append(model_info_line)\n",
    "\n",
    "    if experiment_name == \"BASIC_RAG_SEMANTIC_CHUNKGING\":\n",
    "        labels.append(f\"K={VECTOR_STORE_TOP_K} / {DEFAULT_SEMANTIC_CHUNKING_METHOD}\")\n",
    "    else:\n",
    "        labels.append(f\"K={VECTOR_STORE_TOP_K} / {DEFAULT_CHUNKING_METHOD}\")\n",
    "    handles.append(model_info_line)\n",
    "\n",
    "    plt.legend(prop={'family': 'monospace', 'size': 9})\n",
    "    plt.legend(handles, labels, loc='upper right', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_folder_name, f\"{experiment_name}_{BOOTSTRAPS_N}_{SAMPLE_SIZE}\"))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_experiments_config(results_folder_name, experiment_name, query_prompt_idx, \n",
    "                               experiment_mean, experiment_ci, execution_time):\n",
    "\n",
    "    params_str = f\"Experiment: {experiment_name}\\n\"\n",
    "    params_str = params_str + f\"{datetime.now()}\\n\"\n",
    "    params_str = params_str + f\"Execution duration in minutes: {round(execution_time/60,2)}\\n\"\n",
    "    params_str = params_str + \"================================================\\n\\n\"\n",
    "\n",
    "    current_query_prompt_str = deh_prompts.query_prompts[query_prompt_idx].template\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + \"Prompt used:\\n\"\n",
    "    params_str = params_str + current_query_prompt_str + \"\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"LLM: {CHAT_MODEL_NAME}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"F1 mean: {round(experiment_mean, 2)}\\n\"\n",
    "    params_str = params_str + f\"95% CI ({round(experiment_ci[0], 2)}, {round(experiment_ci[1],2)})\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"DATA_ROOT = {DATA_ROOT}\\n\"\n",
    "    params_str = params_str + f\"RESULTS_ROOT = {RESULTS_ROOT}\\n\"\n",
    "    params_str = params_str + f\"HYDE_BASED_CONTEXTS_ROOT  = {HYDE_BASED_CONTEXTS_ROOT}\\n\"\n",
    "    params_str = params_str + f\"data_file = {DATA_ROOT}/dev-v2.0.json\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"CHROMA_ROOT               = {CHROMA_ROOT}\\n\"\n",
    "    params_str = params_str + f\"VECTOR_STORE_TOP_K        = {VECTOR_STORE_TOP_K}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_SIZE                = {CHUNK_SIZE}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_OVERLAP             = {CHUNK_OVERLAP}\\n\"\n",
    "    params_str = params_str + f\"DEFAULT_CHROMA_PREFIX     = {DEFAULT_CHROMA_PREFIX}\\n\"\n",
    "    params_str = params_str + f\"DEFAULT_CHUNKING_METHOD   = {DEFAULT_CHUNKING_METHOD}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_SQUAD_DATASET       = {CHUNK_SQUAD_DATASET}\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"REFRESH_QUESTION_CONTEXTS = {REFRESH_QUESTION_CONTEXTS}\\n\"\n",
    "    params_str = params_str + f\"REFRESH_HYDE_CONTEXTS     = {REFRESH_HYDE_CONTEXTS}\\n\"\n",
    "    params_str = params_str + f\"RESTORE_QAS_WITH_CONTEXTS = {RESTORE_QAS_WITH_CONTEXTS}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"        \n",
    "    params_str = params_str + f\"SAMPLE_SIZE  = {SAMPLE_SIZE}\\n\"\n",
    "    params_str = params_str + f\"BOOTSTRAPS_N = {BOOTSTRAPS_N}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_0  = {LLM_MODEL_NAME_0}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_1  = {LLM_MODEL_NAME_1}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_2  = {LLM_MODEL_NAME_2}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_3  = {LLM_MODEL_NAME_3}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_4  = {LLM_MODEL_NAME_4}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_5  = {LLM_MODEL_NAME_5}\\n\"\n",
    "    params_str = params_str + f\"CHAT_MODEL_NAME   = {CHAT_MODEL_NAME}\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"MAX_TOKENS        = {MAX_TOKENS}\\n\"\n",
    "    params_str = params_str + f\"TEMPERATURE       = {TEMPERATURE}\\n\"\n",
    "    params_str = params_str + f\"TOP_P             = {TOP_P}\\n\"\n",
    "    params_str = params_str + f\"FREQUENCY_PENALTY = {FREQUENCY_PENALTY}\\n\"\n",
    "    params_str = params_str + f\"PRESENCE_PENALTY  = {PRESENCE_PENALTY}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    judges = [llm_models[judge_i] for judge_i in judge_llms]\n",
    "    params_str = params_str + f\"Judge LLMS = {judges}\\n\"\n",
    "    params_str = params_str + f\"JUDGES_SUPPRESS_THRESHOLD   = {JUDGES_SUPPRESS_THRESHOLD}\\n\\n\"\n",
    "\n",
    "    with open(f\"{results_folder_name}/experiments_config_{experiment_name.lower()}.txt\", 'w') as f:\n",
    "        f.write(params_str)\n",
    "\n",
    "    print(\"Persisting the experiment config done...\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.) Functions for calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it exists, get the Hyde-based context for a given question\n",
    "def get_hyde_based_context(question):\n",
    "    hyde_based_context = df_qas_with_contexts[df_qas_with_contexts[\"question\"] == question]\n",
    "    if hyde_based_context.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return hyde_based_context[\"hyde_based_context\"].values[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# Get the metrics for a set of predictions (preds) that have been generated in a run\n",
    "# Note that preds may only contain a single LLM generated answer\n",
    "def get_squad_metrics(dataset, preds, verbose=False):\n",
    "    squad_metrics = squad_scoring.calc_squad_metrics(dataset, preds);\n",
    "    return squad_metrics[\"precision\"], squad_metrics[\"recall\"], squad_metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and confidence interval for a list of (F1) scores (scores_l)\n",
    "def calculate_mean_confidence_interval(scores_l):\n",
    "\n",
    "    # Calculate mean\n",
    "    mean = np.mean(scores_l)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    sample_std_dev = np.std(scores_l, ddof=1)\n",
    "    margin_of_error = 1.96 * sample_std_dev\n",
    "    ci = (max(mean - margin_of_error, 0), min(mean + margin_of_error, 100))\n",
    "\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LangChain runnable chain that first creates the prompt for the\n",
    "# current experiment and that invokes the llm with the prompt\n",
    "def get_runnable_chain(current_query_prompt, llm):\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "    return runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(qas, query_prompt_idx, experiment_name, context_needed=False,\n",
    "                     hyde_based_context_needed=False, suppress_answers=False):\n",
    "\n",
    "    # Create the chain\n",
    "    current_query_prompt = deh_prompts.query_prompts[query_prompt_idx]\n",
    "    print(f\"current_query_prompt = {current_query_prompt.template}\\n\")\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = get_runnable_chain(current_query_prompt, llm)\n",
    "\n",
    "    # Generate the LLM answers for all questions and calculate per-answer metrics \n",
    "    # add each answer to the all_preds\n",
    "    preds = {}\n",
    "    all_preds= {}\n",
    "    prefix = experiment_name.lower()\n",
    "\n",
    "    for i, qa in enumerate(qas):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        qid = qa[\"qid\"]\n",
    "        question = qa[\"question\"]\n",
    "\n",
    "        if context_needed or hyde_based_context_needed:\n",
    "            if not hyde_based_context_needed:\n",
    "                context = qa[\"question_context\"]\n",
    "            elif not context_needed:\n",
    "                context = qa[\"hyde_based_context\"]\n",
    "            else:\n",
    "                context = qa[\"question_context\"] + \"\\n\\n\" + qa[\"hyde_based_context\"]\n",
    "\n",
    "            response = runnable_chain.invoke({\"question\": question, \"context\": context})\n",
    "        else:\n",
    "            response = runnable_chain.invoke({\"question\": question})\n",
    "\n",
    "        llm_answer = response.content\n",
    "\n",
    "        if \"DONT KNOW\" in llm_answer.upper() or llm_answer.upper().startswith(\"NO CONTEXT PROVIDED\") or llm_answer.upper().startswith(\"NO INFORMATION GIVEN\"): # or llm_answer.upper().startswith(\"UNFORTUNATELY\"):     \n",
    "            llm_answer = \"\"\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        all_preds[qid] = llm_answer\n",
    "\n",
    "        scores = squad_scoring.calc_squad_metrics(dataset, preds)\n",
    "        f1 = scores[\"f1\"]\n",
    "        precision = scores[\"precision\"]\n",
    "        recall = scores[\"recall\"]\n",
    "        jaccard = scores[\"jaccard\"]\n",
    "\n",
    "        preds = {}\n",
    "        qa[f\"{prefix}_llm_answer\"] = llm_answer\n",
    "        qa[f\"{prefix}_f1\"] = f1\n",
    "        qa[f\"{prefix}_precision\"] = precision\n",
    "        qa[f\"{prefix}_recall\"] = recall\n",
    "        qa[f\"{prefix}_jaccard\"] = jaccard\n",
    "\n",
    "    if suppress_answers:\n",
    "        threw_away_answer = False\n",
    "        threw_away_answers_cnt = 0\n",
    "        preds = {}\n",
    "\n",
    "        final_judges_verdicts = get_final_judges_verdicts(qas, experiment_name, hyde_context_needed)\n",
    "\n",
    "        for i, qa in enumerate(qas):\n",
    "\n",
    "            qid = qa[\"qid\"]\n",
    "            question = qa[\"question\"]\n",
    "\n",
    "            if final_judges_verdicts[i][0] == \"NO\":\n",
    "                if not qa[f\"{prefix}_llm_answer\"] == \"\":\n",
    "                    threw_away_answer = True\n",
    "                    threw_away_answers_cnt += 1\n",
    "                    print(f\"Threw away answer for question {i}: {qa['question']}\")\n",
    "                qa[f\"{prefix}_llm_answer\"] = \"\"\n",
    "            else:\n",
    "                if final_judges_verdicts[i][1] < JUDGES_SUPPRESS_THRESHOLD:\n",
    "                    if not qa[f\"{prefix}_llm_answer\"] == \"\":\n",
    "                        threw_away_answer = True\n",
    "                        threw_away_answers_cnt += 1\n",
    "                        print(f\"Threw away answer for question {i}: {qa['question']}\")\n",
    "                    qa[f\"{prefix}_llm_answer\"] = \"\"\n",
    "\n",
    "            if threw_away_answer:\n",
    "\n",
    "                preds[qid] = \"\"\n",
    "                all_preds[qid] = \"\"\n",
    "                scores = squad_scoring.calc_squad_metrics(dataset, preds)\n",
    "                f1 = scores[\"f1\"]\n",
    "                precision = scores[\"precision\"]\n",
    "                recall = scores[\"recall\"]\n",
    "                jaccard = scores[\"jaccard\"]\n",
    "\n",
    "                preds = {}\n",
    "                qa[f\"{prefix}_llm_answer\"] = \"\"\n",
    "                qa[f\"{prefix}_f1\"] = f1\n",
    "                qa[f\"{prefix}_precision\"] = precision\n",
    "                qa[f\"{prefix}_recall\"] = recall\n",
    "                qa[f\"{prefix}_jaccard\"] = jaccard     \n",
    "\n",
    "            threw_away_answer = False   \n",
    "\n",
    "        print(f\"Threw away {threw_away_answers_cnt} answers.\")\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.) Functions that implement LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_answer_tuple(input_string):\n",
    "    \n",
    "    # Trim whitespace\n",
    "    parts = input_string.strip()\n",
    "    # Remove parentheses\n",
    "    parts = parts.strip(\"()\")\n",
    "    # Split on comma\n",
    "    parts = parts.split(\", \")\n",
    "    if len(parts) != 2:\n",
    "        return (\"NO\", 0)\n",
    "\n",
    "    answer = parts[0].strip().upper()  # \"Yes\" or \"No\"\n",
    "    if answer not in [\"YES\", \"NO\"]:\n",
    "        return (\"NO\", 0)\n",
    "    try:\n",
    "        score = float(parts[1])  # Convert score to a float\n",
    "    except ValueError:\n",
    "        return (\"NO\", 0)\n",
    "    \n",
    "    return (answer, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_verdict(judge_verdicts):\n",
    "\n",
    "    judge_answers = judge_verdicts[0]\n",
    "    yes_count = judge_answers.count('YES')\n",
    "    no_count = judge_answers.count('NO')\n",
    "\n",
    "    if no_count > yes_count:\n",
    "        return \"NO\", 0.0\n",
    "    \n",
    "    judge_scores = list(judge_verdicts[1])\n",
    "    return \"YES\", sum(judge_scores) / yes_count\n",
    "\n",
    "def get_majority_verditcs(all_judge_answers, all_judge_scores):\n",
    "\n",
    "    majority_verdicts = []\n",
    "    for per_question_judge_verdicts in zip(zip(*all_judge_answers), zip(*all_judge_scores)):\n",
    "        majority_verdict = get_majority_verdict(per_question_judge_verdicts)\n",
    "        majority_verdicts.append(majority_verdict)\n",
    "\n",
    "    return majority_verdicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_judges_verdicts(sample_ldict, experiment_name, hyde_context_needed=False):\n",
    "    judge_current_query_prompt = deh_prompts.query_prompts[9]\n",
    "    #judge_current_query_prompt = deh_prompts.BASIC_RAG_LLM_AS_A_JUDGE_PROMPT\n",
    "    print(f\"judge_current_query_prompt = {judge_current_query_prompt.template}\\n\")\n",
    "\n",
    "\n",
    "    for ele in sample_ldict:\n",
    "        print(ele)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    all_judge_answers = []\n",
    "    all_judge_scores = []\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    for judge_i, judge_llm in enumerate(judge_llms):\n",
    "        judge_chain = judge_current_query_prompt | get_llm(judge_current_query_prompt, True, judge_llm)\n",
    "        judge_answers = []\n",
    "        judge_scores = []\n",
    "\n",
    "        for i, qa in enumerate(sample_ldict):\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processing question {i} for judge {judge_i}...\")\n",
    "\n",
    "            question = qa[\"question\"]\n",
    "            context = qa[\"question_context\"]\n",
    "            if hyde_context_needed:\n",
    "                hyde_based_context = qa[\"hyde_based_context\"]\n",
    "                context = \"\\n\\n\".join([context, hyde_based_context])\n",
    "            answer_key = experiment_name.lower() + \"_llm_answer\"\n",
    "            answer = qa[answer_key]\n",
    "\n",
    "            response = judge_chain.invoke({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "            a, s = convert_string_to_answer_tuple(response.content)\n",
    "            judge_answers.append(a)\n",
    "            judge_scores.append(s)\n",
    "            \n",
    "        all_judge_answers.append(judge_answers)\n",
    "        all_judge_scores.append(judge_scores)\n",
    "\n",
    "    final_judges_verdicts = get_majority_verditcs(all_judge_answers, all_judge_scores)\n",
    "    print(\"Printing final_judges_verdicts:\")\n",
    "    for final_verdict in final_judges_verdicts:\n",
    "        print(final_verdict)\n",
    "\n",
    "    return final_judges_verdicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.) Functions for Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_scores(scores_l, clip_perc):\n",
    "    scores_sorted_l = sorted(scores_l)\n",
    "\n",
    "    clip_cnt = int(len(scores_sorted_l) * clip_perc / 100)\n",
    "    print(f\"clip_cnt: {clip_cnt}\")\n",
    "\n",
    "    # Now clip both ends by clip_perc percent\n",
    "    clipped_scores_l = scores_sorted_l[clip_cnt:-clip_cnt] if clip_cnt > 0 else scores_sorted_l\n",
    "    return clipped_scores_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bootstrapping(scores_l, results_folder_name, experiment_name, bootstraps_n = BOOTSTRAPS_N):\n",
    "    \n",
    "    mu_hats = []\n",
    "    n = len(scores_l)\n",
    "    scores_l = clip_scores(scores_l, 2.5)\n",
    "\n",
    "    for i in range(bootstraps_n):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processing sample {i}...\")\n",
    "        bootstrap_sample = random.choices(scores_l, k=n) # sample with replacement\n",
    "        mu_hat = np.mean(bootstrap_sample)\n",
    "        mu_hats.append(mu_hat)\n",
    "\n",
    "    bootstraps_mean, ci = calculate_mean_confidence_interval(mu_hats)\n",
    "    plt = generate_histogram(mu_hats, bootstraps_mean, ci, results_folder_name, experiment_name)\n",
    "    plt.show();\n",
    "    return bootstraps_mean, ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.) Function for running one experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(qas, start_timestamp_str, experiment_name, query_prompt_idx, \n",
    "                   context_needed=False, hyde_context_needed=False, \n",
    "                   suppress_answers=False):\n",
    "\n",
    "    exp_start_time = time.perf_counter()\n",
    "    results_folder_name = create_results_folder(start_timestamp_str, experiment_name)\n",
    "\n",
    "    print(f\"\\n\\n============= Now running experiment {experiment_name} =============\\n\")\n",
    "    \n",
    "    print(f\"============= Calculating scores for {experiment_name} =============\\n\")\n",
    "    print(f\"SAMPLE_SIZE: {SAMPLE_SIZE}\\n\")\n",
    "    all_preds = calculate_scores(qas, query_prompt_idx, experiment_name, context_needed, \n",
    "                                 hyde_context_needed, suppress_answers)\n",
    "\n",
    "    eval_dict = squad_scoring.calc_squad_metrics(dataset, all_preds)\n",
    "    print(f\"\\n================= SQuAD Scores:==================================\\n\")\n",
    "    for key, value in eval_dict.items():\n",
    "        print(f\"{key}: {round(value,2)}\")\n",
    "        if \"total\" in key:\n",
    "            print(\"\")\n",
    "                \n",
    "    print(f\"============= Persisting results for {experiment_name} =============\\n\")\n",
    "    df = pd.DataFrame(qas)\n",
    "    persist_results(results_folder_name, experiment_name, df, eval_dict)\n",
    "    \n",
    "    print(f\"============= Bootstrapping for {experiment_name} =============\\n\")\n",
    "    print(f\"BOOTSTRAPS_N: {BOOTSTRAPS_N}\")\n",
    "\n",
    "    experiment_mean, experiment_ci = do_bootstrapping(df[f\"{experiment_name.lower()}_f1\"].dropna().tolist(), results_folder_name, experiment_name, BOOTSTRAPS_N)\n",
    "\n",
    "    # print(f\"============= Get RAGAS metrics for {experiment_name} =============\\n\")\n",
    "    # TODO: add RAGAS metrics to the notebook\n",
    "    \n",
    "    # generate_ragas_metrics(df, experiment_name, query_prompt_idx, results_folder_name)\n",
    "\n",
    "    exp_end_time = time.perf_counter()\n",
    "    execution_time = exp_end_time - exp_start_time\n",
    "    execution_times_entry = {}\n",
    "    execution_times_entry[\"experiment_name\"] = experiment_name\n",
    "    execution_times_entry[\"execution_time\"] = execution_time\n",
    "    execution_times_entry[\"sample_size\"] = SAMPLE_SIZE\n",
    "    execution_times_entry[\"bootstrap_n\"] = BOOTSTRAPS_N\n",
    "    execution_times_l.append(execution_times_entry)\n",
    "\n",
    "    print(f\"Will now persist the experiment configuration for {experiment_name}...\")\n",
    "    persist_experiments_config(results_folder_name, experiment_name, query_prompt_idx, \n",
    "                               experiment_mean, experiment_ci, execution_time)\n",
    "\n",
    "    return execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII.) Creating samples for the experiment\n",
    "\n",
    "Creating a sample from df_qas_with_contexts for bootstrapping and bootstrapping with Hyde\n",
    "\n",
    "To do bootstrapping, a random sample of size SAMPLE_SIZE from is drawn from the datarame \"df_qas_with_contexts\". There are three different sub-types of samples, depending on the type of the experiment:\n",
    "\n",
    "- one \"normal\" sample (df_qas_for_bootstrapping_sample)\n",
    "- one specific sample for Hyde (df_qas_for_hyde_bootstrapping_sample)\n",
    "- one specific sample if semantic chunking is used (ldict_qas_for_boostrapping_sample_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: names of samples are far too long; make them much shorter\n",
    "# TODO: check if separate sample for semantic chunking is really necessary\n",
    "\n",
    "df_qas_for_bootstrapping = df_qas_with_contexts[['qid', 'question', 'question_context', 'hyde_based_context']]\n",
    "df_qas_for_bootstrapping_sample = df_qas_for_bootstrapping.sample(n=SAMPLE_SIZE, replace=True)  #, random_state=42)\n",
    "\n",
    "ldict_qas_for_boostrapping_sample = df_qas_for_bootstrapping_sample.to_dict(orient='records')\n",
    "df_qas_for_bootstrapping_semantic = df_qas_with_contexts_semantic[['qid', 'question', 'question_context', 'hyde_based_context']]\n",
    "\n",
    "# TODO: There is surely a better way to implement this with dataframe indexing, but for the moment\n",
    "# I'll just keep it as is...\n",
    "sample_idx = set(df_qas_for_bootstrapping_sample.index)\n",
    "ldict_qas_for_boostrapping_sample_semantic = []\n",
    "for index, row in df_qas_for_bootstrapping_semantic.iterrows():\n",
    "    if index in sample_idx:\n",
    "        ldict_qas_for_boostrapping_sample_semantic.append(row.to_dict())\n",
    "\n",
    "df_qas_for_hyde_bootstrapping = df_qas_for_bootstrapping[df_qas_for_bootstrapping['hyde_based_context'].notna()]\n",
    "df_qas_for_hyde_bootstrapping_sample = df_qas_for_hyde_bootstrapping.sample(n=SAMPLE_SIZE, replace=True) #, random_state=42)\n",
    "ldict_qas_for_hyde_bootstrapping_sample = df_qas_for_hyde_bootstrapping_sample.to_dict(orient='records')   \n",
    "\n",
    "sample_ldicts = [ldict_qas_for_boostrapping_sample, \n",
    "                 ldict_qas_for_hyde_bootstrapping_sample,\n",
    "                 ldict_qas_for_boostrapping_sample_semantic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX.) Running the current experiment\n",
    "\n",
    "Runs one single experiment in the current version of the notebook, but can be easily modified to run a series of experiments in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEMPERATURE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEMPERATURE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJUDGES_SUPPRESS_THRESHOLD: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mJUDGES_SUPPRESS_THRESHOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_ldict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_timestamp_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_prompt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_needed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyde_context_needed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress_answers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================== Execution times (in seconds): ==================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m df_execution_times \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(execution_times_l, \n\u001b[1;32m     28\u001b[0m                                 columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap_n\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(qas, start_timestamp_str, experiment_name, query_prompt_idx, context_needed, hyde_context_needed, suppress_answers)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============= Calculating scores for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =============\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAMPLE_SIZE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAMPLE_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_prompt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_needed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mhyde_context_needed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress_answers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m eval_dict \u001b[38;5;241m=\u001b[39m squad_scoring\u001b[38;5;241m.\u001b[39mcalc_squad_metrics(dataset, all_preds)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================= SQuAD Scores:==================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 62\u001b[0m, in \u001b[0;36mcalculate_scores\u001b[0;34m(qas, query_prompt_idx, experiment_name, context_needed, hyde_based_context_needed, suppress_answers)\u001b[0m\n\u001b[1;32m     59\u001b[0m threw_away_answers_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     60\u001b[0m preds \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 62\u001b[0m final_judges_verdicts \u001b[38;5;241m=\u001b[39m \u001b[43mget_final_judges_verdicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyde_context_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, qa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(qas):\n\u001b[1;32m     66\u001b[0m     qid \u001b[38;5;241m=\u001b[39m qa[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[23], line 33\u001b[0m, in \u001b[0;36mget_final_judges_verdicts\u001b[0;34m(sample_ldict, experiment_name, hyde_context_needed)\u001b[0m\n\u001b[1;32m     30\u001b[0m answer_key \u001b[38;5;241m=\u001b[39m experiment_name\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_llm_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m answer \u001b[38;5;241m=\u001b[39m qa[answer_key]\n\u001b[0;32m---> 33\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mjudge_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m a, s \u001b[38;5;241m=\u001b[39m convert_string_to_answer_tuple(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     35\u001b[0m judge_answers\u001b[38;5;241m.\u001b[39mappend(a)\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_ollama/chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_ollama/chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_ollama/chat_models.py:527\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    518\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    519\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         tools\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    528\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    529\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[1;32m    530\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m         options\u001b[38;5;241m=\u001b[39mOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    532\u001b[0m         keep_alive\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/ollama/_client.py:80\u001b[0m, in \u001b[0;36mClient._stream\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m---> 80\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m      \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_client.py:880\u001b[0m, in \u001b[0;36mClient.stream\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;124;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    868\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    869\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    879\u001b[0m )\n\u001b[0;32m--> 880\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_timestamp_str = get_timestamp_as_string()\n",
    "results_folder_name = create_results_folder(start_timestamp_str)\n",
    "\n",
    "exp = current_experiment\n",
    "\n",
    "sample_ldict = sample_ldicts[exp[\"sample_ldicts_idx\"]]\n",
    "experiment_name = exp[\"name\"]\n",
    "query_prompt_idx = exp[\"query_prompt_idx\"]\n",
    "context_needed = exp[\"context_needed\"]\n",
    "hyde_context_needed = exp[\"hyde_context_needed\"]\n",
    "suppress_answers = exp[\"suppress_answers\"]\n",
    "\n",
    "print(f\"experiment_name: {experiment_name}\")\n",
    "print(f\"query_prompt_idx: {query_prompt_idx}\")\n",
    "print(f\"context_needed: {context_needed}\")\n",
    "print(f\"hyde_context_needed: {hyde_context_needed}\")\n",
    "print(f\"suppress_answers: {suppress_answers}\")\n",
    "print(f\"CHAT_MODEL_NAME: {CHAT_MODEL_NAME}\")\n",
    "print(f\"VECTOR_STORE_TOP_K: {VECTOR_STORE_TOP_K}\")\n",
    "print(f\"TEMPERATURE: {TEMPERATURE}\")\n",
    "print(f\"JUDGES_SUPPRESS_THRESHOLD: {JUDGES_SUPPRESS_THRESHOLD}\")\n",
    "\n",
    "run_experiment(sample_ldict, start_timestamp_str, experiment_name, query_prompt_idx,\n",
    "            context_needed, hyde_context_needed, suppress_answers)\n",
    "\n",
    "print(f\"\\n================== Execution times (in seconds): ==================================\\n\")\n",
    "df_execution_times = pd.DataFrame(execution_times_l, \n",
    "                                columns=['experiment_name', 'execution_time', 'sample_size', 'bootstrap_n'])\n",
    "\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "df_execution_times.head(100)              \n",
    "\n",
    "print(f\"Total execution time in Minutes: {round(sum(df_execution_times['execution_time']) / 60, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the display format for floats\n",
    "pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total execution time in Minutes: {round(sum(df_execution_times['execution_time']) / 60, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
