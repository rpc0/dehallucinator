{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for deh experiments specific modules\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\".\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import squad_scoring\n",
    "import deh_prompts\n",
    "import deh_vector_store\n",
    "import deh_squad_data\n",
    "import deh_hyde\n",
    "import deh_experiments_config\n",
    "globals().update(deh_experiments_config.__dict__)\n",
    "import deh_globals\n",
    "globals().update(deh_globals.__dict__)\n",
    "# from deh_llm import get_llm\n",
    "import deh_llm\n",
    "globals().update(deh_llm.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading SQuAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading squad data...\n",
      "\n",
      "Number of raw entries in squad_raw: 26232\n",
      "Number of unique titles: 35\n",
      "Number of unique contexts: 1204\n",
      "Number of unique questions: 11858\n",
      "Number of unique answers: 16209\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/qas_with_contexts.csv\"\n",
    "\n",
    "print(f\"Loading squad data...\\n\")\n",
    "\n",
    "column_names = [\"title\", \"squad_context\", \"qid\", \"question\", \"is_impossible\", \"answer\"]\n",
    "squad_raw = pd.read_csv(f\"{DATA_ROOT}/squad_raw.csv\", names=column_names, skiprows=1)\n",
    "df_squad_raw = pd.DataFrame(squad_raw)\n",
    "print(f\"Number of raw entries in squad_raw: {len(df_squad_raw)}\")\n",
    "\n",
    "df_titles = pd.DataFrame(df_squad_raw['title'].unique(), columns=[\"title\"])\n",
    "print(f\"Number of unique titles: {len(df_titles)}\")\n",
    "\n",
    "df_contexts = pd.DataFrame(df_squad_raw['squad_context'].unique(), columns=[\"squad_context\"])\n",
    "print(f\"Number of unique contexts: {len(df_contexts)}\")\n",
    "\n",
    "df_qas = df_squad_raw[['title', 'squad_context', 'qid', 'question', 'is_impossible']].drop_duplicates()\n",
    "df_qas = df_qas.reset_index(drop=True)\n",
    "print(f\"Number of unique questions: {len(df_qas)}\")\n",
    "\n",
    "df_squad_answers = df_squad_raw[['qid', 'question', 'answer']].drop_duplicates()\n",
    "print(f\"Number of unique answers: {len(df_squad_answers)}\")           \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus not yet included); if configured, re-chunk the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating contexts for the dataset...\n",
      "Chunking method: pseudo_semantic\n",
      "Number of chunks --> 1204\n",
      "\n",
      "Adding chunk 0 to the vector store...\n",
      "Adding chunk 10 to the vector store...\n",
      "Adding chunk 20 to the vector store...\n",
      "Adding chunk 30 to the vector store...\n",
      "Adding chunk 40 to the vector store...\n",
      "Adding chunk 50 to the vector store...\n",
      "Adding chunk 60 to the vector store...\n",
      "Adding chunk 70 to the vector store...\n",
      "Adding chunk 80 to the vector store...\n",
      "Adding chunk 90 to the vector store...\n",
      "Adding chunk 100 to the vector store...\n",
      "Adding chunk 110 to the vector store...\n",
      "Adding chunk 120 to the vector store...\n",
      "Adding chunk 130 to the vector store...\n",
      "Adding chunk 140 to the vector store...\n",
      "Adding chunk 150 to the vector store...\n",
      "Adding chunk 160 to the vector store...\n",
      "Adding chunk 170 to the vector store...\n",
      "Adding chunk 180 to the vector store...\n",
      "Adding chunk 190 to the vector store...\n",
      "Adding chunk 200 to the vector store...\n",
      "Adding chunk 210 to the vector store...\n",
      "Adding chunk 220 to the vector store...\n",
      "Adding chunk 230 to the vector store...\n",
      "Adding chunk 240 to the vector store...\n",
      "Adding chunk 250 to the vector store...\n",
      "Adding chunk 260 to the vector store...\n",
      "Adding chunk 270 to the vector store...\n",
      "Adding chunk 280 to the vector store...\n",
      "Adding chunk 290 to the vector store...\n",
      "Adding chunk 300 to the vector store...\n",
      "Adding chunk 310 to the vector store...\n",
      "Adding chunk 320 to the vector store...\n",
      "Adding chunk 330 to the vector store...\n",
      "Adding chunk 340 to the vector store...\n",
      "Adding chunk 350 to the vector store...\n",
      "Adding chunk 360 to the vector store...\n",
      "Adding chunk 370 to the vector store...\n",
      "Adding chunk 380 to the vector store...\n",
      "Adding chunk 390 to the vector store...\n",
      "Adding chunk 400 to the vector store...\n",
      "Adding chunk 410 to the vector store...\n",
      "Adding chunk 420 to the vector store...\n",
      "Adding chunk 430 to the vector store...\n",
      "Adding chunk 440 to the vector store...\n",
      "Adding chunk 450 to the vector store...\n",
      "Adding chunk 460 to the vector store...\n",
      "Adding chunk 470 to the vector store...\n",
      "Adding chunk 480 to the vector store...\n",
      "Adding chunk 490 to the vector store...\n",
      "Adding chunk 500 to the vector store...\n",
      "Adding chunk 510 to the vector store...\n",
      "Adding chunk 520 to the vector store...\n",
      "Adding chunk 530 to the vector store...\n",
      "Adding chunk 540 to the vector store...\n",
      "Adding chunk 550 to the vector store...\n",
      "Adding chunk 560 to the vector store...\n",
      "Adding chunk 570 to the vector store...\n",
      "Adding chunk 580 to the vector store...\n",
      "Adding chunk 590 to the vector store...\n",
      "Adding chunk 600 to the vector store...\n",
      "Adding chunk 610 to the vector store...\n",
      "Adding chunk 620 to the vector store...\n",
      "Adding chunk 630 to the vector store...\n",
      "Adding chunk 640 to the vector store...\n",
      "Adding chunk 650 to the vector store...\n",
      "Adding chunk 660 to the vector store...\n",
      "Adding chunk 670 to the vector store...\n",
      "Adding chunk 680 to the vector store...\n",
      "Adding chunk 690 to the vector store...\n",
      "Adding chunk 700 to the vector store...\n",
      "Adding chunk 710 to the vector store...\n",
      "Adding chunk 720 to the vector store...\n",
      "Adding chunk 730 to the vector store...\n",
      "Adding chunk 740 to the vector store...\n",
      "Adding chunk 750 to the vector store...\n",
      "Adding chunk 760 to the vector store...\n",
      "Adding chunk 770 to the vector store...\n",
      "Adding chunk 780 to the vector store...\n",
      "Adding chunk 790 to the vector store...\n",
      "Adding chunk 800 to the vector store...\n",
      "Adding chunk 810 to the vector store...\n",
      "Adding chunk 820 to the vector store...\n",
      "Adding chunk 830 to the vector store...\n",
      "Adding chunk 840 to the vector store...\n",
      "Adding chunk 850 to the vector store...\n",
      "Adding chunk 860 to the vector store...\n",
      "Adding chunk 870 to the vector store...\n",
      "Adding chunk 880 to the vector store...\n",
      "Adding chunk 890 to the vector store...\n",
      "Adding chunk 900 to the vector store...\n",
      "Adding chunk 910 to the vector store...\n",
      "Adding chunk 920 to the vector store...\n",
      "Adding chunk 930 to the vector store...\n",
      "Adding chunk 940 to the vector store...\n",
      "Adding chunk 950 to the vector store...\n",
      "Adding chunk 960 to the vector store...\n",
      "Adding chunk 970 to the vector store...\n",
      "Adding chunk 980 to the vector store...\n",
      "Adding chunk 990 to the vector store...\n",
      "Adding chunk 1000 to the vector store...\n",
      "Adding chunk 1010 to the vector store...\n",
      "Adding chunk 1020 to the vector store...\n",
      "Adding chunk 1030 to the vector store...\n",
      "Adding chunk 1040 to the vector store...\n",
      "Adding chunk 1050 to the vector store...\n",
      "Adding chunk 1060 to the vector store...\n",
      "Adding chunk 1070 to the vector store...\n",
      "Adding chunk 1080 to the vector store...\n",
      "Adding chunk 1090 to the vector store...\n",
      "Adding chunk 1100 to the vector store...\n",
      "Adding chunk 1110 to the vector store...\n",
      "Adding chunk 1120 to the vector store...\n",
      "Adding chunk 1130 to the vector store...\n",
      "Adding chunk 1140 to the vector store...\n",
      "Adding chunk 1150 to the vector store...\n",
      "Adding chunk 1160 to the vector store...\n",
      "Adding chunk 1170 to the vector store...\n",
      "Adding chunk 1180 to the vector store...\n",
      "Adding chunk 1190 to the vector store...\n",
      "Adding chunk 1200 to the vector store...\n"
     ]
    }
   ],
   "source": [
    "contexts = list(df_contexts[\"squad_context\"].values)\n",
    "if CHUNK_SQUAD_DATASET:    \n",
    "    deh_vector_store.chunk_squad_dataset(contexts, dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "else:\n",
    "    print(\"Chunking not foreseen. Skipping chunking.\")\n",
    "\n",
    "# Intiialize the Chroma vector store\n",
    "vector_store = deh_vector_store.get_vector_store(DEFAULT_CHROMA_PREFIX, DEFAULT_CHUNKING_METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading qas with contexts data (if data is not to be restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not restoring qas_with_contexts from the CSV file, then read\n",
    "# the data from the csv file (i.e. it exists and is correct)\n",
    "if not RESTORE_QAS_WITH_CONTEXTS:\n",
    "    # Loading the question contexts from the CSV file\n",
    "    print(f\"Loading qas_with_contexts from the CSV file: {csv_file_path}\")\n",
    "    qas_with_contexts_csv_file_path = f\"{DATA_ROOT}/qas_with_contexts.csv\"\n",
    "\n",
    "    df_qas_with_contexts = pd.read_csv(qas_with_contexts_csv_file_path) #, names=column_names)\n",
    "\n",
    "    # drop the answer column if it exists, since it leads to duplicates\n",
    "    if 'answer' in df_qas_with_contexts.columns:\n",
    "        df_qas_with_contexts = df_qas_with_contexts.drop(columns=['answer'])\n",
    "\n",
    "    df_qas_with_contexts = df_qas_with_contexts.drop_duplicates()\n",
    "    print(f\"Rows in dataframe df_qas_with_contexts: {len(df_qas_with_contexts)}\")\n",
    "    hyde_articles_cnt = df_qas_with_contexts['hyde_article'].notna().sum()\n",
    "    hyde_based_contexts_cnt = df_qas_with_contexts['hyde_based_context'].notna().sum()\n",
    "    print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "    print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n",
    "else:\n",
    "    print(f\"RESTORE_QAS_WITH_CONTEXTS is True. Skipping loading qas_with_contexts from the CSV file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restore qas with contexts (if configured); alternatively refreseh contexts in existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_df_qas_with_contexts_file(csv_file_path, df_qas):\n",
    "\n",
    "    # Add columns hyde_article, question_context, hyde_based_context to df_qas_with_contexts\n",
    "    if not 'hyde_article' in df_qas.columns:\n",
    "        df_qas['hyde_article'] = np.nan\n",
    "    if not 'question_context' in df_qas.columns:\n",
    "        df_qas['question_context'] = np.nan\n",
    "    if not 'hyde_based_context' in df_qas.columns:\n",
    "        df_qas['hyde_based_context'] = np.nan\n",
    "\n",
    "    # Get Hyde data\n",
    "    hyde_based_context_path = f\"{HYDE_BASED_CONTEXTS_ROOT}/hyde_based_contexts.csv\"\n",
    "    df_hyde_based_contexts = pd.read_csv(hyde_based_context_path)\n",
    "\n",
    "    print(f\"Rows in dataframe df_hyde_based_contexts: {len(df_hyde_based_contexts)}\")\n",
    "    hyde_articles_cnt = df_hyde_based_contexts['hyde_article'].notna().sum()\n",
    "    hyde_based_contexts_cnt = df_hyde_based_contexts['hyde_based_context'].notna().sum()\n",
    "\n",
    "    # Merge df_qas with df_hyde_based_contexts based on the 'qid' column\n",
    "    merged = df_qas.merge(df_hyde_based_contexts, on='qid', how='left', suffixes=('', '_df_hyde_based_contexts'))\n",
    "    df_qas['hyde_article'] = merged['hyde_article_df_hyde_based_contexts']\n",
    "\n",
    "    print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "    print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n",
    "\n",
    "    df_qas.to_csv(csv_file_path, header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO an df_qas_with_contexts anpassen !!!!!!!!\n",
    "\n",
    "if RESTORE_QAS_WITH_CONTEXTS:\n",
    "    restore_df_qas_with_contexts_file(csv_file_path, df_qas)\n",
    "\n",
    "# Refresh question contexts (normal contexts and hyde-based contexts)\n",
    "# TODO: might be useful to first empty the two columns in the dataframe\n",
    "if RESTORE_QAS_WITH_CONTEXTS or REFRESH_QUESTION_CONTEXTS or REFRESH_HYDE_CONTEXTS:\n",
    "    print(f\"Re-Generating contexts for the dataset and persisting the data...\")\n",
    "    list_of_qas = df_qas.to_dict(orient='records')\n",
    "\n",
    "    for i, qa in enumerate(list_of_qas):\n",
    "        # print(i)\n",
    "        if i %100 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        if RESTORE_QAS_WITH_CONTEXTS or REFRESH_QUESTION_CONTEXTS:\n",
    "            question = qa[\"question\"]\n",
    "            \n",
    "            top_docs = vector_store.similarity_search(\n",
    "                query = question,\n",
    "                k = VECTOR_STORE_TOP_K,\n",
    "            )\n",
    "            qa[\"question_context\"] = \" \".join([top_doc.page_content for top_doc in top_docs])\n",
    "\n",
    "        if RESTORE_QAS_WITH_CONTEXTS or REFRESH_HYDE_CONTEXTS:\n",
    "            hyde_article = qa[\"hyde_article\"]\n",
    "            #print(f\"hyde_article: {hyde_article}\")\n",
    "            if pd.isna(hyde_article):\n",
    "                hyde_article = \"\"\n",
    "            elif len(hyde_article) == 0:\n",
    "                hyde_article = \"\"\n",
    "            else:\n",
    "                top_docs = vector_store.similarity_search(\n",
    "                    query = hyde_article,\n",
    "                    k = VECTOR_STORE_TOP_K,\n",
    "                )\n",
    "\n",
    "                qa[\"hyde_based_context\"] = \" \".join([top_doc.page_content for top_doc in top_docs])\n",
    "\n",
    "    df_qas = pd.DataFrame(list_of_qas)\n",
    "    df_qas_with_contexts = df_qas.copy()\n",
    "    df_qas.to_csv(csv_file_path, header=True, index=False)\n",
    "else:\n",
    "    print(f\"Not restoring df_qas_with_contexts or refresh of the contexts...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show names of all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "print(\"Names of Dataframes and their lenghts:\\n\")\n",
    "global_keys_copy = copy.deepcopy(list(globals().keys()))\n",
    "\n",
    "my_l = [{n: len(globals()[n])} for n in global_keys_copy if n.startswith(\"df_\")]\n",
    "df_dfs = pd.DataFrame([(k, v) for d in my_l for k, v in d.items()], columns=['df name', 'rows'])\n",
    "df_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_qas_with_contexts = df_qas.copy()\n",
    "# df_qas_with_contexts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions that are needed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runnable chain\n",
    "def get_runnable_chain(current_query_prompt, llm):\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "    return runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hyde context for a question\n",
    "def get_hyde_based_context(question):\n",
    "    hyde_based_context = df_qas_with_contexts[df_qas_with_contexts[\"question\"] == question]\n",
    "    if hyde_based_context.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return hyde_based_context[\"hyde_based_context\"].values[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# Get the metrics for a set of predictions (preds) that have been generated in a run\n",
    "def get_squad_metrics(dataset, preds, verbose=False):\n",
    "    squad_metrics = squad_scoring.calc_squad_metrics(dataset, preds);\n",
    "    return squad_metrics[\"precision\"], squad_metrics[\"recall\"], squad_metrics[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and confidence interval for a list of scores\n",
    "# TODO: Check if this is calculation is correct !!\n",
    "def calculate_mean_confidence_interval(scores_l):\n",
    "\n",
    "    # Calculate mean\n",
    "    mean = np.mean(scores_l)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    sample_std_dev = np.std(scores_l, ddof=1)\n",
    "    margin_of_error = 1.96 * sample_std_dev\n",
    "    ci = (max(mean - margin_of_error, 0), min(mean + margin_of_error, 100))\n",
    "\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram for a list of scores and persist it\n",
    "def generate_histogram(scores_l, mean, ci, results_folder_name, experiment_name):\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(scores_l, bins=30, density=False, edgecolor='black', alpha=0.6, color = 'lightblue' ) # color='aquamarine')\n",
    "    plt.xlim(0, 100)\n",
    "\n",
    "    plt.title(f\"F1-Scores for {experiment_name} - (Bootstraps: {BOOTSTRAPS_N} - Sample Size: {SAMPLE_SIZE})\", fontsize=10)\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    max_len = 6\n",
    "    mean_label = f\"{mean: .2f}\".rjust(max_len)\n",
    "    plt.axvline(mean, color='red', linestyle='dotted', linewidth=2, label=f'Mean F1:          {mean_label}')\n",
    "\n",
    "    # Add vertical lines for the 95% confidence interval\n",
    "    lower = f\"{ci[0]: .2f}\".rjust(max_len)\n",
    "    upper = f\"{ci[1]: .2f}\".rjust(max_len)\n",
    "    plt.axvline(ci[0], color='orange', linestyle='dashdot', linewidth=1.5, label=f\"95% CI Lower:  {lower}\")\n",
    "    plt.axvline(ci[1], color='orange', linestyle='dashdot', linewidth=1.5, label=f\"95% CI Upper:  {upper}\")\n",
    "\n",
    "    ax = plt.gca()  # Get current axis\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(10))  # Major ticks every 10 units\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(5))   # Minor ticks every 5 units\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(10))   # Example: Major ticks every 5 units on the y-axis\n",
    "    #ax.yaxis.set_major_formatter(PercentFormatter(xmax=len(data)))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=10))\n",
    "    \n",
    "    # Customize grid for major and minor ticks\n",
    "    ax.grid(which='major', color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', color='lightgray', linestyle=':', linewidth=0.5)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(prop={'family': 'monospace', 'size': 10})\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder_name, f\"{experiment_name}_{BOOTSTRAPS_N}_{SAMPLE_SIZE}\"))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLM as a Judge functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_answer_tuple(input_string):\n",
    "    # Trim whitespace\n",
    "    parts = input_string.strip()\n",
    "    # Remove parentheses\n",
    "    parts = parts.strip(\"()\")\n",
    "    # Split on comma\n",
    "    parts = parts.split(\", \")\n",
    "    if len(parts) != 2:\n",
    "        #raise ValueError(\"Invalid answer string\")\n",
    "        return (\"NO\", 0)\n",
    "\n",
    "    answer = parts[0].strip().upper()  # \"Yes\" or \"No\"\n",
    "    if answer not in [\"YES\", \"NO\"]:\n",
    "        #raise ValueError(\"Invalid answer string\")\n",
    "        return (\"NO\", 0)\n",
    "    try:\n",
    "        score = float(parts[1])  # Convert score to a float\n",
    "    except ValueError:\n",
    "        return (\"NO\", 0)\n",
    "    \n",
    "    return (answer, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_verdict(judge_verdicts):\n",
    "\n",
    "    judge_answers = judge_verdicts[0]\n",
    "    yes_count = judge_answers.count('YES')\n",
    "    no_count = judge_answers.count('NO')\n",
    "\n",
    "    if no_count > yes_count:\n",
    "        return \"NO\", 0.0\n",
    "    \n",
    "    judge_scores = list(judge_verdicts[1])\n",
    "    return \"YES\", sum(judge_scores) / yes_count\n",
    "\n",
    "def get_majority_verditcs(all_judge_answers, all_judge_scores):\n",
    "\n",
    "    majority_verdicts = []\n",
    "    for per_question_judge_verdicts in zip(zip(*all_judge_answers), zip(*all_judge_scores)):\n",
    "        majority_verdict = get_majority_verdict(per_question_judge_verdicts)\n",
    "        majority_verdicts.append(majority_verdict)\n",
    "\n",
    "    return majority_verdicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_judges_verdicts(sample_ldict, experiment_name, hyde_context_needed=False):\n",
    "    judge_current_query_prompt = deh_prompts.query_prompts[4]\n",
    "    print(f\"judge_current_query_prompt = {judge_current_query_prompt.template}\\n\")\n",
    "    for ele in sample_ldict:\n",
    "        print(ele)\n",
    "        print(\"\\n\")\n",
    "\n",
    "#    judge_llms = [MISTRAL_LATEST, GEMMA2_9B, QWEN2_5_7B]\n",
    "\n",
    "    all_judge_answers = []\n",
    "    all_judge_scores = []\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    for judge_i, judge_llm in enumerate(judge_llms):\n",
    "        judge_chain = judge_current_query_prompt | get_llm(judge_current_query_prompt, True, judge_llm)\n",
    "        judge_answers = []\n",
    "        judge_scores = []\n",
    "\n",
    "        for i, qa in enumerate(sample_ldict):\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processing question {i} for judge {judge_i}...\")\n",
    "\n",
    "            question = qa[\"question\"]\n",
    "            context = qa[\"question_context\"]\n",
    "            if hyde_context_needed:\n",
    "                hyde_based_context = qa[\"hyde_based_context\"]\n",
    "                context = \"\\n\\n\".join([context, hyde_based_context])\n",
    "            answer_key = experiment_name.lower() + \"_llm_answer\"\n",
    "            answer = qa[answer_key]\n",
    "\n",
    "            # print(f\"\\nQuestion: {question}\")\n",
    "            # print(f\"Context[:100]: {context[:100]}\")\n",
    "            # print(f\"Answer: {answer}\")\n",
    "\n",
    "            response = judge_chain.invoke({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "            # print(f\"Judge Response: {response.content}\")\n",
    "            a, s = convert_string_to_answer_tuple(response.content)\n",
    "            judge_answers.append(a)\n",
    "            judge_scores.append(s)\n",
    "            \n",
    "        all_judge_answers.append(judge_answers)\n",
    "        all_judge_scores.append(judge_scores)\n",
    "\n",
    "    final_judges_verdicts = get_majority_verditcs(all_judge_answers, all_judge_scores)\n",
    "    return final_judges_verdicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions for Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp and format the start timestamp as a string\n",
    "def get_timestamp_as_string():\n",
    "    start_timestamp = datetime.now()\n",
    "    start_timestamp_str = start_timestamp.strftime('%Y%m%d_%H%M%S')\n",
    "    return start_timestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(qas, query_prompt_idx, experiment_name, context_needed=False,\n",
    "                     hyde_context_needed=False, suppress_answers=False):\n",
    "\n",
    "    # Create the chain\n",
    "    current_query_prompt = deh_prompts.query_prompts[query_prompt_idx]\n",
    "    print(f\"current_query_prompt = {current_query_prompt.template}\\n\")\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = get_runnable_chain(current_query_prompt, llm)\n",
    "\n",
    "    # Generate the LLM answers for all questions and calculate per-answer metrics \n",
    "    # add each answer to the all_preds\n",
    "    preds = {}\n",
    "    all_preds= {}\n",
    "\n",
    "    for i, qa in enumerate(qas):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        qid = qa[\"qid\"]\n",
    "        question = qa[\"question\"]\n",
    "\n",
    "        if context_needed:\n",
    "            if experiment_name in [\"BASIC_RAG_HYDE\"]:\n",
    "                context = qa[\"hyde_based_context\"]\n",
    "            elif experiment_name in [\"FULL_RAG\"]:\n",
    "                context = qa[\"question_context\"] + \"\\n\\n\" + qa[\"hyde_based_context\"]\n",
    "            elif experiment_name in [\"BASIC_RAG\", \"BASIC_RAG_DONT_LIE\",\n",
    "                                     \"BASIC_RAG_SUPPRESS_ANSWERS\", \"BASIC_RAG_SEMANTIC\"]:\n",
    "                context = qa[\"question_context\"]\n",
    "            response = runnable_chain.invoke({\"question\": question, \"context\": context})\n",
    "        else:\n",
    "            response = runnable_chain.invoke({\"question\": question})\n",
    "\n",
    "        llm_answer = response.content\n",
    "\n",
    "        if \"DONT KNOW\" in llm_answer.upper() or llm_answer.upper().startswith(\"NO CONTEXT PROVIDED\") or llm_answer.upper().startswith(\"NO INFORMATION GIVEN\"):  \n",
    "            llm_answer = \"\"\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        all_preds[qid] = llm_answer\n",
    "\n",
    "        scores = squad_scoring.calc_squad_metrics(dataset, preds)\n",
    "        f1 = scores[\"f1\"]\n",
    "        precision = scores[\"precision\"]\n",
    "        recall = scores[\"recall\"]\n",
    "\n",
    "        preds = {}\n",
    "        qa[f\"{experiment_name.lower()}_llm_answer\"] = llm_answer\n",
    "        qa[f\"{experiment_name.lower()}_f1\"] = f1\n",
    "        qa[f\"{experiment_name.lower()}_precision\"] = precision\n",
    "        qa[f\"{experiment_name.lower()}_recall\"] = recall\n",
    "\n",
    "    if suppress_answers:\n",
    "        final_judges_verdicts = get_final_judges_verdicts(qas, experiment_name, hyde_context_needed)\n",
    "        for i, qa in enumerate(qas):\n",
    "            if final_judges_verdicts[i][0] == \"NO\":\n",
    "                qa[f\"{experiment_name.lower()}_llm_answer\"] = \"\"\n",
    "            else:\n",
    "                if final_judges_verdicts[i][1] < JUDGES_SUPPRESS_THRESHOLD:\n",
    "                    qa[f\"{experiment_name.lower()}_llm_answer\"] = \"\"\n",
    "\n",
    "    return all_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_results(results_folder_name, experiment_name, df):\n",
    "\n",
    "    df.to_csv(f\"{results_folder_name}/qas_{experiment_name.lower()}_scores.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_experiments_config(results_folder_name, experiment_name, experiment_mean, experiment_ci, execution_time):\n",
    "\n",
    "    params_str = f\"Experiment: {experiment_name}\\n\"\n",
    "    params_str = params_str + f\"{datetime.now()}\\n\"\n",
    "    params_str = params_str + f\"Execution duration in minutes: {round(execution_time/60,2)}\\n\"\n",
    "    params_str = params_str + \"================================================\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"F1 mean: {experiment_mean}\\n\"\n",
    "    params_str = params_str + f\"95% CI ({experiment_ci[0]}, {experiment_ci[1]})\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"DATA_ROOT = {DATA_ROOT}\\n\"\n",
    "    params_str = params_str + f\"RESULTS_ROOT = {RESULTS_ROOT}\\n\"\n",
    "    params_str = params_str + f\"HYDE_BASED_CONTEXTS_ROOT  = {HYDE_BASED_CONTEXTS_ROOT}\\n\"\n",
    "    params_str = params_str + f\"data_file = {DATA_ROOT}/dev-v2.0.json\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"CHROMA_ROOT               = {CHROMA_ROOT}\\n\"\n",
    "    params_str = params_str + f\"VECTOR_STORE_TOP_K        = {VECTOR_STORE_TOP_K}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_SIZE                = {CHUNK_SIZE}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_OVERLAP             = {CHUNK_OVERLAP}\\n\"\n",
    "    params_str = params_str + f\"DEFAULT_CHROMA_PREFIX     = {DEFAULT_CHROMA_PREFIX}\\n\"\n",
    "    params_str = params_str + f\"DEFAULT_CHUNKING_METHOD   = {DEFAULT_CHUNKING_METHOD}\\n\"\n",
    "    params_str = params_str + f\"CHUNK_SQUAD_DATASET       = {CHUNK_SQUAD_DATASET}\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"REFRESH_QUESTION_CONTEXTS = {REFRESH_QUESTION_CONTEXTS}\\n\"\n",
    "    params_str = params_str + f\"REFRESH_HYDE_CONTEXTS     = {REFRESH_HYDE_CONTEXTS}\\n\"\n",
    "    params_str = params_str + f\"READ_QAS_FROM_FILE        = {READ_QAS_FROM_FILE}\\n\"\n",
    "    params_str = params_str + f\"RESTORE_QAS_WITH_CONTEXTS = {RESTORE_QAS_WITH_CONTEXTS}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"        \n",
    "    params_str = params_str + f\"SAMPLE_SIZE  = {SAMPLE_SIZE}\\n\"\n",
    "    params_str = params_str + f\"BOOTSTRAPS_N = {BOOTSTRAPS_N}\\n\\n\"\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_0  = {LLM_MODEL_NAME_0}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_1  = {LLM_MODEL_NAME_1}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_2  = {LLM_MODEL_NAME_2}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_3  = {LLM_MODEL_NAME_3}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_4  = {LLM_MODEL_NAME_4}\\n\"\n",
    "    params_str = params_str + f\"LLM_MODEL_NAME_5  = {LLM_MODEL_NAME_5}\\n\"\n",
    "    params_str = params_str + f\"CHAT_MODEL_NAME   = {CHAT_MODEL_NAME}\\n\\n\"\n",
    "    \n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    params_str = params_str + f\"MAX_TOKENS        = {MAX_TOKENS}\\n\"\n",
    "    params_str = params_str + f\"TEMPERATURE       = {TEMPERATURE}\\n\"\n",
    "    params_str = params_str + f\"TOP_P             = {TOP_P}\\n\"\n",
    "    params_str = params_str + f\"FREQUENCY_PENALTY = {FREQUENCY_PENALTY}\\n\"\n",
    "    params_str = params_str + f\"PRESENCE_PENALTY  = {PRESENCE_PENALTY}\\n\\n\"\n",
    "\n",
    "\n",
    "    params_str = params_str + \"--------------------------------------------------------\\n\"\n",
    "    judges = [llm_models[judge_i] for judge_i in judge_llms]\n",
    "    params_str = params_str + f\"Judge LLMS = {judges}\\n\"\n",
    "    params_str = params_str + f\"JUDGES_SUPPRESS_THRESHOLD   = {JUDGES_SUPPRESS_THRESHOLD}\\n\\n\"\n",
    "\n",
    "    with open(f\"{results_folder_name}/experiments_config_{experiment_name.lower()}.txt\", 'w') as f:\n",
    "        f.write(params_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_scores(scores_l, clip_perc):\n",
    "    scores_sorted_l = sorted(scores_l)\n",
    "\n",
    "    clip_cnt = int(len(scores_sorted_l) * clip_perc / 100)\n",
    "    print(f\"clip_cnt: {clip_cnt}\")\n",
    "\n",
    "    # Now clip both ends by clip_perc percent\n",
    "    clipped_scores_l = scores_sorted_l[clip_cnt:-clip_cnt] if clip_cnt > 0 else scores_sorted_l\n",
    "    return clipped_scores_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bootstrapping(scores_l, results_folder_name, experiment_name, bootstraps_n = BOOTSTRAPS_N):\n",
    "    \n",
    "    mu_hats = []\n",
    "    n = len(scores_l)\n",
    "    scores_l = clip_scores(scores_l, 2.5)\n",
    "\n",
    "    for i in range(bootstraps_n):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing sample {i}...\")\n",
    "        bootstrap_sample = random.choices(scores_l, k=n) # sample with replacement\n",
    "        mu_hat = np.mean(bootstrap_sample)\n",
    "        mu_hats.append(mu_hat)\n",
    "\n",
    "    bootstraps_mean, ci = calculate_mean_confidence_interval(mu_hats)\n",
    "    plt = generate_histogram(mu_hats, bootstraps_mean, ci, results_folder_name, experiment_name)\n",
    "    plt.show();\n",
    "    return bootstraps_mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_folder(experiment_name):\n",
    "    start_timestamp_str = get_timestamp_as_string()\n",
    "    results_folder_name = f\"{RESULTS_ROOT}/{experiment_name}/results_{start_timestamp_str}\"\n",
    "    if not os.path.exists(results_folder_name):\n",
    "        os.makedirs(results_folder_name, exist_ok=True)\n",
    "    return results_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(qas, experiment_name, query_prompt_idx, \n",
    "                       context_needed=False, hyde_context_needed=False, \n",
    "                       suppress_answers=False):\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    print(f\"============= Creating results folder for {experiment_name} =============\")\n",
    "    results_folder_name = create_results_folder(experiment_name)\n",
    "    \n",
    "    print(f\"============= Calculating scores for {experiment_name} =============\")\n",
    "    print(f\"SAMPLE_SIZE: {SAMPLE_SIZE}\\n\")\n",
    "    all_preds = calculate_scores(qas, query_prompt_idx, experiment_name, context_needed, \n",
    "                                 hyde_context_needed, suppress_answers)\n",
    "\n",
    "    print(f\"============= Persisting results for {experiment_name} =============\")\n",
    "    df = pd.DataFrame(qas)\n",
    "    persist_results(results_folder_name, experiment_name, df)\n",
    "    \n",
    "    print(f\"\\n============= Bootstrapping for {experiment_name} =============\")\n",
    "    print(f\"BOOTSTRAPS_N: {BOOTSTRAPS_N}\")\n",
    "\n",
    "    experiment_mean, experiment_ci = do_bootstrapping(df[f\"{experiment_name.lower()}_f1\"].dropna().tolist(), results_folder_name, experiment_name, BOOTSTRAPS_N)\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times_entry = {}\n",
    "    execution_times_entry[\"experiment_name\"] = experiment_name\n",
    "    execution_times_entry[\"execution_time\"] = execution_time\n",
    "    execution_times_entry[\"sample_size\"] = SAMPLE_SIZE\n",
    "    execution_times_entry[\"bootstrap_n\"] = BOOTSTRAPS_N\n",
    "    execution_times_l.append(execution_times_entry)\n",
    "\n",
    "    persist_experiments_config(results_folder_name, experiment_name, experiment_mean, experiment_ci, execution_time)\n",
    "\n",
    "    return execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a sample from df_cas_with_contexts for bootstrapping and bootstrapping with Hyde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qas_for_bootstrapping = df_qas_with_contexts[['qid', 'question', 'question_context', 'hyde_based_context']]\n",
    "df_qas_for_bootstrapping_sample = df_qas_for_bootstrapping.sample(n=SAMPLE_SIZE, replace=True)\n",
    "ldict_qas_for_boostrapping_sample = df_qas_for_bootstrapping_sample.to_dict(orient='records')\n",
    "\n",
    "df_qas_for_hyde_bootstrapping = df_qas_for_bootstrapping[df_qas_for_bootstrapping['hyde_based_context'].notna()]\n",
    "df_qas_for_hyde_bootstrapping_sample = df_qas_for_hyde_bootstrapping.sample(n=SAMPLE_SIZE, replace=True)\n",
    "ldict_qas_for_hyde_bootstrapping_sample = df_qas_for_hyde_bootstrapping_sample.to_dict(orient='records')\n",
    "\n",
    "sample_ldicts = [ldict_qas_for_boostrapping_sample, ldict_qas_for_hyde_bootstrapping_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conducting all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all experiments and execute them one by one if they are included\n",
    "for index, row in df_experiments.iterrows():\n",
    "    if not row[\"include\"]:\n",
    "        continue\n",
    "\n",
    "    sample_ldict = sample_ldicts[row[\"sample_ldicts_idx\"]]\n",
    "    experiment_name = row[\"name\"]\n",
    "    query_prompt_idx = row[\"query_prompt_idx\"]\n",
    "    context_needed = row[\"context_needed\"]\n",
    "    hyde_context_needed = row[\"hyde_context_needed\"]\n",
    "    suppress_answers = row[\"suppress_answers\"]\n",
    "    \n",
    "    print(f\"sample_ldict: {sample_ldict}\")\n",
    "    print(f\"experiment_name: {experiment_name}\")\n",
    "    print(f\"query_prompt_idx: {query_prompt_idx}\")\n",
    "    print(f\"context_needed: {context_needed}\")\n",
    "    print(f\"hyde_context_needed: {hyde_context_needed}\")\n",
    "    print(f\"suppress_answers: {suppress_answers}\")\n",
    "    \n",
    "    conduct_experiment(sample_ldict, experiment_name, query_prompt_idx,\n",
    "                       context_needed, hyde_context_needed, suppress_answers)\n",
    "\n",
    "print(f\"\\n================== Execution times (in seconds): ==================================\\n\")\n",
    "df_execution_times = pd.DataFrame(execution_times_l, \n",
    "                                  columns=['experiment_name', 'execution_time', 'sample_size', 'bootstrap_n'])\n",
    "\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "df_execution_times.head(100)              \n",
    "\n",
    "print(f\"Total execution time in Minutes: {round(sum(df_execution_times['execution_time']) / 60, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the display format for floats\n",
    "pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total execution time in Minutes: {round(sum(df_execution_times['execution_time']) / 60, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
