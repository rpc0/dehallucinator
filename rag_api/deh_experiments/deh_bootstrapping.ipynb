{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for deh experiments specific modules\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\".\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import squad_scoring\n",
    "import deh_prompts\n",
    "import deh_vector_store\n",
    "import deh_squad_data\n",
    "import deh_hyde\n",
    "import deh_experiments_config\n",
    "globals().update(deh_experiments_config.__dict__)\n",
    "import deh_globals\n",
    "globals().update(deh_globals.__dict__)\n",
    "# from deh_llm import get_llm\n",
    "import deh_llm\n",
    "globals().update(deh_llm.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading SQuAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading squad data...\n",
      "\n",
      "Number of raw entries in squad_raw: 26232\n",
      "Number of unique titles: 35\n",
      "Number of unique contexts: 1204\n",
      "Number of unique questions: 11858\n",
      "Number of unique answers: 16209\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/qas_with_contexts.csv\"\n",
    "\n",
    "print(f\"Loading squad data...\\n\")\n",
    "\n",
    "column_names = [\"title\", \"squad_context\", \"qid\", \"question\", \"is_impossible\", \"answer\"]\n",
    "squad_raw = pd.read_csv(f\"{DATA_ROOT}/squad_raw.csv\", names=column_names, skiprows=1)\n",
    "df_squad_raw = pd.DataFrame(squad_raw)\n",
    "print(f\"Number of raw entries in squad_raw: {len(df_squad_raw)}\")\n",
    "\n",
    "df_titles = pd.DataFrame(df_squad_raw['title'].unique(), columns=[\"title\"])\n",
    "print(f\"Number of unique titles: {len(df_titles)}\")\n",
    "\n",
    "df_contexts = pd.DataFrame(df_squad_raw['squad_context'].unique(), columns=[\"squad_context\"])\n",
    "print(f\"Number of unique contexts: {len(df_contexts)}\")\n",
    "\n",
    "df_qas = df_squad_raw[['title', 'squad_context', 'qid', 'question', 'is_impossible']].drop_duplicates()\n",
    "df_qas = df_qas.reset_index(drop=True)\n",
    "print(f\"Number of unique questions: {len(df_qas)}\")\n",
    "\n",
    "df_squad_answers = df_squad_raw[['qid', 'question', 'answer']].drop_duplicates()\n",
    "print(f\"Number of unique answers: {len(df_squad_answers)}\")           \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus not yet included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking not foreseen. Skipping chunking.\n"
     ]
    }
   ],
   "source": [
    "contexts = list(df_contexts[\"squad_context\"].values)\n",
    "if CHUNK_SQUAD_DATASET:    \n",
    "    deh_vector_store.chunk_squad_dataset(contexts, dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "else:\n",
    "    print(\"Chunking not foreseen. Skipping chunking.\")\n",
    "\n",
    "# Intiialize the Chroma vector store\n",
    "vector_store = deh_vector_store.get_vector_store(DEFAULT_CHROMA_PREFIX, DEFAULT_CHROMA_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading qas with contexts data (if data is not to be restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not restoring qas_with_contexts from the CSV file, then read\n",
    "# the data from the csv file (i.e. it exists and is correct)\n",
    "if not RESTORE_QAS_WITH_CONTEXTS:\n",
    "    # Loading the question contexts from the CSV file\n",
    "    qas_with_contexts_csv_file_path = f\"{DATA_ROOT}/qas_with_contexts.csv\"\n",
    "\n",
    "    df_qas_with_contexts = pd.read_csv(qas_with_contexts_csv_file_path) #, names=column_names)\n",
    "\n",
    "    # drop the answer column if it exists, since it leads to duplicates\n",
    "    if 'answer' in df_qas_with_contexts.columns:\n",
    "        df_qas_with_contexts = df_qas_with_contexts.drop(columns=['answer'])\n",
    "\n",
    "    df_qas_with_contexts = df_qas_with_contexts.drop_duplicates()\n",
    "    print(f\"Rows in dataframe df_qas_with_contexts: {len(df_qas_with_contexts)}\")\n",
    "    hyde_articles_cnt = df_qas_with_contexts['hyde_article'].notna().sum()\n",
    "    hyde_based_contexts_cnt = df_qas_with_contexts['hyde_based_context'].notna().sum()\n",
    "    print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "    print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restore qas with contexts (if configured); alternatively refreseh contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_df_qas_with_contexts_file(csv_file_path, df_qas):\n",
    "\n",
    "    # Add columns hyde_article, question_context, hyde_based_context to df_qas_with_contexts\n",
    "    if not 'hyde_article' in df_qas.columns:\n",
    "        df_qas['hyde_article'] = np.nan\n",
    "    if not 'question_context' in df_qas.columns:\n",
    "        df_qas['question_context'] = np.nan\n",
    "    if not 'hyde_based_context' in df_qas.columns:\n",
    "        df_qas['hyde_based_context'] = np.nan\n",
    "\n",
    "    # Get Hyde data\n",
    "    hyde_based_context_path = f\"{HYDE_BASED_CONTEXTS_ROOT}/hyde_based_contexts.csv\"\n",
    "    df_hyde_based_contexts = pd.read_csv(hyde_based_context_path)\n",
    "\n",
    "    print(f\"Rows in dataframe df_hyde_based_contexts: {len(df_hyde_based_contexts)}\")\n",
    "    hyde_articles_cnt = df_hyde_based_contexts['hyde_article'].notna().sum()\n",
    "    hyde_based_contexts_cnt = df_hyde_based_contexts['hyde_based_context'].notna().sum()\n",
    "\n",
    "    # Merge df_qas with df_hyde_based_contexts based on the 'qid' column\n",
    "    merged = df_qas.merge(df_hyde_based_contexts, on='qid', how='left', suffixes=('', '_df_hyde_based_contexts'))\n",
    "    df_qas['hyde_article'] = merged['hyde_article_df_hyde_based_contexts']\n",
    "\n",
    "    print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "    print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n",
    "\n",
    "    df_qas.to_csv(csv_file_path, header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RESTORE_QAS_WITH_CONTEXTS:\n",
    "#     restore_df_qas_with_contexts(csv_file_path, df_qas)\n",
    "\n",
    "#     # Get Hyde data\n",
    "#     hyde_based_context_path = f\"{HYDE_BASED_CONTEXTS_ROOT}/hyde_based_contexts.csv\"\n",
    "#     df_hyde_based_contexts = pd.read_csv(hyde_based_context_path)\n",
    "\n",
    "#     print(f\"Rows in dataframe df_hyde_based_contexts: {len(df_hyde_based_contexts)}\")\n",
    "#     hyde_articles_cnt = df_hyde_based_contexts['hyde_article'].notna().sum()\n",
    "#     hyde_based_contexts_cnt = df_hyde_based_contexts['hyde_based_context'].notna().sum()\n",
    "\n",
    "#     # Merge df1 with df2 based on the 'qid' column\n",
    "#     merged = df_qas.merge(df_hyde_based_contexts, on='qid', how='left', suffixes=('', '_df_hyde_based_contexts'))\n",
    "#     df_qas['hyde_article'] = merged['hyde_article_df_hyde_based_contexts']\n",
    "\n",
    "#     print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "#     print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO an df_qas_with_contexts anpassen !!!!!!!!\n",
    "\n",
    "if RESTORE_QAS_WITH_CONTEXTS:\n",
    "    restore_df_qas_with_contexts_file(csv_file_path, df_qas)\n",
    "\n",
    "# Refresh question contexts (normal contexts and hyde-based contexts)\n",
    "# TODO: might be useful to first empty the two columns in the dataframe\n",
    "if RESTORE_QAS_WITH_CONTEXTS or REFRESH_QUESTION_CONTEXTS or REFRESH_HYDE_CONTEXTS:\n",
    "    print(f\"Re-Generating contexts for the dataset and persisting the data...\")\n",
    "    list_of_qas = df_qas.to_dict(orient='records')\n",
    "\n",
    "    for i, qa in enumerate(list_of_qas):\n",
    "        # print(i)\n",
    "        if i %100 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        if RESTORE_QAS_WITH_CONTEXTS or REFRESH_QUESTION_CONTEXTS:\n",
    "            question = qa[\"question\"]\n",
    "            \n",
    "            top_docs = vector_store.similarity_search(\n",
    "                query = question,\n",
    "                k = VECTOR_STORE_TOP_K,\n",
    "            )\n",
    "            qa[\"question_context\"] = \" \".join([top_doc.page_content for top_doc in top_docs])\n",
    "\n",
    "        if RESTORE_QAS_WITH_CONTEXTS or REFRESH_HYDE_CONTEXTS:\n",
    "            hyde_article = qa[\"hyde_article\"]\n",
    "            #print(f\"hyde_article: {hyde_article}\")\n",
    "            if pd.isna(hyde_article):\n",
    "                hyde_article = \"\"\n",
    "            elif len(hyde_article) == 0:\n",
    "                hyde_article = \"\"\n",
    "            else:\n",
    "                top_docs = vector_store.similarity_search(\n",
    "                    query = hyde_article,\n",
    "                    k = VECTOR_STORE_TOP_K,\n",
    "                )\n",
    "\n",
    "                qa[\"hyde_based_context\"] = \" \".join([top_doc.page_content for top_doc in top_docs])\n",
    "\n",
    "    df_qas = pd.DataFrame(list_of_qas)\n",
    "    df_qas.to_csv(csv_file_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qas# Convert DataFrame to a list of dictionaries\n",
    "list_of_qas = df_qas.to_dict(orient='records')\n",
    "list_of_dicts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Hyde data (articles and Hyde-based contetxts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyde_based_context_path = f\"{HYDE_BASED_CONTEXTS_ROOT}/hyde_based_contexts.csv\"\n",
    "# df_hyde_based_contexts = pd.read_csv(hyde_based_context_path)\n",
    "\n",
    "# print(f\"Rows in dataframe df_hyde_based_contexts: {len(df_hyde_based_contexts)}\")\n",
    "# hyde_articles_cnt = df_hyde_based_contexts['hyde_article'].notna().sum()\n",
    "# hyde_based_contexts_cnt = df_hyde_based_contexts['hyde_based_context'].notna().sum()\n",
    "# print(f\"Number of questions with Hyde articles: {hyde_articles_cnt}\")\n",
    "# print(f\"Number of questions with Hyde based contexts: {hyde_based_contexts_cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show names of all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "print(\"Names of Dataframes and their lenghts:\\n\")\n",
    "global_keys_copy = copy.deepcopy(list(globals().keys()))\n",
    "\n",
    "my_l = [{n: len(globals()[n])} for n in global_keys_copy if n.startswith(\"df_\")]\n",
    "df_dfs = pd.DataFrame([(k, v) for d in my_l for k, v in d.items()], columns=['df name', 'rows'])\n",
    "df_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus not yet included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts = list(df_contexts[\"squad_context\"].values)\n",
    "# if CHUNK_SQUAD_DATASET:    \n",
    "#     deh_vector_store.chunk_squad_dataset(contexts, dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "# else:\n",
    "#     print(\"Chunking not foreseen. Skipping chunking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize the Chroma vector store\n",
    "# vector_store = deh_vector_store.get_vector_store(DEFAULT_CHROMA_PREFIX, DEFAULT_CHROMA_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get hyde-based contexts\n",
    "\n",
    "Always get hyde-based contexts that already exist and these to the qas dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not READ_QAS_FROM_FILE:\n",
    "#     hyde_based_context_path = f\"{HYDE_BASED_CONTEXTS_ROOT}/hyde_based_contexts.csv\"\n",
    "#     hyde_based_contexts, questions_already_processed = deh_hyde.get_hyde_based_contexts(hyde_based_context_path)\n",
    "#     print(f\"Number of questions with hyde-based context: {len(questions_already_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not READ_QAS_FROM_FILE:\n",
    "#     # Now add the hyde-based contexts to the qas dataset\n",
    "#     def get_hyde_based_info_from_qid(qid):\n",
    "#         for hbc in hyde_based_contexts:\n",
    "#             if hbc[\"qid\"] == qid:\n",
    "#                 return (hbc[\"hyde_article\"], hbc[\"hyde_based_context\"])\n",
    "#         return (\"\", \"\")\n",
    "\n",
    "#     for qa in qas:\n",
    "#         qid = qa[\"qid\"]\n",
    "#         hyde_article, hyde_based_context = get_hyde_based_info_from_qid(qid)\n",
    "#         qa[\"hyde_article\"] = hyde_article\n",
    "#         qa[\"hyde_based_context\"] = hyde_based_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refresh Contexts (if configured in deh_globals.py)\n",
    "\n",
    "Questions:\n",
    "\n",
    "- if configured, either: generate question and hyde contexts and then persist\n",
    "- or: read the question contexts from a .csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_question_contexts(qas, csv_file_path):\n",
    "\n",
    "    # Write the the qas dataset including the question contexts to a CSV file\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        fieldnames = [\"title\", \"context\", \"qid\", \"question\", \"is_impossible\",\n",
    "                      \"answer\", \"hyde_article\", \"hyde_based_context\", \"question_context\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()   # Write the header row\n",
    "        writer.writerows(qas)  # Write the data rows\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions that are needed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runnable chain\n",
    "def get_runnable_chain(current_query_prompt, llm):\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "    return runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hyde context for a question\n",
    "def get_hyde_based_context(question):\n",
    "    hyde_based_context = df_qas_with_contexts[df_qas_with_contexts[\"question\"] == question]\n",
    "    if hyde_based_context.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return hyde_based_context[\"hyde_based_context\"].values[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate the LLM answers, using a runnable chain and the sample of questions provided\n",
    "# def generate_llm_answers(runnable_chain, qas_sample, hyde=False):\n",
    "    \n",
    "#     preds = {}\n",
    "\n",
    "#     sample_size = len(qas_sample)\n",
    "#     print(f\"sample_size: {sample_size}\")\n",
    "\n",
    "#     for i, qa in enumerate(qas_sample):\n",
    "\n",
    "#         question = qa[\"question\"]\n",
    "#         if hyde:\n",
    "#             #context = qa[\"hyde_context\"]\n",
    "#             context = get_hyde_based_context(question)\n",
    "#         else:\n",
    "#             context = qa[\"vector_store_context\"]\n",
    "            \n",
    "#         # print(f\"question --> {question}\")\n",
    "#         # print(context)\n",
    "#         response = runnable_chain.invoke({\"context\": context, \"question\": question})\n",
    "                \n",
    "#         qid = squad_scoring.get_qid_from_question(question, dataset)\n",
    "        \n",
    "#         if response.content.upper() == \"DONT KNOW\":\n",
    "#             llm_answer = \"\"\n",
    "#         else:\n",
    "#             llm_answer = response.content\n",
    "\n",
    "#         preds[qid] = llm_answer\n",
    "#         qas_sample[i][\"llm_answer\"] = llm_answer\n",
    "\n",
    "#     return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# Get the metrics for a set of predictions (preds) that have been generated in a run\n",
    "def get_squad_metrics(dataset, preds, verbose=False):\n",
    "    squad_metrics = squad_scoring.calc_squad_metrics(dataset, preds);\n",
    "    return squad_metrics[\"precision\"], squad_metrics[\"recall\"], squad_metrics[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and confidence interval for a list of scores\n",
    "# TODO: Check if this is calculation is correct !!\n",
    "def calculate_mean_confidence_interval(scores_l):\n",
    "\n",
    "    # Calculate mean\n",
    "    mean = np.mean(scores_l)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    sample_std_dev = np.std(scores_l, ddof=1)\n",
    "    margin_of_error = 1.96 * sample_std_dev\n",
    "    ci = (max(mean - margin_of_error, 0), min(mean + margin_of_error, 100))\n",
    "    # if ci[0] < 0:\n",
    "    #     ci = (0, ci[1])\n",
    "    # if ci[1] > 100:\n",
    "    #     ci = (ci[0], 100)\n",
    "\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate a histogram for a list of scores and persist it\n",
    "def generate_histogram(scores_l, mean, ci, results_folder_name, experiment_name):\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(scores_l, bins=30, density=False, edgecolor='black', alpha=0.6, color = 'lightblue' ) # color='aquamarine')\n",
    "    plt.xlim(0, 100)\n",
    "\n",
    "    plt.title(f\"F1-Scores for {experiment_name} - (Bootstraps: {BOOTSTRAPS_N} - Sample Size: {SAMPLE_SIZE})\", fontsize=10)\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    max_len = 6\n",
    "    mean_label = f\"{mean: .2f}\".rjust(max_len)\n",
    "    plt.axvline(mean, color='red', linestyle='dotted', linewidth=2, label=f'Mean F1:          {mean_label}')\n",
    "\n",
    "    # Add vertical lines for the 95% confidence interval\n",
    "    lower = f\"{ci[0]: .2f}\".rjust(max_len)\n",
    "    upper = f\"{ci[1]: .2f}\".rjust(max_len)\n",
    "    plt.axvline(ci[0], color='orange', linestyle='dashdot', linewidth=1.5, label=f\"95% CI Lower:  {lower}\")\n",
    "    plt.axvline(ci[1], color='orange', linestyle='dashdot', linewidth=1.5, label=f\"95% CI Upper:  {upper}\")\n",
    "\n",
    "    ax = plt.gca()  # Get current axis\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(10))  # Major ticks every 10 units\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(5))   # Minor ticks every 5 units\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(10))   # Example: Major ticks every 5 units on the y-axis\n",
    "    #ax.yaxis.set_major_formatter(PercentFormatter(xmax=len(data)))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=10))\n",
    "    \n",
    "    # Customize grid for major and minor ticks\n",
    "    ax.grid(which='major', color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', color='lightgray', linestyle=':', linewidth=0.5)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(prop={'family': 'monospace', 'size': 10})\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder_name, f\"{experiment_name}_{BOOTSTRAPS_N}_{SAMPLE_SIZE}\"))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions for Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp and format the start timestamp as a string\n",
    "def get_timestamp_as_string():\n",
    "    start_timestamp = datetime.now()\n",
    "    start_timestamp_str = start_timestamp.strftime('%Y%m%d_%H%M%S')\n",
    "    return start_timestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(qas, query_prompt_idx, experiment_name, context_needed=False, suppress_answers=False):\n",
    "\n",
    "    # Create the chain\n",
    "    current_query_prompt = deh_prompts.query_prompts[query_prompt_idx]\n",
    "    print(f\"current_query_prompt = {current_query_prompt.template}\\n\")\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = get_runnable_chain(current_query_prompt, llm)\n",
    "\n",
    "    # Generate the LLM answers for all questions and calculate per-answer metrics \n",
    "    # add each answer to the all_preds\n",
    "    preds = {}\n",
    "    all_preds= {}\n",
    "    for i, qa in enumerate(qas):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        qid = qa[\"qid\"]\n",
    "        question = qa[\"question\"]\n",
    "\n",
    "        if context_needed:\n",
    "            if experiment_name == \"BASIC_RAG_HYDE\":\n",
    "                context = qa[\"hyde_based_context\"]\n",
    "            elif experiment_name == \"FULL_RAG\":\n",
    "                context = qa[\"question_context\"] + \"\\n\\n\" + qa[\"hyde_based_context\"]\n",
    "            else:\n",
    "                context = qa[\"question_context\"]\n",
    "            response = runnable_chain.invoke({\"question\": question, \"context\": context})\n",
    "        else:\n",
    "            response = runnable_chain.invoke({\"question\": question})\n",
    "\n",
    "        llm_answer = response.content\n",
    "\n",
    "        if llm_answer.upper() == \"DONT KNOW\":\n",
    "            if suppress_answers:\n",
    "                continue\n",
    "            else:\n",
    "                llm_answer = \"\"\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        all_preds[qid] = llm_answer\n",
    "\n",
    "        scores = squad_scoring.calc_squad_metrics(dataset, preds)\n",
    "        f1 = scores[\"f1\"]\n",
    "        precision = scores[\"precision\"]\n",
    "        recall = scores[\"recall\"]\n",
    "\n",
    "        preds = {}\n",
    "        qa[f\"{experiment_name.lower()}_llm_answer\"] = llm_answer\n",
    "        qa[f\"{experiment_name.lower()}_f1\"] = f1\n",
    "        qa[f\"{experiment_name.lower()}_precision\"] = precision\n",
    "        qa[f\"{experiment_name.lower()}_recall\"] = recall\n",
    "\n",
    "    return all_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_results(results_folder_name, experiment_name, df):\n",
    "\n",
    "    df.to_csv(f\"{results_folder_name}/qas_{experiment_name.lower()}_scores.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_scores(scores_l, clip_perc):\n",
    "    scores_sorted_l = sorted(scores_l)\n",
    "\n",
    "    clip_cnt = int(len(scores_sorted_l) * clip_perc / 100)\n",
    "    print(f\"clip_cnt: {clip_cnt}\")\n",
    "\n",
    "    # Now clip both ends by clip_perc percent\n",
    "    clipped_scores_l = scores_sorted_l[clip_cnt:-clip_cnt] if clip_cnt > 0 else scores_sorted_l\n",
    "    return clipped_scores_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bootstrapping(scores_l, results_folder_name, experiment_name, bootstraps_n = BOOTSTRAPS_N):\n",
    "    \n",
    "    mu_hats = []\n",
    "    n = len(scores_l)\n",
    "    scores_l = clip_scores(scores_l, 2.5)\n",
    "    # print(f\"scores_l: {scores_l}\")\n",
    "    for i in range(bootstraps_n):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing sample {i}...\")\n",
    "        bootstrap_sample = random.choices(scores_l, k=n) # sample with replacement\n",
    "        mu_hat = np.mean(bootstrap_sample)\n",
    "        mu_hats.append(mu_hat)\n",
    "\n",
    "    bootstraps_mean, ci = calculate_mean_confidence_interval(mu_hats)\n",
    "    plt = generate_histogram(mu_hats, bootstraps_mean, ci, results_folder_name, experiment_name)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_folder(experiment_name):\n",
    "    start_timestamp_str = get_timestamp_as_string()\n",
    "    results_folder_name = f\"{RESULTS_ROOT}/{experiment_name}/results_{start_timestamp_str}\"\n",
    "    if not os.path.exists(results_folder_name):\n",
    "        os.makedirs(results_folder_name, exist_ok=True)\n",
    "    return results_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(qas, experiment_name, query_prompt_idx, \n",
    "                       context_needed=False, hyde_context_needed=False, \n",
    "                       suppress_answers=False):\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    print(f\"============= Creating results folder for {experiment_name} =============\")\n",
    "    results_folder_name = create_results_folder(experiment_name)\n",
    "    \n",
    "    print(f\"============= Calculating scores for {experiment_name} =============\")\n",
    "    print(f\"SAMPLE_SIZE: {SAMPLE_SIZE}\\n\")\n",
    "    all_preds = calculate_scores(qas, query_prompt_idx, experiment_name, context_needed, suppress_answers)\n",
    "\n",
    "    print(f\"============= Persisting results for {experiment_name} =============\")\n",
    "    df = pd.DataFrame(qas)\n",
    "    persist_results(results_folder_name, experiment_name, df)\n",
    "\n",
    "    print(f\"\\n============= Bootstrapping for {experiment_name} =============\")\n",
    "    print(f\"BOOTSTRAPS_N: {BOOTSTRAPS_N}\")\n",
    "\n",
    "    do_bootstrapping(df[f\"{experiment_name.lower()}_f1\"].dropna().tolist(), results_folder_name, experiment_name, BOOTSTRAPS_N)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times_entry = {}\n",
    "    execution_times_entry[\"experiment_name\"] = experiment_name\n",
    "    execution_times_entry[\"execution_time\"] = execution_time\n",
    "    execution_times_entry[\"sample_size\"] = SAMPLE_SIZE\n",
    "    execution_times_entry[\"bootstrap_n\"] = BOOTSTRAPS_N\n",
    "    execution_times_l.append(execution_times_entry)\n",
    "    return execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a sample from qas for bootstrapping and bootstrapping with Hyde (will be used for all experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qas_for_bootstrapping = df_qas_with_contexts[['qid', 'question', 'question_context', 'hyde_based_context']]\n",
    "df_qas_for_bootstrapping_sample = df_qas_for_bootstrapping.sample(n=SAMPLE_SIZE, replace=True)\n",
    "ldict_qas_for_boostrapping_sample = df_qas_for_bootstrapping_sample.to_dict(orient='records')\n",
    "\n",
    "df_qas_for_hyde_bootstrapping = df_qas_for_bootstrapping[df_qas_for_bootstrapping['hyde_based_context'].notna()]\n",
    "df_qas_for_hyde_bootstrapping_sample = df_qas_for_hyde_bootstrapping.sample(n=SAMPLE_SIZE, replace=True)\n",
    "ldict_qas_for_hyde_bootstrapping_sample = df_qas_for_hyde_bootstrapping_sample.to_dict(orient='records')\n",
    "\n",
    "sample_ldicts = [ldict_qas_for_boostrapping_sample, ldict_qas_for_hyde_bootstrapping_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conducting all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_experiments.iterrows():\n",
    "    if not row[\"include\"]:\n",
    "        continue\n",
    "\n",
    "    sample_ldict = sample_ldicts[row[\"sample_ldicts_idx\"]]\n",
    "    experiment_name = row[\"name\"]\n",
    "    query_prompt_idx = row[\"query_prompt_idx\"]\n",
    "    context_needed = row[\"context_needed\"]\n",
    "    hyde_context_needed = row[\"hyde_context_needed\"]\n",
    "    suppress_answsers = row[\"suppress_answers\"]\n",
    "    \n",
    "    conduct_experiment(sample_ldict, experiment_name, query_prompt_idx,\n",
    "                       context_needed, hyde_context_needed, suppress_answsers)\n",
    "\n",
    "print(f\"\\n================== Execution times (in seconds): ==================================\\n\")\n",
    "df_execution_times = pd.DataFrame(execution_times_l, \n",
    "                                  columns=['experiment_name', 'execution_time', 'sample_size', 'bootstrap_n'])\n",
    "\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "df_execution_times.head(100)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the display format for floats\n",
    "pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_llm_judges_score(runnable_chain, qas_sample, hyde=False):\n",
    "    \n",
    "#     preds = {}\n",
    "\n",
    "#     sample_size = len(qas_sample)\n",
    "#     print(f\"sample_size: {sample_size}\")\n",
    "\n",
    "#     for i, qa in enumerate(qas_sample):\n",
    "\n",
    "#         question = qa[\"question\"]\n",
    "#         if hyde:\n",
    "#             #context = qa[\"hyde_context\"]\n",
    "#             context = get_hyde_based_context(question)\n",
    "#         else:\n",
    "#             context = qa[\"vector_store_context\"]\n",
    "            \n",
    "#         # print(f\"question --> {question}\")\n",
    "#         # print(context)\n",
    "#         response = runnable_chain.invoke({\"context\": context, \"question\": question})\n",
    "                \n",
    "#         qid = squad_scoring.get_qid_from_question(question, dataset)\n",
    "        \n",
    "#         if response.content.upper() == \"DONT KNOW\":\n",
    "#             llm_answer = \"\"\n",
    "#         else:\n",
    "#             llm_answer = response.content\n",
    "\n",
    "#         preds[qid] = llm_answer\n",
    "#         qas_sample[i][\"llm_answer\"] = llm_answer\n",
    "\n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_current_query_prompt = deh_prompts.query_prompts[4]\n",
    "print(f\"judge_current_query_prompt = {judge_current_query_prompt.template}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_answer_tuple(input_string):\n",
    "    # Trim whitespace\n",
    "    parts = input_string.strip()\n",
    "    # Remove parentheses\n",
    "    parts = parts.strip(\"()\")\n",
    "    # Split on comma\n",
    "    parts = parts.split(\", \")\n",
    "    if len(parts) != 2:\n",
    "        #raise ValueError(\"Invalid answer string\")\n",
    "        return (\"NO\", 0)\n",
    "\n",
    "    answer = parts[0].strip().upper()  # \"Yes\" or \"No\"\n",
    "    if answer not in [\"YES\", \"NO\"]:\n",
    "        #raise ValueError(\"Invalid answer string\")\n",
    "        return (\"NO\", 0)\n",
    "    try:\n",
    "        score = float(parts[1])  # Convert score to a float\n",
    "    except ValueError:\n",
    "        return (\"NO\", 0)\n",
    "    \n",
    "    return (answer, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all_judge_verdicts in zip(zip(*all_judge_answers), zip(*all_judge_scores)):\n",
    "#     print(get_judge_verditcs)\n",
    "\n",
    "def get_majority_verdict(judge_verdicts):\n",
    "\n",
    "    judge_answers = judge_verdicts[0]\n",
    "    yes_count = judge_answers.count('YES')\n",
    "    no_count = judge_answers.count('NO')\n",
    "\n",
    "    if no_count > yes_count:\n",
    "        return \"NO\", 0.0\n",
    "    \n",
    "    judge_scores = list(judge_verdicts[1])\n",
    "    return \"YES\", sum(judge_scores) / yes_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_verditcs(all_judge_answers, all_judge_scores):\n",
    "\n",
    "    majority_verdicts = []\n",
    "    for per_question_judge_verdicts in zip(zip(*all_judge_answers), zip(*all_judge_scores)):\n",
    "        majority_verdict = get_majority_verdict(per_question_judge_verdicts)\n",
    "        majority_verdicts.append(majority_verdict)\n",
    "        print(per_question_judge_verdicts)\n",
    "        print(majority_verdict)\n",
    "        print(\"\")\n",
    "\n",
    "    return majority_verdicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llms = [MISTRAL_LATEST, GEMMA2_9B, QWEN2_5_7B]\n",
    "\n",
    "all_judge_answers = []\n",
    "all_judge_scores = []\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "for judge_llm in judge_llms:\n",
    "    judge_chain = judge_current_query_prompt | get_llm(judge_current_query_prompt, True, judge_llm)\n",
    "    judge_answers = []\n",
    "    judge_scores = []\n",
    "\n",
    "    for i, qa in enumerate(sample_ldicts[0]):\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        question = qa[\"question\"]\n",
    "        context = qa[\"question_context\"]\n",
    "        answer_key = experiment_name.lower() + \"_llm_answer\"\n",
    "        answer = qa[answer_key]\n",
    "        # print(f\"Question: {question}\")\n",
    "        # print(f\"Answer: {answer}\")\n",
    "        #for i in range(3):\n",
    "        response = judge_chain.invoke({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "        a, s = convert_string_to_answer_tuple(response.content)\n",
    "        judge_answers.append(a)\n",
    "        judge_scores.append(s)\n",
    "        \n",
    "        # print(f\"Judge Answer: {a}\")\n",
    "        # print(f\"Score: {s}\")\n",
    "        # print(\"\") \n",
    "\n",
    "    # print(f\"judge_answers: {judge_answers}\")\n",
    "    # print(f\"judge_scores: {judge_scores}\")    \n",
    "\n",
    "    all_judge_answers.append(judge_answers)\n",
    "    all_judge_scores.append(judge_scores)\n",
    "\n",
    "# print(f\"all_judge_answers: {all_judge_answers}\")\n",
    "# print(f\"all_judge_scores: {all_judge_scores}\")  \n",
    "\n",
    "final_judges_verdicts = get_majority_verditcs(all_judge_answers, all_judge_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_judges_verdicts\n",
    "\n",
    "THRESHOLD = 0.7\n",
    "filter = [True if val[1] <= THRESHOLD\n",
    "          else False for val in final_judges_verdicts]\n",
    "\n",
    "sum(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_judges_verdicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
