{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "from csv import DictReader\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Prompts\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Runnables\n",
    "from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for squad_scoring and prompts\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import squad_scoring\n",
    "import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders for storing data and the results\n",
    "DATA_ROOT = \"../../../deh_data_results/data\"         # Set to your own data folder\n",
    "RESULTS_ROOT = \"../../../deh_data_results/results\"   # Set to your own results folder\n",
    "HYDE_BASED_CONTEXTS_ROOT = F\"{DATA_ROOT}/hyde_based_contexts\"   # Set to your own results folder\n",
    "\n",
    "# Vector Store Parameters\n",
    "ollama_embedding_model = \"avr/sfr-embedding-mistral\"\n",
    "embeddings = OllamaEmbeddings(model=ollama_embedding_model)\n",
    "persist_directory = f\"{DATA_ROOT}/chroma_deh_rag_db\"\n",
    "collection_name = \"deh_rag_per_article\"\n",
    "\n",
    "# Chunking Parameters\n",
    "NAIVE_CHUNKING = 0\n",
    "PER_CONTEXT_CHUNKING = 1\n",
    "PER_ARTICLE_CHUNKING = 2\n",
    "CHUNKING_METHOD = NAIVE_CHUNKING\n",
    "CHUNK_SQUAD_DATASET = False         # Set to True to vectorize the squad dataset. If False, \n",
    "                                    # then the documents and their embeddings should already\n",
    "                                    # exist in the vector store.\n",
    "\n",
    "# CONTEXT Creation Parameters\n",
    "CREATE_QUESTION_CONTEXTS = False     # Set to True to create question contexts from the vector store; \n",
    "                                    # if False, the question contexts are loaded from a csv file.\n",
    "CREATE_HYDE_CONTEXTS = True         # Set to True to create hyde contexts; if False,\n",
    "                                    # the hyde contexts are loaded from a csv file.                                    \n",
    "\n",
    "# LLM Parameters\n",
    "CHAT_MODEL_NAME = \"llama3.1\"\n",
    "MAX_TOKENS = 100\n",
    "TEMPERATURE = 0.5\n",
    "TOP_P = 0.95\n",
    "FREQUENCY_PENALTY = 0.0\n",
    "PRESENCE_PENALTY = 0.0\n",
    "\n",
    "CURRENT_QUERY_PROMPT_IDX = 0\n",
    "\n",
    "# Bootstrap Parameters\n",
    "SAMPLE_SIZE = 1000\n",
    "BOOTSTRAPS_N = 10000\n",
    "#TODO check if this code is ok for setting the seed\n",
    "# SEED = 42\n",
    "# set_seed = random.seed(SEED)\n",
    "\n",
    "# Experiment Parameters - define all the experiments to run\n",
    "experiments = [{\"name\": \"NO_RAG\", \"rag\": False, \"rag_model\": None, \"query_prompt_idx\": 0, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG\", \"rag\": True, \"rag_model\": \"basic\", \"query_prompt_idx\": 1, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_DONT_LIE\", \"rag\": True, \"rag_model\": \"basic_dont_lie\", \"query_prompt_idx\": 2, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_HYDE\", \"rag\": True, \"rag_model\": \"basic_hyde\", \"query_prompt_idx\": 2, \"conduct\": True},\n",
    "               {\"name\": \"BASIC_RAG_MILVUS\", \"rag\": True, \"rag_model\": \"basic_milvus\", \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_SEMANTIC_CHUNKING\", \"rag\": True, \"rag_model\": \"basic_semantic_chunking\", \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_SUPPRESS_ANSWERS\", \"rag\": True, \"rag_model\": \"basic_suppress_answers\", \"query_prompt_idx\": 2, \"conduct\": False},\n",
    "               {\"name\": \"FULL_RAG\", \"rag\": True, \"rag_model\": \"full\", \"conduct\": False}]\n",
    "\n",
    "PERSIST_ANSWER_SAMPLES = False   # Set to True to persist the llm answers for each sample, for each experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the prompts and a function to get the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompts = [\n",
    "    PromptTemplate(\n",
    "        template=prompts.rag_text_prompts[2],\n",
    "        input_variables = [\"question\"]\n",
    "    ),\n",
    "    # PromptTemplate(\n",
    "    #     template=prompts.rag_text_prompts[1],\n",
    "    #     input_variables = [\"context\", \"question\"]\n",
    "    # ),\n",
    "\n",
    "    PromptTemplate(\n",
    "        template = \"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Please only use the following pieces of retrieved context to answer the question.\n",
    "    Use ten words maximum and keep the answer concise.\n",
    "\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "        input_variables = [\"context\", \"question\"]\n",
    "    ),\n",
    "\n",
    "    PromptTemplate(\n",
    "        template=(\"\"\"\n",
    "                You are an assistant for question-answering tasks.\n",
    "                Use the following pieces of retrieved context to answer the question.\n",
    "                If you don't know the answer, just return 'DONT KNOW'. \n",
    "                If you know the answer, keep it as short and concise as possible,\n",
    "                i.e. to a maximum of a couple of words.\n",
    "\n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "        ),\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template = prompts.hyde_prompts[1],\n",
    "        input_variables = [\"question\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the llm instance, based on the current query prompt\n",
    "def get_llm(current_query_prompt):\n",
    "    llm = ChatOllama(\n",
    "        prompt_template = current_query_prompt,\n",
    "        model = CHAT_MODEL_NAME,\n",
    "        max_tokens = MAX_TOKENS,\n",
    "        temperature = TEMPERATURE,\n",
    "        top_p = TOP_P,\n",
    "        frequency_penalty = FREQUENCY_PENALTY,\n",
    "        presence_penalty = PRESENCE_PENALTY,\n",
    "        gpu_use = True\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus to be added later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Intiialize the Chroma vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name = collection_name,\n",
    "    embedding_function = embeddings,\n",
    "    persist_directory = persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# articles in the dataset:            35\n",
      "# contexts in the dataset:          1204\n",
      "# questions in the dataset:        26232\n",
      "# unique entries in the dataset:   11849\n",
      "# unique questions in the dataset: 11849\n"
     ]
    }
   ],
   "source": [
    "data_file = f\"{DATA_ROOT}/dev-v2.0.json\"\n",
    "dataset = squad_scoring.load_dataset(data_file)\n",
    "\n",
    "articles = []\n",
    "contexts = []\n",
    "qas = []\n",
    "\n",
    "for article in dataset:\n",
    "    title = article[\"title\"]\n",
    "    articles.append(title)\n",
    "    for p in article['paragraphs']:\n",
    "        context = p[\"context\"]\n",
    "        contexts.append(context)\n",
    "        for qa in p['qas']:\n",
    "            question = qa[\"question\"]\n",
    "            id = qa[\"id\"]\n",
    "            is_impossible = qa[\"is_impossible\"]\n",
    "            if is_impossible:\n",
    "                for pa in qa[\"plausible_answers\"]:\n",
    "                    answer = pa[\"text\"]\n",
    "                    qas.append({\"title\": title, \"context\": context, \"qid\": id, \"question\": question, \n",
    "                                \"is_impossible\": is_impossible, \"answer\": answer})\n",
    "            else:\n",
    "                for a in qa[\"answers\"]:\n",
    "                    answer = a[\"text\"]\n",
    "                    qas.append({\"title\": title, \"context\": context, \"qid\": id, \"question\": question, \n",
    "                                \"is_impossible\": is_impossible, \"answer\": answer})\n",
    "\n",
    "# Store dataset as a csv file\n",
    "csv_file = f\"{DATA_ROOT}/dev-v2.0.csv\"\n",
    "with open(csv_file, mode='w') as file:\n",
    "    fieldnames = ['title', 'context', 'qid', 'question', 'is_impossible', 'answer']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for qa in qas:\n",
    "        writer.writerow(qa)\n",
    "\n",
    "# Remove duplicates from qas (there are duplicates in qas, because many possible answers are the same)\n",
    "unique_qas = list({frozenset(item.items()): item for item in qas}.values())\n",
    "\n",
    "# Find unique questions - needed for further processing\n",
    "unique_questions = list(set([qa[\"question\"] for qa in qas]))\n",
    "\n",
    "print(f\"# articles in the dataset:            {len(articles)}\")\n",
    "print(f\"# contexts in the dataset:          {len(contexts)}\")\n",
    "print(f\"# questions in the dataset:        {len(qas)}\")   \n",
    "print(f\"# unique entries in the dataset:   {len(unique_questions)}\")\n",
    "print(f\"# unique questions in the dataset: {len(unique_questions)}\")\n",
    "\n",
    "#TODO check if it makes sense to use the same seed for all experiments\n",
    "#set_seed = random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If configured, chunk the SQuAD dataset and add to the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================\n",
    "def get_vector_store(chunking_method):\n",
    "\n",
    "    collection_name = f\"deh_rag_{chunking_method}\"\n",
    "    return Chroma(\n",
    "        collection_name = collection_name,\n",
    "        embedding_function = embeddings,\n",
    "        persist_directory = persist_directory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_squad_dataset(squad_contexts, chunk_size=400, chunk_overlap=50):\n",
    "\n",
    "    print(f\"Creating contexts for the dataset...\")\n",
    "    chunking_methods = [\"naive\", \"per_context\", \"per_article\"]\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    def get_splitter(chunking_method, chunk_size, chunk_overlap):\n",
    "        \n",
    "        if chunking_method in [\"naive\", \"per_context\", \"per_article\"]:\n",
    "            return RecursiveCharacterTextSplitter(\n",
    "                chunk_size = chunk_size,\n",
    "                chunk_overlap = chunk_overlap\n",
    "            )\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "            return RecursiveCharacterTextSplitter(\n",
    "                chunk_size = chunk_size,\n",
    "                chunk_overlap = chunk_overlap\n",
    "            )\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    def chunk_contexts(contexts, chunk_size, chunk_overlap):\n",
    "        \n",
    "        splitter = get_splitter(chunking_method, chunk_size, chunk_overlap)\n",
    "\n",
    "        if chunking_method == \"naive\":\n",
    "            all_contexts = \"\\n\\n\".join(contexts)\n",
    "            chunks = splitter.create_documents([all_contexts])  \n",
    "        elif chunking_method == \"per_context\":\n",
    "            chunks = []\n",
    "            for context in contexts:\n",
    "                context_specific_chunks = splitter.create_documents([context])\n",
    "                for chunk in context_specific_chunks:\n",
    "                    chunks.append(chunk)\n",
    "        elif chunking_method == \"per_article\":\n",
    "            chunks = []\n",
    "            for article in dataset:\n",
    "                article_contexts = []\n",
    "\n",
    "                for p in article['paragraphs']:\n",
    "                    article_contexts.append(p[\"context\"])\n",
    "\n",
    "                all_article_contexts = \"\\n\\n\".join(article_contexts)\n",
    "                article_chunks = splitter.create_documents([all_article_contexts]) \n",
    "                for article_chunk in article_chunks:\n",
    "                    chunks.append(article_chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    def add_chunks_to_vector_store(chunks, vector_store):\n",
    "        ids = [str(i) for i in list(range(len(chunks)))]\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Adding chunk {i} to the vector store...\")\n",
    "            vector_store.add_documents(documents=[chunk], ids=ids[i]);      \n",
    "\n",
    "\n",
    "    # ============================ MAIN LOOP =========================================\n",
    "    for chunking_method in chunking_methods:\n",
    "\n",
    "        print(f\"Chunking method: {chunking_method}\")\n",
    "        collection_name = f\"deh_rag_{chunking_method}\"\n",
    "        print(f\"Collection name: {collection_name}\")\n",
    "\n",
    "        #vector_store = get_vector_store(chunking_method)\n",
    "        chunks = chunk_contexts(squad_contexts, chunk_size, chunk_overlap)\n",
    "        #chunks = chunk_contexts(squad_contexts, 1000, 100)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata = {\"source\": \"squad\"}\n",
    "        print(f\"Number of chunks --> {len(chunks)}\\n\")  \n",
    "\n",
    "        add_chunks_to_vector_store(chunks, vector_store)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHUNK_SQUAD_DATASET:\n",
    "    chunk_squad_dataset(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# if VECTORIZE_SQUAD_DATASET:\n",
    "#     print(f\"Creating contexts for the dataset...\")\n",
    "\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=400,\n",
    "#         chunk_overlap=50\n",
    "#     )\n",
    "#     all_chunks = []\n",
    "#     for context in contexts:\n",
    "#         chunks = text_splitter.create_documents([context])\n",
    "#         for chunk in chunks:\n",
    "#             all_chunks.append(chunk)\n",
    "        \n",
    "#     print(f\"Number of chunks --> {len(all_chunks)}\\n\")\n",
    "#     print(chunks[0])      \n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         chunk.metadata = {\"source\": \"squad\"}\n",
    "    \n",
    "#     ids = [str(i) for i in list(range(len(all_chunks)))]\n",
    "#     vector_store.add_documents(documents=all_chunks, ids=ids);\n",
    "# else:\n",
    "#     print(\"Not vectorizing the SQuAD dataset...\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# if VECTORIZE_SQUAD_DATASET:\n",
    "#     print(f\"Creating contexts for the dataset...\")\n",
    "\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1000,\n",
    "#         chunk_overlap=100\n",
    "#     )\n",
    "\n",
    "#     all_contexts = \"\\n\\n\".join(contexts)\n",
    "#     chunks = text_splitter.create_documents([all_contexts])   \n",
    "\n",
    "#     print(f\"Number of chunks --> {len(chunks)}\\n\")\n",
    "#     print(chunks[0])      \n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         chunk.metadata = {\"source\": \"squad\"}\n",
    "    \n",
    "#     ids = [str(i) for i in list(range(len(chunks)))]\n",
    "#     vector_store.add_documents(documents=chunks, ids=ids);\n",
    "# else:\n",
    "#     print(\"Not vectorizing the SQuAD dataset...\")\n",
    "\n",
    "# data_file = f\"{DATA_ROOT}/dev-v2.0.json\"\n",
    "# dataset = squad_scoring.load_dataset(data_file)\n",
    "\n",
    "# vector_store = Chroma(\n",
    "#     collection_name = collection_name,\n",
    "#     embedding_function = embeddings,\n",
    "#     persist_directory = persist_directory\n",
    "# )\n",
    "\n",
    "# # --------------------------------------------------------------------------\n",
    "# if VECTORIZE_SQUAD_DATASET:\n",
    "#     # print(f\"Creating contexts for the dataset...\")\n",
    "\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1000,\n",
    "#         chunk_overlap=100\n",
    "#     )\n",
    "#     article_contexts_lens = []\n",
    "#     curr_id = 0\n",
    "\n",
    "#     for article in dataset:\n",
    "#         title = article[\"title\"]\n",
    "#         article_contexts = []\n",
    "\n",
    "#         for p in article['paragraphs']:\n",
    "#             article_contexts.append(p[\"context\"])\n",
    "#         # article_contexts_lens.append(len(article_contexts))\n",
    "#         # print(f\"article_contexts_lens: {article_contexts_lens}\")\n",
    "#         all_article_contexts = \"\\n\\n\".join(article_contexts)\n",
    "#         article_chunks = text_splitter.create_documents([all_article_contexts]) \n",
    "#         for article_chunk in article_chunks:\n",
    "#             article_chunk.metadata = {\"source\": \"squad\", \"article\": title}\n",
    "#         ids = [str(i) for i in list(range(curr_id, curr_id + len(article_chunks)))]\n",
    "#         curr_id += len(article_chunks)\n",
    "#         vector_store.add_documents(documents=article_chunks, ids=ids)\n",
    "\n",
    "\n",
    "#     #print(f\"sum(article_contexts_lens): {sum(article_contexts_lens)}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contexts\n",
    "\n",
    "Questions:\n",
    "\n",
    "- if configured: generate question contexts and then persist\n",
    "- else: read the question and hyde contexts from a .csv file\n",
    "\n",
    "Hyde:\n",
    "\n",
    "- if configured: generate Hyde-based contexts and then persist\n",
    "- else: read the Hyde-based contexts from a .csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_question_contexts(csv_file_path):\n",
    "\n",
    "    # Write the the qas dataset including the question contexts to a CSV file\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"qid\", \"question\", \"title\", \"context\", \"is_impossible\", \"question_context\", \"answer\"])\n",
    "        writer.writeheader()   # Write the header row\n",
    "        writer.writerows(qas)  # Write the data rows\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_hyde_contexts(csv_file_path, hyde_contexts):\n",
    "\n",
    "    # Write the the Hyde contexts to a CSV file\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"question\", \"hyde_context\"])\n",
    "        writer.writeheader()   # Write the header row\n",
    "        writer.writerows(hyde_contexts)  # Write the data rows\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting question contexts (either generate them, or read from a file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/qas_with_question_contexts.csv\"\n",
    "\n",
    "if CREATE_QUESTION_CONTEXTS:\n",
    "    print(f\"Creating question contexts for the dataset and persisting these in a csv file...\")\n",
    "    for i, qa in enumerate(qas):\n",
    "        if i %100 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "        question = qa[\"question\"]\n",
    "        \n",
    "        top_docs = vector_store.similarity_search(\n",
    "            query = question,\n",
    "            k = 5\n",
    "        )\n",
    "        qa[\"question_context\"] = \" \".join([top_doc.page_content for top_doc in top_docs])\n",
    "    \n",
    "    persist_question_contexts(csv_file_path)\n",
    "\n",
    "else:\n",
    "    print(f\"Reading qas and question contexts from a csv file...\")\n",
    "    \n",
    "    # Read the qas (including contexts) from a CSV file\n",
    "    qas = []\n",
    "\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            qas.append({\"qid\": row[0], \"question\": row[1], \"title\": row[2],\n",
    "                        \"context\": row[3], \"is_impossible\": row[4], \n",
    "                        \"question_context\": row[5], \"answer\": row[6]})\n",
    "\n",
    "\n",
    "    # TODO --> !!!!!!!!!!!!!!!!!!!!!! \n",
    "    # qas = random.sample(qas, SAMPLE_SIZE)\n",
    "    # print(f\"len(qas) --> {len(qas)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(qas))\n",
    "qas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyde_contexts_already_done(csv_file_path, suffixes_l):\n",
    "    # if not os.path.exists(csv_file_path):\n",
    "    #     return []\n",
    "\n",
    "    hyde_contexts_already_done = []\n",
    "    for suffix in suffixes_l:\n",
    "        csv_file_path_iter = csv_file_path[:-4] + suffix + \".csv\"\n",
    "        print(f\"Reading hyde contexts from a csv file: {csv_file_path_iter}\")\n",
    "\n",
    "        with open(csv_file_path_iter, mode='r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            \n",
    "            for i, row in enumerate(csv_reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                hyde_contexts_already_done.append({\"question\": row[0], \"hyde_context\": row[1]})\n",
    "    print(f\"len(hyde_contexts_already_done) --> {len(hyde_contexts_already_done)}\")\n",
    "    return hyde_contexts_already_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/hyde_contexts.csv\"\n",
    "\n",
    "hyde_contexts_already_done = get_hyde_contexts_already_done(csv_file_path, ['_1', '_2'])\n",
    "hyde_questions_already_done = [hc[\"question\"] for hc in hyde_contexts_already_done]\n",
    "print(hyde_questions_already_done[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_HYDE_CONTEXTS = False\n",
    "print(DATA_ROOT)\n",
    "if CREATE_HYDE_CONTEXTS:\n",
    "    print(f\"Creating Hyde contexts for the dataset and persisting these in a csv file...\")\n",
    "    unique_questions = list(set([qa[\"question\"] for qa in qas]))\n",
    "    print(f\"Number of unique questions: {len(unique_questions)}\")\n",
    "\n",
    "    current_query_prompt = query_prompts[3]\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "\n",
    "    for i, question in enumerate(unique_questions[:3000]):\n",
    "        #if i %10 == 0:\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(f\"Processing question {i}...\")\n",
    "        print(f\"Question: {question}\")\n",
    "        if question in hyde_questions_already_done:\n",
    "            print(\"Question already done...\")\n",
    "            continue\n",
    "        response = runnable_chain.invoke({\"question\": question})\n",
    "        hyde_contexts_already_done.append({\"question\": question, \"hyde_context\": response.content})\n",
    "        hyde_questions_already_done.append(question)\n",
    "        print(\"\")\n",
    "    \n",
    "    csv_file_path = f\"{DATA_ROOT}/hyde_contexts_2.csv\"\n",
    "    persist_hyde_contexts(csv_file_path, hyde_contexts_already_done)\n",
    "    hyde_contexts = hyde_contexts_already_done\n",
    "\n",
    "else:\n",
    "    print(f\"Reading the Hyde contexts from a csv file...\")\n",
    "\n",
    "    hyde_contexts = []\n",
    "    #with open(csv_file_path, mode='r') as file:\n",
    "    with open(f\"{DATA_ROOT}/hyde_contexts_2_MASTER_COPY_UNBEDINGT_BEHALTEN.csv\", mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            hyde_contexts.append({\"question\": row[0], \"hyde_context\": row[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_s = set([hyde_context[\"question\"] for hyde_context in hyde_contexts])\n",
    "len(hyde_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions that are needed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runnable chain\n",
    "def get_runnable_chain(current_query_prompt, llm):\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "    return runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hyde context for a question\n",
    "# Could be made much more efficient by creating a dictionary of hyde contexts\n",
    "def get_hyde_context(question):\n",
    "    for hc in hyde_contexts:\n",
    "        if hc[\"question\"] == question:\n",
    "            return hc[\"hyde_context\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the LLM answers, using a runnable chain and the sample of questions provided\n",
    "def generate_llm_answers(runnable_chain, qas_sample, hyde=False):\n",
    "    \n",
    "    preds = {}\n",
    "\n",
    "    sample_size = len(qas_sample)\n",
    "    print(f\"sample_size: {sample_size}\")\n",
    "\n",
    "    for i, qa in enumerate(qas_sample):\n",
    "\n",
    "        question = qa[\"question\"]\n",
    "        if hyde:\n",
    "            #context = qa[\"hyde_context\"]\n",
    "            context = get_hyde_context(question)\n",
    "        else:\n",
    "            context = qa[\"vector_store_context\"]\n",
    "            \n",
    "        # print(f\"question --> {question}\")\n",
    "        # print(context)\n",
    "        response = runnable_chain.invoke({\"context\": context, \"question\": question})\n",
    "                \n",
    "        qid = squad_scoring.get_qid_from_question(question, dataset)\n",
    "        \n",
    "        if response.content.upper() == \"DONT KNOW\":\n",
    "            llm_answer = \"\"\n",
    "        else:\n",
    "            llm_answer = response.content\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        qas_sample[i][\"llm_answer\"] = llm_answer\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# Get the metrics for a set of predictions (preds) that have been generated in a run\n",
    "def get_squad_metrics(dataset, preds, verbose=False):\n",
    "    squad_metrics = squad_scoring.calc_squad_metrics(dataset, preds);\n",
    "    return squad_metrics[\"precision\"], squad_metrics[\"recall\"], squad_metrics[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and confidence interval for a list of scores\n",
    "# TODO: Check if this is calculation is correct !!\n",
    "def calculate_mean_confidence_interval(scores_l):\n",
    "\n",
    "    # Calculate mean\n",
    "    mean = np.mean(scores_l)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    \n",
    "    # confidence = 0.95\n",
    "    # n = len(scores_l)\n",
    "    # std_err = stats.sem(scores_l)  # Standard error of the mean\n",
    "    # h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)  # Margin of error\n",
    "    # ci = (mean - h, mean + h)\n",
    "\n",
    "    sample_std_dev = np.std(scores_l, ddof=1)\n",
    "    # n = len(scores_l)\n",
    "    # standard_error = sample_std_dev / np.sqrt(n)\n",
    "    margin_of_error = 1.96 * sample_std_dev\n",
    "    ci = (mean - margin_of_error, mean + margin_of_error)\n",
    "\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram for a list of scores and persist it\n",
    "def generate_histogram(scores_l, mean, ci, results_folder_name, experiment_name):\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(scores_l, bins=30, density=False, edgecolor='black', alpha=0.6, color = 'lightblue' ) # color='aquamarine')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.title(f\"F1-Scores for {experiment_name} - (Bootstraps: {BOOTSTRAPS_N} - Sample Size: {SAMPLE_SIZE})\", fontsize=10)\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    plt.axvline(mean, color='red', linestyle='dotted', linewidth=2, label=f'Mean F1: {round(mean, 2)}')\n",
    "\n",
    "    # Add vertical lines for the 95% confidence interval\n",
    "    plt.axvline(ci[0], color='orange', linestyle='dashdot', linewidth=1.5, label='95% CI Lower')\n",
    "    plt.axvline(ci[1], color='orange', linestyle='dashdot', linewidth=1.5, label='95% CI Upper')\n",
    "\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "    plt.savefig(os.path.join(results_folder_name, f\"{experiment_name}_{BOOTSTRAPS_N}_{SAMPLE_SIZE}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions for Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp and format the start timestamp as a string\n",
    "def get_timestamp_as_string():\n",
    "    start_timestamp = datetime.now()\n",
    "    start_timestamp_str = start_timestamp.strftime('%Y%m%d_%H%M%S')\n",
    "    return start_timestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding f1, precision, recall for NO_RAG\n",
    "\n",
    "def calculate_scores(qas, query_prompt_idx, experiment_name, context_needed=False, suppress_answers=False):\n",
    "\n",
    "    # Create the chain\n",
    "    current_query_prompt = query_prompts[query_prompt_idx]\n",
    "    print(f\"current_query_prompt = {current_query_prompt.template}\\n\")\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = get_runnable_chain(current_query_prompt, llm)\n",
    "\n",
    "    # Generate the LLM answers for all questions and calculate per-answer metrics \n",
    "    preds = {}\n",
    "    for i, qa in enumerate(qas):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "\n",
    "        qid = qa[\"qid\"]\n",
    "        question = qa[\"question\"]\n",
    "\n",
    "        if context_needed:\n",
    "            context = qa[\"question_context\"]\n",
    "            response = runnable_chain.invoke({\"question\": question, \"context\": context})\n",
    "        else:\n",
    "            response = runnable_chain.invoke({\"question\": question})\n",
    "\n",
    "        llm_answer = response.content\n",
    "\n",
    "        if llm_answer.upper() == \"DONT KNOW\":\n",
    "            if suppress_answers:\n",
    "                continue\n",
    "            else:\n",
    "                llm_answer = \"\"\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        scores = squad_scoring.calc_squad_metrics(dataset, preds)\n",
    "        f1 = scores[\"f1\"]\n",
    "        precision = scores[\"precision\"]\n",
    "        recall = scores[\"recall\"]\n",
    "\n",
    "        preds = {}\n",
    "        qa[f\"{experiment_name.lower()}_llm_answer\"] = llm_answer\n",
    "        qa[f\"{experiment_name.lower()}_f1\"] = f1\n",
    "        qa[f\"{experiment_name.lower()}_precision\"] = precision\n",
    "        qa[f\"{experiment_name.lower()}_recall\"] = recall\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_results(results_folder_name, experiment_name, df):\n",
    "\n",
    "    # if not os.path.exists(results_folder_name):\n",
    "    #     os.makedirs(results_folder_name)\n",
    "    # print(f\"results_folder_name: {results_folder_name}\")\n",
    "    df = pd.DataFrame(qas)\n",
    "    df.to_csv(f\"{results_folder_name}/qas_{experiment_name.lower()}_scores.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bootstrapping(scores_l, results_folder_name, experiment_name, bootstraps_n = BOOTSTRAPS_N):\n",
    "\n",
    "    mu_hats = []\n",
    "    n = len(scores_l)\n",
    "    print(f\"scores_l: {scores_l}\")\n",
    "    for i in range(bootstraps_n):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing sample {i}...\")\n",
    "        bootstrap_sample = random.choices(scores_l, k=n) # sample with replacement\n",
    "        mu_hat = np.mean(bootstrap_sample)\n",
    "        mu_hats.append(mu_hat)\n",
    "\n",
    "    bootstraps_mean, ci = calculate_mean_confidence_interval(mu_hats)\n",
    "    print(f\"mu_hats: {mu_hats}\")\n",
    "    generate_histogram(mu_hats, bootstraps_mean, ci, results_folder_name, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_folder(experiment_name):\n",
    "    start_timestamp_str = get_timestamp_as_string()\n",
    "    results_folder_name = f\"{RESULTS_ROOT}/{experiment_name}/results_{start_timestamp_str}\"\n",
    "    if not os.path.exists(results_folder_name):\n",
    "        os.makedirs(results_folder_name, exist_ok=True)\n",
    "    return results_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(qas, experiment_name, query_prompt_idx, context_needed=False, suppress_answers=False):\n",
    "\n",
    "    print(f\"============= Creating results folder for {experiment_name} =============\")\n",
    "    results_folder_name = create_results_folder(experiment_name)\n",
    "    \n",
    "    print(f\"============= Calculating scores for {experiment_name} =============\")\n",
    "    print(f\"SAMPLE_SIZE: {SAMPLE_SIZE}\")\n",
    "    calculate_scores(qas, query_prompt_idx, experiment_name, context_needed, suppress_answers)\n",
    "\n",
    "\n",
    "    print(f\"============= Persisting results for {experiment_name} =============\")\n",
    "    df = pd.DataFrame(qas)\n",
    "    persist_results(results_folder_name, experiment_name, df)\n",
    "\n",
    "    print(f\"============= Bootstrapping for {experiment_name} =============\")\n",
    "    print(f\"BOOTSTRAPS_N: {BOOTSTRAPS_N}\")\n",
    "    do_bootstrapping(df[f\"{experiment_name.lower()}_f1\"].dropna().tolist(), results_folder_name, experiment_name, BOOTSTRAPS_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a sample from qas for bootstrapping (will be used for all experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting qid, question, question_context\n",
    "qas_for_bootstrapping = [{\"qid\": qa[\"qid\"], \"question\": qa[\"question\"], \"question_context\": qa[\"question_context\"]} for qa in qas]\n",
    "# Removing duplicates\n",
    "qas_for_bootstrapping = list({frozenset(item.items()): item for item in qas_for_bootstrapping}.values())\n",
    "# Taking a sample of size SAMPLE_SIZE\n",
    "qas_for_bootstrapping = random.sample(qas_for_bootstrapping, SAMPLE_SIZE)\n",
    "\n",
    "len(qas_for_bootstrapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Resulst for NO_RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduct_experiment(qas_for_bootstrapping, \"NO_RAG\", 0, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating scores for BASIC_RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduct_experiment(qas_for_bootstrapping, \"BASIC_RAG\", 1, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate results for BASIC_RAG_DONT_LIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduct_experiment(qas_for_bootstrapping, \"BASIC_RAG_DONT_LIE\", 2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Caclulation for BASIC_RAG_SUPPRESS_ANSWSERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduct_experiment(qas_for_bootstrapping, \"BASIC_RAG_SUPPRESS_ANSWSERSa\", 2, context_needed=True, suppress_answers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
