{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26875bf",
   "metadata": {},
   "source": [
    "##### Imports Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "29ee970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce660d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c54c78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "\n",
    "from csv import DictReader\n",
    "#from tqdm.autonotebook import tqdm\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "23e1a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Prompts\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a8b764f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for squad_scoring and prompts\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import squad_scoring\n",
    "import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3670c6",
   "metadata": {},
   "source": [
    "##### Set Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e06b8d-0584-4170-940c-3077344ce89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Parameters\n",
    "ollama_embedding_model = \"avr/sfr-embedding-mistral\"\n",
    "embeddings = OllamaEmbeddings(model=ollama_embedding_model)\n",
    "persist_directory = \"./chroma_deh_rag_db\"\n",
    "collection_name = \"deh_rag\"\n",
    "VECTORIZE_SQUAD_DATASET = False\n",
    "CREATE_QUESTION_CONTEXTS = False\n",
    "\n",
    "# LLM Parameters\n",
    "CHAT_MODEL_NAME = \"llama3.1\"\n",
    "MAX_TOKENS = 100\n",
    "TEMPERATURE = 0.5\n",
    "TOP_P = 0.95\n",
    "FREQUENCY_PENALTY = 0.0\n",
    "PRESENCE_PENALTY = 0.0\n",
    "\n",
    "CURRENT_QUERY_PROMPT_IDX = 0\n",
    "\n",
    "# Bootstrap Parameters\n",
    "SAMPLE_SIZE = 30\n",
    "BOOTSTRAP_CNT = 100\n",
    "\n",
    "# Experiment Parameters - define all the experiments to run\n",
    "experiments = [{\"name\": \"NO_RAG\", \"rag\": False, \"rag_model\": None, \"query_prompt_idx\": 0, \"feasible\": True},\n",
    "               {\"name\": \"BASIC_RAG\", \"rag\": True, \"rag_model\": \"basic\", \"query_prompt_idx\": 1, \"feasible\": True},\n",
    "               {\"name\": \"BASIC_RAG_DONT_LIE\", \"rag\": True, \"rag_model\": \"basic_dont_lie\", \"query_prompt_idx\": 2, \"feasible\": True},\n",
    "               {\"name\": \"BASIC_RAG_HYDE\", \"rag\": True, \"rag_model\": \"basic_hyde\", \"feasible\": False},\n",
    "               {\"name\": \"BASIC_RAG_MILVUS\", \"rag\": True, \"rag_model\": \"basic_milvus\", \"feasible\": False},\n",
    "               {\"name\": \"BASIC_RAG_SEMANTIC_CHUNKING\", \"rag\": True, \"rag_model\": \"basic_semantic_chunking\", \"feasible\": False},\n",
    "               {\"name\": \"BASIC_RAG_SUPPRESS_ANSWSERS\", \"rag\": True, \"rag_model\": \"basic_suppress_answers\", \"feasible\": False},\n",
    "               {\"name\": \"FULL_RAG\", \"rag\": True, \"rag_model\": \"full\", \"feasible\": False}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e87057",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus to be added later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "346f2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize the Chroma vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name = collection_name,\n",
    "    embedding_function = embeddings,\n",
    "    persist_directory = persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0328570",
   "metadata": {},
   "source": [
    "##### Load the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f31d8356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#articles in the dataset:     35\n",
      "#contexts in the dataset:   1204\n",
      "#questions in the dataset: 20302\n"
     ]
    }
   ],
   "source": [
    "data_file = \"./data/dev-v2.0.json\"\n",
    "dataset = squad_scoring.load_dataset(data_file)\n",
    "\n",
    "articles = []\n",
    "contexts = []\n",
    "qas = []\n",
    "\n",
    "for article in dataset:\n",
    "    title = article[\"title\"]\n",
    "    articles.append(title)\n",
    "    for p in article['paragraphs']:\n",
    "        context = p[\"context\"]\n",
    "        contexts.append(context)\n",
    "        for qa in p['qas']:\n",
    "            question = qa[\"question\"]\n",
    "            for a in qa[\"answers\"]:\n",
    "                answer = a[\"text\"]\n",
    "                qas.append({\"title\": title, \"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "print(f\"#articles in the dataset:     {len(articles)}\")\n",
    "print(f\"#contexts in the dataset:   {len(contexts)}\")\n",
    "print(f\"#questions in the dataset: {len(qas)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8842693",
   "metadata": {},
   "source": [
    "##### If VECTORIZE_SQUAD_DATASET --> Chunk the SQuAD dataset and add to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "97133f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not vectorizing the SQuAD dataset...\n"
     ]
    }
   ],
   "source": [
    "if VECTORIZE_SQUAD_DATASET:\n",
    "    print(f\"Creating contexts for the dataset...\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    all_contexts = \"\\n\\n\".join(contexts)\n",
    "    chunks = text_splitter.create_documents([all_contexts])   \n",
    "\n",
    "    print(f\"Number of chunks --> {len(chunks)}\\n\")\n",
    "    print(chunks[0])      \n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata = {\"source\": \"squad\"}\n",
    "    \n",
    "    ids = [str(i) for i in list(range(len(chunks)))]\n",
    "    vector_store.add_documents(documents=chunks, ids=ids);\n",
    "else:\n",
    "    print(\"Not vectorizing the SQuAD dataset...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09ac9d",
   "metadata": {},
   "source": [
    "##### If CREATE_QUESTION_CONTEXTS --> generate question contexts and persist - Else --> Read the question contexts from a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "72d86bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_contexts(csv_file_path):\n",
    "\n",
    "    # Write the list of dictionaries to a CSV file\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"context\", \"question\", \"answer\"])\n",
    "        writer.writeheader()  # Write the header row\n",
    "        writer.writerows(qas)  # Write the data rows\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "577ed41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading qas and question contexts from the csv file...\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"./data/qas_with_contexts.csv\"\n",
    "\n",
    "if CREATE_QUESTION_CONTEXTS:\n",
    "    print(f\"Creating question contexts for the dataset and persisting these in a csv file...\")\n",
    "    for i, qa in enumerate(qas):\n",
    "        if i %100 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "        question = qa[\"question\"]\n",
    "        \n",
    "        top_docs = vector_store.similarity_search(\n",
    "            query = question,\n",
    "            k = 5\n",
    "        )\n",
    "        qa[\"context\"] = \" \".join([top_doc.page_content for top_doc in top_docs])\n",
    "    \n",
    "    persist_contexts(csv_file_path)\n",
    "\n",
    "else:\n",
    "    print(f\"Reading qas and question contexts from the csv file...\")\n",
    "    \n",
    "    # Read the qas (including contexts) from CSV file\n",
    "    qas = []\n",
    "\n",
    "    # Reading the CSV file\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            qas.append({\"title\": row[0], \"context\": row[1], \"question\": row[2], \"answer\": row[3]})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da86729",
   "metadata": {},
   "source": [
    "##### Define prompts and functions that are needed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "52a6cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompts = [\n",
    "    PromptTemplate(\n",
    "        template=prompts.rag_text_prompts[2],\n",
    "        input_variables = [\"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template=prompts.rag_text_prompts[1],\n",
    "        input_variables = [\"context\", \"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template=(\"\"\"\n",
    "                You are an assistant for question-answering tasks.\n",
    "                Use the following pieces of retrieved context to answer the question.\n",
    "                If you don't know the answer, just return 'DONT KNOW'. \n",
    "                If you know the answer keep it as short and concise as possible,\n",
    "                i.e. to a maximum of a couple of words.\n",
    "\n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "        ),\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the llm instance, based on the current query prompt\n",
    "def get_llm(current_query_prompt):\n",
    "    llm = ChatOllama(\n",
    "        prompt_template = current_query_prompt,\n",
    "        model = CHAT_MODEL_NAME,\n",
    "        max_tokens = MAX_TOKENS,\n",
    "        temperature = TEMPERATURE,\n",
    "        top_p = TOP_P,\n",
    "        frequency_penalty = FREQUENCY_PENALTY,\n",
    "        presence_penalty = PRESENCE_PENALTY,\n",
    "        gpu_use = True\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "53bd4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runnable chain\n",
    "def get_runnable_chain(current_query_prompt, llm):\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "    return runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the LLM answers, using a runnable chain and a sample of questions\n",
    "def generate_llm_answers(runnable_chain, qas_sample):\n",
    "    \n",
    "    preds = {}\n",
    "\n",
    "    sample_size = len(qas_sample)\n",
    "    print(f\"sample_size: {sample_size}\\n\")\n",
    "\n",
    "    for i, qa in enumerate(qas_sample):\n",
    "\n",
    "        response = runnable_chain.invoke({\"context\": qa[\"context\"], \"question\": qa[\"question\"]})\n",
    "        \n",
    "        question = qa[\"question\"]\n",
    "        qid = squad_scoring.get_qid_from_question(question, dataset)\n",
    "        \n",
    "        if response.content == \"DONT KNOW\":\n",
    "            llm_answer = \"\"\n",
    "        else:\n",
    "            llm_answer = response.content\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        qas_sample[i][\"llm_answer\"] = llm_answer\n",
    "\n",
    "    # print(f\"\\nFinished generating predictions for {sample_size} questions...\")\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# Get the metrics for a set of predictions (preds) that have been generated in a run\n",
    "def get_squad_metrics(dataset, preds, verbose=False):\n",
    "    squad_metrics = squad_scoring.calc_squad_metrics(dataset, preds);\n",
    "    return squad_metrics[\"precision\"], squad_metrics[\"recall\"], squad_metrics[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to persist a qas sample, including a copy with the LLM answers\n",
    "def persist_qas_sample_results(qas_sample, run_id, results_folder_name):\n",
    "    csv_file_path = f\"{results_folder_name}/qas_sample_with_llm_answers_{run_id}.csv\"\n",
    "    csv_file_path_squad_format = f\"{results_folder_name}/qas_sample_with_llm_answers_{run_id}.csv\"\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"context\", \"question\", \"answer\", \"llm_answer\"])\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        # Write rows\n",
    "        writer.writerows(qas_sample)\n",
    "\n",
    "    qas_sample_squad_format = [{\"question\": qa[\"question\"], \"answer\": qa[\"llm_answer\"]} for qa in qas_sample]\n",
    "    # Write to CSV in SQuAD format\n",
    "    with open(csv_file_path_squad_format, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"question\", \"answer\"])\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        # Write rows\n",
    "        writer.writerows(qas_sample_squad_format)\n",
    "\n",
    "    print(f\"Data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct one single experiment (e.g. NO_RAG, BASIC_RAG, etc.)\n",
    "def conduct_experiment(experiment, persist_answser_samples=False, results_folder_name=None):\n",
    "    \n",
    "    current_query_prompt = query_prompts[experiment[\"query_prompt_idx\"]]\n",
    "    print(f\"current_query_prompt = {current_query_prompt.template}\\n\")\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = get_runnable_chain(current_query_prompt, llm)\n",
    "\n",
    "    # Lists that will contain experiment metrics\n",
    "    precision_l = []\n",
    "    recall_l = []\n",
    "    f1_l = []\n",
    "\n",
    "    for i in range(BOOTSTRAP_CNT):\n",
    "        print(f\"{experiment[\"name\"]} - Bootstrap: {i + 1} of {BOOTSTRAP_CNT}...\")\n",
    "        \n",
    "        qas_sample = random.sample(qas, SAMPLE_SIZE)\n",
    "        preds = generate_llm_answers(runnable_chain, qas_sample);\n",
    "        precision, recall, f1 = get_squad_metrics(dataset, preds)\n",
    "\n",
    "        precision_l.append(precision)\n",
    "        recall_l.append(recall)\n",
    "        f1_l.append(f1)\n",
    "\n",
    "        if persist_answser_samples:\n",
    "            persist_qas_sample_results(qas_sample, i, results_folder_name)\n",
    "        \n",
    "    return precision_l, recall_l, f1_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adaa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and confidence interval for a list of scores\n",
    "# TODO: Check if this is calculation is correct !!\n",
    "def calculate_mean_confidence_interval(scores_l):\n",
    "\n",
    "    # Calculate mean\n",
    "    mean = np.mean(scores_l)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    confidence = 0.95\n",
    "    n = len(scores_l)\n",
    "    std_err = stats.sem(scores_l)  # Standard error of the mean\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)  # Margin of error\n",
    "    ci = (mean - h, mean + h)\n",
    "\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram for a list of scores and persist it\n",
    "def generate_histogram(scores_l, mean, ci, results_folder_name, experiment_name):\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(scores_l, bins=30, density=True, edgecolor='black', alpha=0.6, color = 'lightblue' ) # color='aquamarine')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.title(f\"F1-Scores for {experiment_name} - (Bootstraps: {BOOTSTRAP_CNT} - Sample Size: {SAMPLE_SIZE})\", fontsize=10)\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    plt.axvline(mean, color='red', linestyle='dotted', linewidth=2, label=f'Mean F1: {round(mean, 2)}')\n",
    "\n",
    "    # Add vertical lines for the 95% confidence interval\n",
    "    plt.axvline(ci[0], color='orange', linestyle='dashdot', linewidth=1.5, label='95% CI Lower')\n",
    "    plt.axvline(ci[1], color='orange', linestyle='dashdot', linewidth=1.5, label='95% CI Upper')\n",
    "\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "    plt.savefig(os.path.join(results_folder_name, f\"{experiment_name}_{BOOTSTRAP_CNT}_{SAMPLE_SIZE}\"))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the metrics for an experiment in a CSV file\n",
    "def persist_metrics(results_folder_name, precision_l, recall_l, f1_l, experiment_name):\n",
    "    # Combine lists into rows\n",
    "    rows = zip(precision_l, recall_l, f1_l)\n",
    "\n",
    "    # Write to a CSV file\n",
    "    csv_file_path = f\"./{results_folder_name}/{experiment_name}_metrics_{BOOTSTRAP_CNT}_{SAMPLE_SIZE}.csv\"\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file) \n",
    "        writer.writerow([\"precision\", \"recall\", \"f1\"]) # Write header\n",
    "        writer.writerows(rows) # Write data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68a80d",
   "metadata": {},
   "source": [
    "##### Conduct all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c9f59415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp\n",
    "start_timestamp = datetime.now()\n",
    "\n",
    "# Format the start timestamp as a string\n",
    "start_timestamp_str = start_timestamp.strftime('%Y%m%d_%H%M%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Conducting experiment: NO_RAG\n",
      "results_folder_name: ./results/results_20241124_231452/results_NO_RAG\n",
      "\n",
      "current_query_prompt = \n",
      "    You are an assistant for question-answering tasks.\n",
      "    Use ten words maximum and keep the answer concise.\n",
      "\n",
      "    Question: {question}\n",
      "\n",
      "    Answer:\n",
      "    \n",
      "\n",
      "NO_RAG - Bootstrap: 1 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 2 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 3 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 4 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 5 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 6 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 7 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 8 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 9 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 10 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 11 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 12 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 13 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 14 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 15 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 16 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 17 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 18 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 19 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 20 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 21 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 22 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 23 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 24 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 25 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 26 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 27 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 28 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 29 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 30 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 31 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 32 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 33 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 34 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 35 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 36 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 37 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 38 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 39 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 40 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 41 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 42 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 43 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 44 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 45 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 46 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 47 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 48 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 49 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 50 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 51 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 52 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 53 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 54 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 55 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 56 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 57 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 58 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 59 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 60 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 61 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 62 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 63 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 64 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 65 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 66 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 67 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 68 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 69 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 70 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 71 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 72 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 73 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 74 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 75 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 76 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 77 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 78 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 79 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 80 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 81 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 82 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 83 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 84 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 85 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 86 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 87 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 88 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 89 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 90 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 91 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 92 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 93 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 94 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 95 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 96 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 97 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 98 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 99 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "NO_RAG - Bootstrap: 100 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Conducting experiment: BASIC_RAG\n",
      "results_folder_name: ./results/results_20241124_231452/results_BASIC_RAG\n",
      "\n",
      "current_query_prompt = \n",
      "    You are an assistant for question-answering tasks.\n",
      "    Use the following pieces of retrieved context to answer the question.\n",
      "    Use ten words maximum and keep the answer concise.\n",
      "\n",
      "    Question: {question}\n",
      "    Context: {context}\n",
      "\n",
      "    Answer:\n",
      "    \n",
      "\n",
      "BASIC_RAG - Bootstrap: 1 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 2 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 3 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 4 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 5 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 6 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 7 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 8 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 9 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 10 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 11 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 12 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 13 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 14 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 15 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 16 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 17 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 18 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 19 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 20 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 21 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 22 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 23 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 24 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 25 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 26 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 27 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 28 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 29 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 30 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 31 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 32 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 33 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 34 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 35 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 36 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 37 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 38 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 39 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 40 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 41 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 42 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 43 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 44 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 45 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 46 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 47 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 48 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 49 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 50 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 51 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 52 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 53 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 54 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 55 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 56 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 57 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 58 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 59 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 60 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 61 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 62 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 63 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 64 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 65 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 66 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 67 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 68 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 69 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 70 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 71 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 72 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 73 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 74 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 75 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 76 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 77 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 78 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 79 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 80 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 81 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 82 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 83 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 84 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 85 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 86 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 87 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 88 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 89 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 90 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 91 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 92 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 93 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 94 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 95 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 96 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 97 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 98 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 99 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG - Bootstrap: 100 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Conducting experiment: BASIC_RAG_DONT_LIE\n",
      "results_folder_name: ./results/results_20241124_231452/results_BASIC_RAG_DONT_LIE\n",
      "\n",
      "current_query_prompt = \n",
      "                You are an assistant for question-answering tasks.\n",
      "                Use the following pieces of retrieved context to answer the question.\n",
      "                If you don't know the answer, just return 'DONT KNOW'. \n",
      "                If you know the answer keep it as short and concise as possible,\n",
      "                i.e. to a maximum of a couple of words.\n",
      "\n",
      "                Question: {question}\n",
      "                Context: {context}\n",
      "\n",
      "                Answer:\n",
      "                \n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 1 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 2 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 3 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 4 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 5 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 6 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 7 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 8 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 9 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 10 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 11 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 12 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 13 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 14 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 15 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 16 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 17 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 18 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 19 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 20 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 21 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 22 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 23 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 24 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 25 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 26 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 27 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 28 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 29 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 30 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 31 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 32 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 33 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 34 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 35 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 36 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 37 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 38 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 39 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 40 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 41 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 42 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 43 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 44 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 45 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 46 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 47 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 48 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 49 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 50 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 51 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 52 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 53 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 54 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 55 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 56 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 57 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 58 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 59 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 60 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 61 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 62 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 63 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 64 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 65 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 66 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 67 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 68 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 69 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 70 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 71 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 72 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 73 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 74 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 75 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 76 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 77 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 78 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 79 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 80 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 81 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 82 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 83 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 84 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 85 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 86 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 87 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 88 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 89 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 90 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 91 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 92 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 93 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 94 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 95 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 96 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 97 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 98 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 99 of 100...\n",
      "sample_size: 30\n",
      "\n",
      "BASIC_RAG_DONT_LIE - Bootstrap: 100 of 100...\n",
      "sample_size: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "\n",
    "for experiment in experiments:\n",
    "    if not experiment[\"feasible\"]:  # Skip experiments that are not feasible for the moment\n",
    "        continue\n",
    "    \n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Conducting experiment: {experiment['name']}\")\n",
    "\n",
    "    results_folder_name = f\"./results/results_{start_timestamp_str}/results_{experiment['name']}\\n\"\n",
    "    os.makedirs(results_folder_name)\n",
    "    print(f\"results_folder_name: {results_folder_name}\")\n",
    "\n",
    "    precision_l, recall_l, f1_l = conduct_experiment(experiment) #, persist_answser_samples=True, results_folder_name=results_folder_name)  \n",
    "\n",
    "    # Remove 2.5% from each tail of f1_l (precision_l and recall_l are not taken into account)\n",
    "    remove_cnt = int(BOOTSTRAP_CNT * 0.05)\n",
    "    f1_l = f1_l[remove_cnt:-remove_cnt]\n",
    "    mean, ci = calculate_mean_confidence_interval(f1_l)\n",
    "\n",
    "    plt = generate_histogram(f1_l, mean, ci, results_folder_name, experiment[\"name\"])\n",
    "    persist_metrics(results_folder_name, precision_l, recall_l, f1_l, experiment[\"name\"])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for conducting all experiments: 1:00:07.267766\n"
     ]
    }
   ],
   "source": [
    "# Calculate the time elapsed for conducting all experiments\n",
    "end_timestamp = datetime.now()\n",
    "\n",
    "print(f\"Elapsed time for conducting all experiments: {end_timestamp - start_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf7414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
