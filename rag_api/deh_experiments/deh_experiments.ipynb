{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26875bf",
   "metadata": {},
   "source": [
    "##### Imports Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce660d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54c78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "\n",
    "from csv import DictReader\n",
    "#from tqdm.autonotebook import tqdm\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e1a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Prompts\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b764f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for squad_scoring and prompts\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import squad_scoring\n",
    "import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3670c6",
   "metadata": {},
   "source": [
    "##### Set Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e06b8d-0584-4170-940c-3077344ce89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders for storing data and the results\n",
    "DATA_ROOT = \"../../../deh_data_results/data\"         # Set to your own data folder\n",
    "RESULTS_ROOT = \"../../../deh_data_results/results\"   # Set to your own results folder\n",
    "\n",
    "# Vector Store Parameters\n",
    "ollama_embedding_model = \"avr/sfr-embedding-mistral\"\n",
    "embeddings = OllamaEmbeddings(model=ollama_embedding_model)\n",
    "persist_directory = f\"{DATA_ROOT}/chroma_deh_rag_db\"\n",
    "collection_name = \"deh_rag\"\n",
    "VECTORIZE_SQUAD_DATASET = False     # Set to True to vectorize the squad dataset. If False, \n",
    "                                    # then the documents and their embeddings should already\n",
    "                                    # exist in the vector store.\n",
    "CREATE_QUESTION_CONTEXTS = False    # Set to True to create question contexts; if False, \n",
    "                                    # the question contexts are loaded from a csv file.\n",
    "CREATE_HYDE_CONTEXTS = True         # Set to True to create hyde contexts; if False,\n",
    "                                    # the hyde contexts are loaded from a csv file.                                    \n",
    "\n",
    "# LLM Parameters\n",
    "CHAT_MODEL_NAME = \"llama3.1\"\n",
    "MAX_TOKENS = 100\n",
    "TEMPERATURE = 0.5\n",
    "TOP_P = 0.95\n",
    "FREQUENCY_PENALTY = 0.0\n",
    "PRESENCE_PENALTY = 0.0\n",
    "\n",
    "CURRENT_QUERY_PROMPT_IDX = 0\n",
    "\n",
    "# Bootstrap Parameters\n",
    "SAMPLE_SIZE = 30\n",
    "BOOTSTRAP_CNT = 30\n",
    "\n",
    "# Experiment Parameters - define all the experiments to run\n",
    "experiments = [{\"name\": \"NO_RAG\", \"rag\": False, \"rag_model\": None, \"query_prompt_idx\": 0, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG\", \"rag\": True, \"rag_model\": \"basic\", \"query_prompt_idx\": 1, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_DONT_LIE\", \"rag\": True, \"rag_model\": \"basic_dont_lie\", \"query_prompt_idx\": 2, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_HYDE\", \"rag\": True, \"rag_model\": \"basic_hyde\", \"conduct\": True},\n",
    "               {\"name\": \"BASIC_RAG_MILVUS\", \"rag\": True, \"rag_model\": \"basic_milvus\", \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_SEMANTIC_CHUNKING\", \"rag\": True, \"rag_model\": \"basic_semantic_chunking\", \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_SUPPRESS_ANSWERS\", \"rag\": True, \"rag_model\": \"basic_suppress_answers\", \"query_prompt_idx\": 2, \"conduct\": False},\n",
    "               {\"name\": \"FULL_RAG\", \"rag\": True, \"rag_model\": \"full\", \"conduct\": False}]\n",
    "\n",
    "PERSIST_ANSWER_SAMPLES = False   # Set to True to persist the llm answers for each sample, for each experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5812d",
   "metadata": {},
   "source": [
    "##### Define the prompts and a function to get the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf394442",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompts = [\n",
    "    PromptTemplate(\n",
    "        template=prompts.rag_text_prompts[2],\n",
    "        input_variables = [\"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template=prompts.rag_text_prompts[1],\n",
    "        input_variables = [\"context\", \"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template=(\"\"\"\n",
    "                You are an assistant for question-answering tasks.\n",
    "                Use the following pieces of retrieved context to answer the question.\n",
    "                If you don't know the answer, just return 'DONT KNOW'. \n",
    "                If you know the answer keep it as short and concise as possible,\n",
    "                i.e. to a maximum of a couple of words.\n",
    "\n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "        ),\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template = prompts.hyde_prompts[1],\n",
    "        input_variables = [\"question\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0888af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the llm instance, based on the current query prompt\n",
    "def get_llm(current_query_prompt):\n",
    "    llm = ChatOllama(\n",
    "        prompt_template = current_query_prompt,\n",
    "        model = CHAT_MODEL_NAME,\n",
    "        max_tokens = MAX_TOKENS,\n",
    "        temperature = TEMPERATURE,\n",
    "        top_p = TOP_P,\n",
    "        frequency_penalty = FREQUENCY_PENALTY,\n",
    "        presence_penalty = PRESENCE_PENALTY,\n",
    "        gpu_use = True\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e87057",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus to be added later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346f2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize the Chroma vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name = collection_name,\n",
    "    embedding_function = embeddings,\n",
    "    persist_directory = persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0328570",
   "metadata": {},
   "source": [
    "##### Load the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31d8356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#articles in the dataset:     35\n",
      "#contexts in the dataset:   1204\n",
      "#questions in the dataset: 20302\n"
     ]
    }
   ],
   "source": [
    "data_file = f\"{DATA_ROOT}/dev-v2.0.json\"\n",
    "dataset = squad_scoring.load_dataset(data_file)\n",
    "\n",
    "articles = []\n",
    "contexts = []\n",
    "qas = []\n",
    "\n",
    "for article in dataset:\n",
    "    title = article[\"title\"]\n",
    "    articles.append(title)\n",
    "    for p in article['paragraphs']:\n",
    "        context = p[\"context\"]\n",
    "        contexts.append(context)\n",
    "        for qa in p['qas']:\n",
    "            question = qa[\"question\"]\n",
    "            for a in qa[\"answers\"]:\n",
    "                answer = a[\"text\"]\n",
    "                qas.append({\"title\": title, \"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "print(f\"#articles in the dataset:     {len(articles)}\")\n",
    "print(f\"#contexts in the dataset:   {len(contexts)}\")\n",
    "print(f\"#questions in the dataset: {len(qas)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8842693",
   "metadata": {},
   "source": [
    "##### If configured, chunk the SQuAD dataset and add to the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97133f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not vectorizing the SQuAD dataset...\n"
     ]
    }
   ],
   "source": [
    "if VECTORIZE_SQUAD_DATASET:\n",
    "    print(f\"Creating contexts for the dataset...\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    all_contexts = \"\\n\\n\".join(contexts)\n",
    "    chunks = text_splitter.create_documents([all_contexts])   \n",
    "\n",
    "    print(f\"Number of chunks --> {len(chunks)}\\n\")\n",
    "    print(chunks[0])      \n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata = {\"source\": \"squad\"}\n",
    "    \n",
    "    ids = [str(i) for i in list(range(len(chunks)))]\n",
    "    vector_store.add_documents(documents=chunks, ids=ids);\n",
    "else:\n",
    "    print(\"Not vectorizing the SQuAD dataset...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09ac9d",
   "metadata": {},
   "source": [
    "##### Contexts\n",
    "\n",
    "Questions:\n",
    "\n",
    "- if configured: generate question contexts and then persist\n",
    "- else: read the question and hyde contexts from a .csv file\n",
    "\n",
    "Hyde:\n",
    "\n",
    "- if configured: generate Hyde contexts and then persist\n",
    "- else: read the Hyde contexts from a .csv fil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d86bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_question_contexts(csv_file_path):\n",
    "\n",
    "    # Write the the qas dataset including the question contexts to a CSV file\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"context\", \"question\", \"answer\", \"question_context\"])\n",
    "        writer.writeheader()   # Write the header row\n",
    "        writer.writerows(qas)  # Write the data rows\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b8a03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_hyde_contexts(csv_file_path):\n",
    "\n",
    "    # Write the the Hyde contexts to a CSV file\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"question\", \"hyde_context\"])\n",
    "        writer.writeheader()   # Write the header row\n",
    "        writer.writerows(qas)  # Write the data rows\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file_path}\")\n",
    "\n",
    "# def generate_hyde_context(question):\n",
    "    \n",
    "#     current_query_prompt = query_prompts[3]\n",
    "#     llm = get_llm(current_query_prompt)\n",
    "#     runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "#     response = runnable_chain.invoke({\"question\": question})\n",
    "#     return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577ed41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating question contexts for the dataset and persisting these in a csv file...\n",
      "Processing question 0...\n",
      "Processing question 100...\n",
      "Processing question 200...\n",
      "Processing question 300...\n",
      "Processing question 400...\n",
      "Processing question 500...\n",
      "Processing question 600...\n",
      "Processing question 700...\n",
      "Processing question 800...\n",
      "Processing question 900...\n",
      "Processing question 1000...\n",
      "Processing question 1100...\n",
      "Processing question 1200...\n",
      "Processing question 1300...\n",
      "Processing question 1400...\n",
      "Processing question 1500...\n",
      "Processing question 1600...\n",
      "Processing question 1700...\n",
      "Processing question 1800...\n",
      "Processing question 1900...\n",
      "Processing question 2000...\n",
      "Processing question 2100...\n",
      "Processing question 2200...\n",
      "Processing question 2300...\n",
      "Processing question 2400...\n",
      "Processing question 2500...\n",
      "Processing question 2600...\n",
      "Processing question 2700...\n",
      "Processing question 2800...\n",
      "Processing question 2900...\n",
      "Processing question 3000...\n",
      "Processing question 3100...\n",
      "Processing question 3200...\n",
      "Processing question 3300...\n",
      "Processing question 3400...\n",
      "Processing question 3500...\n",
      "Processing question 3600...\n",
      "Processing question 3700...\n",
      "Processing question 3800...\n",
      "Processing question 3900...\n",
      "Processing question 4000...\n",
      "Processing question 4100...\n",
      "Processing question 4200...\n",
      "Processing question 4300...\n",
      "Processing question 4400...\n",
      "Processing question 4500...\n",
      "Processing question 4600...\n",
      "Processing question 4700...\n",
      "Processing question 4800...\n",
      "Processing question 4900...\n",
      "Processing question 5000...\n",
      "Processing question 5100...\n",
      "Processing question 5200...\n",
      "Processing question 5300...\n",
      "Processing question 5400...\n",
      "Processing question 5500...\n",
      "Processing question 5600...\n",
      "Processing question 5700...\n",
      "Processing question 5800...\n",
      "Processing question 5900...\n",
      "Processing question 6000...\n",
      "Processing question 6100...\n",
      "Processing question 6200...\n",
      "Processing question 6300...\n",
      "Processing question 6400...\n",
      "Processing question 6500...\n",
      "Processing question 6600...\n",
      "Processing question 6700...\n",
      "Processing question 6800...\n",
      "Processing question 6900...\n",
      "Processing question 7000...\n",
      "Processing question 7100...\n",
      "Processing question 7200...\n",
      "Processing question 7300...\n",
      "Processing question 7400...\n",
      "Processing question 7500...\n",
      "Processing question 7600...\n",
      "Processing question 7700...\n",
      "Processing question 7800...\n",
      "Processing question 7900...\n",
      "Processing question 8000...\n",
      "Processing question 8100...\n",
      "Processing question 8200...\n",
      "Processing question 8300...\n",
      "Processing question 8400...\n",
      "Processing question 8500...\n",
      "Processing question 8600...\n",
      "Processing question 8700...\n",
      "Processing question 8800...\n",
      "Processing question 8900...\n",
      "Processing question 9000...\n",
      "Processing question 9100...\n",
      "Processing question 9200...\n",
      "Processing question 9300...\n",
      "Processing question 9400...\n",
      "Processing question 9500...\n",
      "Processing question 9600...\n",
      "Processing question 9700...\n",
      "Processing question 9800...\n",
      "Processing question 9900...\n",
      "Processing question 10000...\n",
      "Processing question 10100...\n",
      "Processing question 10200...\n",
      "Processing question 10300...\n",
      "Processing question 10400...\n",
      "Processing question 10500...\n",
      "Processing question 10600...\n",
      "Processing question 10700...\n",
      "Processing question 10800...\n",
      "Processing question 10900...\n",
      "Processing question 11000...\n",
      "Processing question 11100...\n",
      "Processing question 11200...\n",
      "Processing question 11300...\n",
      "Processing question 11400...\n",
      "Processing question 11500...\n",
      "Processing question 11600...\n",
      "Processing question 11700...\n",
      "Processing question 11800...\n",
      "Processing question 11900...\n",
      "Processing question 12000...\n",
      "Processing question 12100...\n",
      "Processing question 12200...\n",
      "Processing question 12300...\n",
      "Processing question 12400...\n",
      "Processing question 12500...\n",
      "Processing question 12600...\n",
      "Processing question 12700...\n",
      "Processing question 12800...\n",
      "Processing question 12900...\n",
      "Processing question 13000...\n",
      "Processing question 13100...\n",
      "Processing question 13200...\n",
      "Processing question 13300...\n",
      "Processing question 13400...\n",
      "Processing question 13500...\n",
      "Processing question 13600...\n",
      "Processing question 13700...\n",
      "Processing question 13800...\n",
      "Processing question 13900...\n",
      "Processing question 14000...\n",
      "Processing question 14100...\n",
      "Processing question 14200...\n",
      "Processing question 14300...\n",
      "Processing question 14400...\n",
      "Processing question 14500...\n",
      "Processing question 14600...\n",
      "Processing question 14700...\n",
      "Processing question 14800...\n",
      "Processing question 14900...\n",
      "Processing question 15000...\n",
      "Processing question 15100...\n",
      "Processing question 15200...\n",
      "Processing question 15300...\n",
      "Processing question 15400...\n",
      "Processing question 15500...\n",
      "Processing question 15600...\n",
      "Processing question 15700...\n",
      "Processing question 15800...\n",
      "Processing question 15900...\n",
      "Processing question 16000...\n",
      "Processing question 16100...\n",
      "Processing question 16200...\n",
      "Processing question 16300...\n",
      "Processing question 16400...\n",
      "Processing question 16500...\n",
      "Processing question 16600...\n",
      "Processing question 16700...\n",
      "Processing question 16800...\n",
      "Processing question 16900...\n",
      "Processing question 17000...\n",
      "Processing question 17100...\n",
      "Processing question 17200...\n",
      "Processing question 17300...\n",
      "Processing question 17400...\n",
      "Processing question 17500...\n",
      "Processing question 17600...\n",
      "Processing question 17700...\n",
      "Processing question 17800...\n",
      "Processing question 17900...\n",
      "Processing question 18000...\n",
      "Processing question 18100...\n",
      "Processing question 18200...\n",
      "Processing question 18300...\n",
      "Processing question 18400...\n",
      "Processing question 18500...\n",
      "Processing question 18600...\n",
      "Processing question 18700...\n",
      "Processing question 18800...\n",
      "Processing question 18900...\n",
      "Processing question 19000...\n",
      "Processing question 19100...\n",
      "Processing question 19200...\n",
      "Processing question 19300...\n",
      "Processing question 19400...\n",
      "Processing question 19500...\n",
      "Processing question 19600...\n",
      "Processing question 19700...\n",
      "Processing question 19800...\n",
      "Processing question 19900...\n",
      "Processing question 20000...\n",
      "Processing question 20100...\n",
      "Processing question 20200...\n",
      "Processing question 20300...\n",
      "Data successfully written to ../../../deh_data_results/data/qas_with_question_contexts.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/qas_with_question_contexts.csv\"\n",
    "\n",
    "if CREATE_QUESTION_CONTEXTS:\n",
    "    print(f\"Creating question contexts for the dataset and persisting these in a csv file...\")\n",
    "    for i, qa in enumerate(qas):\n",
    "        if i %100 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "        question = qa[\"question\"]\n",
    "        \n",
    "        top_docs = vector_store.similarity_search(\n",
    "            query = question,\n",
    "            k = 5\n",
    "        )\n",
    "        qa[\"question_context\"] = \" \".join([top_doc.page_content for top_doc in top_docs])\n",
    "    \n",
    "    persist_question_contexts(csv_file_path)\n",
    "\n",
    "else:\n",
    "    print(f\"Reading qas and question contexts from a csv file...\")\n",
    "    \n",
    "    # Read the qas (including contexts) from a CSV file\n",
    "    qas = []\n",
    "\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            qas.append({\"title\": row[0], \"context\": row[1], \"question\": row[2], \"answer\": row[3],\n",
    "                        \"question_context\": row[4]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a52c44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Hyde contexts for the dataset and persisting these in a csv file...\n",
      "Number of unique questions: 5926\n",
      "Processing question 0...\n",
      "Processing question 1...\n",
      "Processing question 2...\n",
      "Processing question 3...\n",
      "Processing question 4...\n",
      "Processing question 5...\n",
      "Processing question 6...\n",
      "Processing question 7...\n",
      "Processing question 8...\n",
      "Processing question 9...\n",
      "Processing question 10...\n",
      "Processing question 11...\n",
      "Processing question 12...\n",
      "Processing question 13...\n",
      "Processing question 14...\n",
      "Processing question 15...\n",
      "Processing question 16...\n",
      "Processing question 17...\n",
      "Processing question 18...\n",
      "Processing question 19...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'answer', 'question_context', 'title', 'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         response \u001b[38;5;241m=\u001b[39m runnable_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[1;32m     18\u001b[0m         hyde_contexts\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyde_context\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mcontent})\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mpersist_hyde_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading the Hyde contexts from a csv file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mpersist_hyde_contexts\u001b[0;34m(csv_file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(file, fieldnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyde_context\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()   \u001b[38;5;66;03m# Write the header row\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqas\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Write the data rows\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData successfully written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/csv.py:167\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[0;34m(self, rowdicts)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterows\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdicts):\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowdicts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/csv.py:159\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    157\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[0;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'answer', 'question_context', 'title', 'context'"
     ]
    }
   ],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/hyde_contexts.csv\"\n",
    "\n",
    "hyde_contexts = []\n",
    "\n",
    "if CREATE_HYDE_CONTEXTS:\n",
    "    print(f\"Creating Hyde contexts for the dataset and persisting these in a csv file...\")\n",
    "    unique_questions = list(set([qa[\"question\"] for qa in qas]))\n",
    "    print(f\"Number of unique questions: {len(unique_questions)}\")\n",
    "\n",
    "    current_query_prompt = query_prompts[3]\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "\n",
    "    for i, question in enumerate(unique_questions[:20]):\n",
    "        #if i %10 == 0:\n",
    "        print(f\"Processing question {i}...\")\n",
    "        response = runnable_chain.invoke({\"question\": question})\n",
    "        hyde_contexts.append({\"question\": question, \"hyde_context\": response.content})\n",
    "    \n",
    "    persist_hyde_contexts(csv_file_path)\n",
    "\n",
    "else:\n",
    "    print(f\"Reading the Hyde contexts from a csv file...\")\n",
    "\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            hyde_contexts.append({\"question\": row[0], \"hyde_context\": row[1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da86729",
   "metadata": {},
   "source": [
    "##### Define functions that are needed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runnable chain\n",
    "def get_runnable_chain(current_query_prompt, llm):\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "    return runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the LLM answers, using a runnable chain and the sample of questions provided\n",
    "def generate_llm_answers(runnable_chain, qas_sample, hyde=False):\n",
    "    \n",
    "    preds = {}\n",
    "\n",
    "    sample_size = len(qas_sample)\n",
    "    print(f\"sample_size: {sample_size}\")\n",
    "\n",
    "    for i, qa in enumerate(qas_sample):\n",
    "\n",
    "        question = qa[\"question\"]\n",
    "        if hyde:\n",
    "            context = qa[\"hyde_context\"]\n",
    "        else:\n",
    "            context = qa[\"vector_store_context\"]\n",
    "            \n",
    "        print(f\"question --> {question}\")\n",
    "        print(context)\n",
    "        response = runnable_chain.invoke({\"context\": context, \"question\": question})\n",
    "                \n",
    "        qid = squad_scoring.get_qid_from_question(question, dataset)\n",
    "        \n",
    "        if response.content.upper() == \"DONT KNOW\":\n",
    "            llm_answer = \"\"\n",
    "        else:\n",
    "            llm_answer = response.content\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        qas_sample[i][\"llm_answer\"] = llm_answer\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "23e1f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# Get the metrics for a set of predictions (preds) that have been generated in a run\n",
    "def get_squad_metrics(dataset, preds, verbose=False):\n",
    "    squad_metrics = squad_scoring.calc_squad_metrics(dataset, preds);\n",
    "    return squad_metrics[\"precision\"], squad_metrics[\"recall\"], squad_metrics[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "d9d2db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to persist a qas sample, including a copy with the LLM answers\n",
    "def persist_qas_sample_results(qas_sample, run_id, results_folder_name):\n",
    "    csv_file_path = f\"{results_folder_name}/qas_sample_with_llm_answers_{run_id}.csv\"\n",
    "    csv_file_path_squad_format = f\"{results_folder_name}/qas_sample_with_llm_answers_{run_id}.csv\"\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"context\", \"question\", \"answer\", \"llm_answer\"])\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        # Write rows\n",
    "        writer.writerows(qas_sample)\n",
    "\n",
    "    qas_sample_squad_format = [{\"question\": qa[\"question\"], \"answer\": qa[\"llm_answer\"]} for qa in qas_sample]\n",
    "    # Write to CSV in SQuAD format\n",
    "    with open(csv_file_path_squad_format, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"question\", \"answer\"])\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        # Write rows\n",
    "        writer.writerows(qas_sample_squad_format)\n",
    "\n",
    "    print(f\"Data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct one single experiment (e.g. NO_RAG, BASIC_RAG, etc.)\n",
    "def conduct_experiment(experiment, persist_answer_samples=False, results_folder_name=None):\n",
    " \n",
    "    current_query_prompt = query_prompts[experiment[\"query_prompt_idx\"]]\n",
    "    print(f\"current_query_prompt = {current_query_prompt.template}\\n\")\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = get_runnable_chain(current_query_prompt, llm)\n",
    "\n",
    "    # Lists that will contain experiment metrics\n",
    "    precision_l = []\n",
    "    recall_l = []\n",
    "    f1_l = []\n",
    "\n",
    "    for i in range(BOOTSTRAP_CNT):\n",
    "        print(f\"{experiment[\"name\"]} - Bootstrap: {i + 1} of {BOOTSTRAP_CNT}...\")\n",
    "        \n",
    "        qas_sample = random.sample(qas, SAMPLE_SIZE)\n",
    "        preds = generate_llm_answers(runnable_chain, qas_sample, hyde = (experiment[\"name\"] == \"BASIC_RAG_HYDE\"));\n",
    "\n",
    "        # For experiment BASIC_RAG_SUPPRESS_ANSWSERS, we need to remove the LLM answers\n",
    "        # from preds that are empty strings.\n",
    "        if experiment[\"name\"] == \"BASIC_RAG_SUPPRESS_ANSWERS\":\n",
    "            print(\"Removing empty LLM answers...\")\n",
    "            preds = {k: v for k, v in preds.items() if v != \"\"}\n",
    "\n",
    "        precision, recall, f1 = get_squad_metrics(dataset, preds)\n",
    "\n",
    "        precision_l.append(precision)\n",
    "        recall_l.append(recall)\n",
    "        f1_l.append(f1)\n",
    "\n",
    "        if persist_answer_samples:\n",
    "            persist_qas_sample_results(qas_sample, i, results_folder_name)\n",
    "        \n",
    "    return precision_l, recall_l, f1_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "6adaa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and confidence interval for a list of scores\n",
    "# TODO: Check if this is calculation is correct !!\n",
    "def calculate_mean_confidence_interval(scores_l):\n",
    "\n",
    "    # Calculate mean\n",
    "    mean = np.mean(scores_l)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    \n",
    "    # confidence = 0.95\n",
    "    # n = len(scores_l)\n",
    "    # std_err = stats.sem(scores_l)  # Standard error of the mean\n",
    "    # h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)  # Margin of error\n",
    "    # ci = (mean - h, mean + h)\n",
    "\n",
    "    sample_std_dev = np.std(scores_l, ddof=1)\n",
    "    n = len(scores_l)\n",
    "    standard_error = sample_std_dev / np.sqrt(n)\n",
    "    margin_of_error = 1.96 * standard_error\n",
    "    ci = (mean - margin_of_error, mean + margin_of_error)\n",
    "\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "d20bb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram for a list of scores and persist it\n",
    "def generate_histogram(scores_l, mean, ci, results_folder_name, experiment_name):\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(scores_l, bins=30, density=True, edgecolor='black', alpha=0.6, color = 'lightblue' ) # color='aquamarine')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.title(f\"F1-Scores for {experiment_name} - (Bootstraps: {BOOTSTRAP_CNT} - Sample Size: {SAMPLE_SIZE})\", fontsize=10)\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    plt.axvline(mean, color='red', linestyle='dotted', linewidth=2, label=f'Mean F1: {round(mean, 2)}')\n",
    "\n",
    "    # Add vertical lines for the 95% confidence interval\n",
    "    plt.axvline(ci[0], color='orange', linestyle='dashdot', linewidth=1.5, label='95% CI Lower')\n",
    "    plt.axvline(ci[1], color='orange', linestyle='dashdot', linewidth=1.5, label='95% CI Upper')\n",
    "\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "    plt.savefig(os.path.join(results_folder_name, f\"{experiment_name}_{BOOTSTRAP_CNT}_{SAMPLE_SIZE}\"))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "5f96d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the metrics for an experiment in a CSV file\n",
    "def persist_metrics(results_folder_name, precision_l, recall_l, f1_l, experiment_name):\n",
    "    # Combine lists into rows\n",
    "    rows = zip(precision_l, recall_l, f1_l)\n",
    "\n",
    "    # Write to a CSV file\n",
    "    csv_file_path = f\"./{results_folder_name}/{experiment_name}_metrics_{BOOTSTRAP_CNT}_{SAMPLE_SIZE}.csv\"\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file) \n",
    "        writer.writerow([\"precision\", \"recall\", \"f1\"]) # Write header\n",
    "        writer.writerows(rows) # Write data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68a80d",
   "metadata": {},
   "source": [
    "##### Conduct all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "c9f59415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp\n",
    "start_timestamp = datetime.now()\n",
    "\n",
    "# Format the start timestamp as a string\n",
    "start_timestamp_str = start_timestamp.strftime('%Y%m%d_%H%M%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "0394249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Conducting experiment: BASIC_RAG_SUPPRESS_ANSWERS\n",
      "results_folder_name: ../../../deh_data_results/results/results_20241125_144947/results_BASIC_RAG_SUPPRESS_ANSWERS\n",
      "\n",
      "current_query_prompt = \n",
      "                You are an assistant for question-answering tasks.\n",
      "                Use the following pieces of retrieved context to answer the question.\n",
      "                If you don't know the answer, just return 'DONT KNOW'. \n",
      "                If you know the answer keep it as short and concise as possible,\n",
      "                i.e. to a maximum of a couple of words.\n",
      "\n",
      "                Question: {question}\n",
      "                Context: {context}\n",
      "\n",
      "                Answer:\n",
      "                \n",
      "\n",
      "BASIC_RAG_SUPPRESS_ANSWERS - Bootstrap: 1 of 30...\n",
      "sample_size: 30\n",
      "\n",
      "Removing empty LLM answers...\n",
      "BASIC_RAG_SUPPRESS_ANSWERS - Bootstrap: 2 of 30...\n",
      "sample_size: 30\n",
      "\n",
      "Removing empty LLM answers...\n",
      "BASIC_RAG_SUPPRESS_ANSWERS - Bootstrap: 3 of 30...\n",
      "sample_size: 30\n",
      "\n",
      "Removing empty LLM answers...\n",
      "BASIC_RAG_SUPPRESS_ANSWERS - Bootstrap: 4 of 30...\n",
      "sample_size: 30\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[390], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(results_folder_name)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_folder_name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_folder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m precision_l, recall_l, f1_l \u001b[38;5;241m=\u001b[39m \u001b[43mconduct_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mpersist_answer_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPERSIST_ANSWER_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mresults_folder_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresults_folder_name\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Remove 2.5% from each tail of f1_l (precision_l and recall_l are not taken into account)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m remove_cnt \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(BOOTSTRAP_CNT \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.025\u001b[39m)  \u001b[38;5;66;03m# take ceiling for very low values of BOOTSTRAP_CNT\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[385], line 18\u001b[0m, in \u001b[0;36mconduct_experiment\u001b[0;34m(experiment, persist_answer_samples, results_folder_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Bootstrap: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBOOTSTRAP_CNT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m qas_sample \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(qas, SAMPLE_SIZE)\n\u001b[0;32m---> 18\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_llm_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunnable_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqas_sample\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# For experiment BASIC_RAG_SUPPRESS_ANSWSERS, we need to remove the LLM answers\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# from preds that are empty strings.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experiment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBASIC_RAG_SUPPRESS_ANSWERS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[382], line 11\u001b[0m, in \u001b[0;36mgenerate_llm_answers\u001b[0;34m(runnable_chain, qas_sample)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, qa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(qas_sample):\n\u001b[0;32m---> 11\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrunnable_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     question \u001b[38;5;241m=\u001b[39m qa[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m     qid \u001b[38;5;241m=\u001b[39m squad_scoring\u001b[38;5;241m.\u001b[39mget_qid_from_question(question, dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_ollama/chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_ollama/chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/langchain_ollama/chat_models.py:527\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    518\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    519\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         tools\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    528\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    529\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[1;32m    530\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m         options\u001b[38;5;241m=\u001b[39mOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    532\u001b[0m         keep_alive\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/ollama/_client.py:87\u001b[0m, in \u001b[0;36mClient._stream\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m:=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_models.py:863\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_text():\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode(text):\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_models.py:850\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    848\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_bytes():\n\u001b[1;32m    851\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(byte_content)\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(text_content):\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_models.py:831\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    832\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_models.py:885\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    882\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_client.py:127\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpx/_transports/default.py:116\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    404\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/http11.py:203\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/envs/dh_p12/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "\n",
    "for experiment in experiments:\n",
    "    if not experiment[\"conduct\"]:  # Skip experiments that should not be done for the moment\n",
    "        continue\n",
    "    \n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Conducting experiment: {experiment['name']}\")\n",
    "\n",
    "    results_folder_name = f\"{RESULTS_ROOT}/results_{start_timestamp_str}/results_{experiment['name']}\\n\"\n",
    "    os.makedirs(results_folder_name)\n",
    "    print(f\"results_folder_name: {results_folder_name}\")\n",
    "\n",
    "    precision_l, recall_l, f1_l = conduct_experiment(experiment,\n",
    "                                                     persist_answer_samples = PERSIST_ANSWER_SAMPLES,\n",
    "                                                     results_folder_name = results_folder_name)  \n",
    "\n",
    "    # Remove 2.5% from each tail of f1_l (precision_l and recall_l are not taken into account)\n",
    "    remove_cnt = math.ceil(BOOTSTRAP_CNT * 0.025)  # take ceiling for very low values of BOOTSTRAP_CNT\n",
    "    # print(f\"len of f1_l before: {len(f1_l)}\")\n",
    "    f1_l = sorted(f1_l)[remove_cnt:-(remove_cnt+1)]\n",
    "    # print(f\"len of f1_l after: {len(f1_l)}\")\n",
    "    mean, ci = calculate_mean_confidence_interval(f1_l)\n",
    "\n",
    "    plt = generate_histogram(f1_l, mean, ci, results_folder_name, experiment[\"name\"])\n",
    "    persist_metrics(results_folder_name, precision_l, recall_l, f1_l, experiment[\"name\"])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for conducting all experiments: 0:53:03.433588\n"
     ]
    }
   ],
   "source": [
    "# Calculate the time elapsed for conducting all experiments\n",
    "end_timestamp = datetime.now()\n",
    "\n",
    "print(f\"Elapsed time for conducting all experiments: {end_timestamp - start_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf7414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
