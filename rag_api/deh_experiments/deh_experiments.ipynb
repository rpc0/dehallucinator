{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26875bf",
   "metadata": {},
   "source": [
    "##### Imports Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce660d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54c78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "\n",
    "from csv import DictReader\n",
    "#from tqdm.autonotebook import tqdm\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e1a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Prompts\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b764f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for squad_scoring and prompts\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import squad_scoring\n",
    "import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3670c6",
   "metadata": {},
   "source": [
    "##### Set Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e06b8d-0584-4170-940c-3077344ce89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders for storing data and the results\n",
    "DATA_ROOT = \"../../../deh_data_results/data\"         # Set to your own data folder\n",
    "RESULTS_ROOT = \"../../../deh_data_results/results\"   # Set to your own results folder\n",
    "\n",
    "# Vector Store Parameters\n",
    "ollama_embedding_model = \"avr/sfr-embedding-mistral\"\n",
    "embeddings = OllamaEmbeddings(model=ollama_embedding_model)\n",
    "persist_directory = f\"{DATA_ROOT}/chroma_deh_rag_db\"\n",
    "collection_name = \"deh_rag\"\n",
    "VECTORIZE_SQUAD_DATASET = False     # Set to True to vectorize the squad dataset. If False, \n",
    "                                    # then the documents and their embeddings should already\n",
    "                                    # exist in the vector store.\n",
    "CREATE_QUESTION_CONTEXTS = False    # Set to True to create question contexts; if False, \n",
    "                                    # the question contexts are loaded from a csv file.\n",
    "CREATE_HYDE_CONTEXTS = False        # Set to True to create hyde contexts; if False,\n",
    "                                    # the hyde contexts are loaded from a csv file.                                    \n",
    "\n",
    "# LLM Parameters\n",
    "CHAT_MODEL_NAME = \"llama3.1\"\n",
    "MAX_TOKENS = 100\n",
    "TEMPERATURE = 0.5\n",
    "TOP_P = 0.95\n",
    "FREQUENCY_PENALTY = 0.0\n",
    "PRESENCE_PENALTY = 0.0\n",
    "\n",
    "CURRENT_QUERY_PROMPT_IDX = 0\n",
    "\n",
    "# Bootstrap Parameters\n",
    "SAMPLE_SIZE = 30\n",
    "BOOTSTRAP_CNT = 200\n",
    "\n",
    "# Experiment Parameters - define all the experiments to run\n",
    "experiments = [{\"name\": \"NO_RAG\", \"rag\": False, \"rag_model\": None, \"query_prompt_idx\": 0, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG\", \"rag\": True, \"rag_model\": \"basic\", \"query_prompt_idx\": 1, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_DONT_LIE\", \"rag\": True, \"rag_model\": \"basic_dont_lie\", \"query_prompt_idx\": 2, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_HYDE\", \"rag\": True, \"rag_model\": \"basic_hyde\", \"query_prompt_idx\": 2, \"conduct\": True},\n",
    "               {\"name\": \"BASIC_RAG_MILVUS\", \"rag\": True, \"rag_model\": \"basic_milvus\", \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_SEMANTIC_CHUNKING\", \"rag\": True, \"rag_model\": \"basic_semantic_chunking\", \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_SUPPRESS_ANSWERS\", \"rag\": True, \"rag_model\": \"basic_suppress_answers\", \"query_prompt_idx\": 2, \"conduct\": False},\n",
    "               {\"name\": \"FULL_RAG\", \"rag\": True, \"rag_model\": \"full\", \"conduct\": False}]\n",
    "\n",
    "PERSIST_ANSWER_SAMPLES = False   # Set to True to persist the llm answers for each sample, for each experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5812d",
   "metadata": {},
   "source": [
    "##### Define the prompts and a function to get the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf394442",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompts = [\n",
    "    PromptTemplate(\n",
    "        template=prompts.rag_text_prompts[2],\n",
    "        input_variables = [\"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template=prompts.rag_text_prompts[1],\n",
    "        input_variables = [\"context\", \"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template=(\"\"\"\n",
    "                You are an assistant for question-answering tasks.\n",
    "                Use the following pieces of retrieved context to answer the question.\n",
    "                If you don't know the answer, just return 'DONT KNOW'. \n",
    "                If you know the answer keep it as short and concise as possible,\n",
    "                i.e. to a maximum of a couple of words.\n",
    "\n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "        ),\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template = prompts.hyde_prompts[1],\n",
    "        input_variables = [\"question\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0888af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the llm instance, based on the current query prompt\n",
    "def get_llm(current_query_prompt):\n",
    "    llm = ChatOllama(\n",
    "        prompt_template = current_query_prompt,\n",
    "        model = CHAT_MODEL_NAME,\n",
    "        max_tokens = MAX_TOKENS,\n",
    "        temperature = TEMPERATURE,\n",
    "        top_p = TOP_P,\n",
    "        frequency_penalty = FREQUENCY_PENALTY,\n",
    "        presence_penalty = PRESENCE_PENALTY,\n",
    "        gpu_use = True\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e87057",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus to be added later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346f2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize the Chroma vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name = collection_name,\n",
    "    embedding_function = embeddings,\n",
    "    persist_directory = persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0328570",
   "metadata": {},
   "source": [
    "##### Load the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31d8356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#articles in the dataset:     35\n",
      "#contexts in the dataset:   1204\n",
      "#questions in the dataset: 20302\n"
     ]
    }
   ],
   "source": [
    "data_file = f\"{DATA_ROOT}/dev-v2.0.json\"\n",
    "dataset = squad_scoring.load_dataset(data_file)\n",
    "\n",
    "articles = []\n",
    "contexts = []\n",
    "qas = []\n",
    "\n",
    "for article in dataset:\n",
    "    title = article[\"title\"]\n",
    "    articles.append(title)\n",
    "    for p in article['paragraphs']:\n",
    "        context = p[\"context\"]\n",
    "        contexts.append(context)\n",
    "        for qa in p['qas']:\n",
    "            question = qa[\"question\"]\n",
    "            for a in qa[\"answers\"]:\n",
    "                answer = a[\"text\"]\n",
    "                qas.append({\"title\": title, \"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "print(f\"#articles in the dataset:     {len(articles)}\")\n",
    "print(f\"#contexts in the dataset:   {len(contexts)}\")\n",
    "print(f\"#questions in the dataset: {len(qas)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8842693",
   "metadata": {},
   "source": [
    "##### If configured, chunk the SQuAD dataset and add to the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97133f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not vectorizing the SQuAD dataset...\n"
     ]
    }
   ],
   "source": [
    "if VECTORIZE_SQUAD_DATASET:\n",
    "    print(f\"Creating contexts for the dataset...\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    all_contexts = \"\\n\\n\".join(contexts)\n",
    "    chunks = text_splitter.create_documents([all_contexts])   \n",
    "\n",
    "    print(f\"Number of chunks --> {len(chunks)}\\n\")\n",
    "    print(chunks[0])      \n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata = {\"source\": \"squad\"}\n",
    "    \n",
    "    ids = [str(i) for i in list(range(len(chunks)))]\n",
    "    vector_store.add_documents(documents=chunks, ids=ids);\n",
    "else:\n",
    "    print(\"Not vectorizing the SQuAD dataset...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09ac9d",
   "metadata": {},
   "source": [
    "##### Contexts\n",
    "\n",
    "Questions:\n",
    "\n",
    "- if configured: generate question contexts and then persist\n",
    "- else: read the question and hyde contexts from a .csv file\n",
    "\n",
    "Hyde:\n",
    "\n",
    "- if configured: generate Hyde contexts and then persist\n",
    "- else: read the Hyde contexts from a .csv fil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d86bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_question_contexts(csv_file_path):\n",
    "\n",
    "    # Write the the qas dataset including the question contexts to a CSV file\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"context\", \"question\", \"answer\", \"question_context\"])\n",
    "        writer.writeheader()   # Write the header row\n",
    "        writer.writerows(qas)  # Write the data rows\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b8a03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_hyde_contexts(csv_file_path, hyde_contexts):\n",
    "\n",
    "    # Write the the Hyde contexts to a CSV file\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"question\", \"hyde_context\"])\n",
    "        writer.writeheader()   # Write the header row\n",
    "        writer.writerows(hyde_contexts)  # Write the data rows\n",
    "\n",
    "    print(f\"Data successfully written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577ed41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading qas and question contexts from a csv file...\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/qas_with_question_contexts.csv\"\n",
    "\n",
    "if CREATE_QUESTION_CONTEXTS:\n",
    "    print(f\"Creating question contexts for the dataset and persisting these in a csv file...\")\n",
    "    for i, qa in enumerate(qas):\n",
    "        if i %100 == 0:\n",
    "            print(f\"Processing question {i}...\")\n",
    "        question = qa[\"question\"]\n",
    "        \n",
    "        top_docs = vector_store.similarity_search(\n",
    "            query = question,\n",
    "            k = 5\n",
    "        )\n",
    "        qa[\"question_context\"] = \" \".join([top_doc.page_content for top_doc in top_docs])\n",
    "    \n",
    "    persist_question_contexts(csv_file_path)\n",
    "\n",
    "else:\n",
    "    print(f\"Reading qas and question contexts from a csv file...\")\n",
    "    \n",
    "    # Read the qas (including contexts) from a CSV file\n",
    "    qas = []\n",
    "\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            qas.append({\"title\": row[0], \"context\": row[1], \"question\": row[2], \"answer\": row[3],\n",
    "                        \"question_context\": row[4]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "359eb524-6f9a-4739-97e2-70fa10b8ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyde_contexts_already_done(csv_file_path):\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        return []\n",
    "\n",
    "    hyde_contexts_already_done = []\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            hyde_contexts_already_done.append({\"question\": row[0], \"hyde_context\": row[1]})\n",
    "    return hyde_contexts_already_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a52c44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the Hyde contexts from a csv file...\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/hyde_contexts.csv\"\n",
    "\n",
    "hyde_contexts_already_done = get_hyde_contexts_already_done(csv_file_path)\n",
    "hyde_questions_already_done = [hc[\"question\"] for hc in hyde_contexts_already_done]\n",
    "# print(hyde_questions_already_done)\n",
    "\n",
    "if CREATE_HYDE_CONTEXTS:\n",
    "    print(f\"Creating Hyde contexts for the dataset and persisting these in a csv file...\")\n",
    "    unique_questions = list(set([qa[\"question\"] for qa in qas]))\n",
    "    print(f\"Number of unique questions: {len(unique_questions)}\")\n",
    "\n",
    "    current_query_prompt = query_prompts[3]\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "\n",
    "    for i, question in enumerate(unique_questions[:1200]):\n",
    "        #if i %10 == 0:\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(f\"Processing question {i}...\")\n",
    "        print(f\"Question: {question}\")\n",
    "        if question in hyde_questions_already_done:\n",
    "            print(\"Question already done...\")\n",
    "            continue\n",
    "        response = runnable_chain.invoke({\"question\": question})\n",
    "        hyde_contexts_already_done.append({\"question\": question, \"hyde_context\": response.content})\n",
    "        hyde_questions_already_done.append(question)\n",
    "        print(\"\")\n",
    "    \n",
    "    persist_hyde_contexts(csv_file_path, hyde_contexts_already_done)\n",
    "    hyde_contexts = hyde_contexts_already_done\n",
    "\n",
    "else:\n",
    "    print(f\"Reading the Hyde contexts from a csv file...\")\n",
    "\n",
    "    hyde_contexts = []\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            hyde_contexts.append({\"question\": row[0], \"hyde_context\": row[1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da86729",
   "metadata": {},
   "source": [
    "##### Define functions that are needed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53bd4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the runnable chain\n",
    "def get_runnable_chain(current_query_prompt, llm):\n",
    "    runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "    return runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7152c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hyde context for a question\n",
    "# Could be made much more efficient by creating a dictionary of hyde contexts\n",
    "def get_hyde_context(question):\n",
    "    for hc in hyde_contexts:\n",
    "        if hc[\"question\"] == question:\n",
    "            return hc[\"hyde_context\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "245f5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the LLM answers, using a runnable chain and the sample of questions provided\n",
    "def generate_llm_answers(runnable_chain, qas_sample, hyde=False):\n",
    "    \n",
    "    preds = {}\n",
    "\n",
    "    sample_size = len(qas_sample)\n",
    "    print(f\"sample_size: {sample_size}\")\n",
    "\n",
    "    for i, qa in enumerate(qas_sample):\n",
    "\n",
    "        question = qa[\"question\"]\n",
    "        if hyde:\n",
    "            #context = qa[\"hyde_context\"]\n",
    "            context = get_hyde_context(question)\n",
    "        else:\n",
    "            context = qa[\"vector_store_context\"]\n",
    "            \n",
    "        # print(f\"question --> {question}\")\n",
    "        # print(context)\n",
    "        response = runnable_chain.invoke({\"context\": context, \"question\": question})\n",
    "                \n",
    "        qid = squad_scoring.get_qid_from_question(question, dataset)\n",
    "        \n",
    "        if response.content.upper() == \"DONT KNOW\":\n",
    "            llm_answer = \"\"\n",
    "        else:\n",
    "            llm_answer = response.content\n",
    "\n",
    "        preds[qid] = llm_answer\n",
    "        qas_sample[i][\"llm_answer\"] = llm_answer\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23e1f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "# Get the metrics for a set of predictions (preds) that have been generated in a run\n",
    "def get_squad_metrics(dataset, preds, verbose=False):\n",
    "    squad_metrics = squad_scoring.calc_squad_metrics(dataset, preds);\n",
    "    return squad_metrics[\"precision\"], squad_metrics[\"recall\"], squad_metrics[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9d2db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to persist a qas sample, including a copy with the LLM answers\n",
    "def persist_qas_sample_results(qas_sample, run_id, results_folder_name):\n",
    "    csv_file_path = f\"{results_folder_name}/qas_sample_with_llm_answers_{run_id}.csv\"\n",
    "    csv_file_path_squad_format = f\"{results_folder_name}/qas_sample_with_llm_answers_{run_id}.csv\"\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"context\", \"question\", \"answer\", \"llm_answer\"])\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        # Write rows\n",
    "        writer.writerows(qas_sample)\n",
    "\n",
    "    qas_sample_squad_format = [{\"question\": qa[\"question\"], \"answer\": qa[\"llm_answer\"]} for qa in qas_sample]\n",
    "    # Write to CSV in SQuAD format\n",
    "    with open(csv_file_path_squad_format, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"question\", \"answer\"])\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        # Write rows\n",
    "        writer.writerows(qas_sample_squad_format)\n",
    "\n",
    "    print(f\"Data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "306d2107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4183"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([qa for qa in qas if qa[\"question\"] in hyde_questions_already_done])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "461b422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct one single experiment (e.g. NO_RAG, BASIC_RAG, etc.)\n",
    "def conduct_experiment(experiment, persist_answer_samples=False, results_folder_name=None):\n",
    " \n",
    "    current_query_prompt = query_prompts[experiment[\"query_prompt_idx\"]]\n",
    "    print(f\"current_query_prompt = {current_query_prompt.template}\\n\")\n",
    "    llm = get_llm(current_query_prompt)\n",
    "    runnable_chain = get_runnable_chain(current_query_prompt, llm)\n",
    "\n",
    "    # Lists that will contain experiment metrics\n",
    "    precision_l = []\n",
    "    recall_l = []\n",
    "    f1_l = []\n",
    "\n",
    "    # !! For now, we are only using those questions that already have a Hyde context\n",
    "    # TODO: Use all questions in the dataset once we have Hyde contexts for all questions\n",
    "    # For now, filter on those questions that have a Hyde context\n",
    "    temp_qas = [qa for qa in qas if qa[\"question\"] in hyde_questions_already_done]\n",
    "    # qas = temp_qas\n",
    "    print(f\"Number of questions with Hyde context in the dataset: {len(temp_qas)}\")\n",
    "    \n",
    "    for i in range(BOOTSTRAP_CNT):\n",
    "        print(f\"{experiment[\"name\"]} - Bootstrap: {i + 1} of {BOOTSTRAP_CNT}...\")\n",
    "        \n",
    "        qas_sample = random.sample(temp_qas, SAMPLE_SIZE)\n",
    "        preds = generate_llm_answers(runnable_chain, qas_sample, hyde = (experiment[\"name\"] == \"BASIC_RAG_HYDE\"));\n",
    "\n",
    "        # For experiment BASIC_RAG_SUPPRESS_ANSWSERS, we need to remove the LLM answers\n",
    "        # from preds that are empty strings.\n",
    "        if experiment[\"name\"] == \"BASIC_RAG_SUPPRESS_ANSWERS\":\n",
    "            print(\"Removing empty LLM answers...\")\n",
    "            preds = {k: v for k, v in preds.items() if v != \"\"}\n",
    "\n",
    "        precision, recall, f1 = get_squad_metrics(dataset, preds)\n",
    "\n",
    "        precision_l.append(precision)\n",
    "        recall_l.append(recall)\n",
    "        f1_l.append(f1)\n",
    "\n",
    "        if persist_answer_samples:\n",
    "            persist_qas_sample_results(qas_sample, i, results_folder_name)\n",
    "        \n",
    "    return precision_l, recall_l, f1_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6adaa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and confidence interval for a list of scores\n",
    "# TODO: Check if this is calculation is correct !!\n",
    "def calculate_mean_confidence_interval(scores_l):\n",
    "\n",
    "    # Calculate mean\n",
    "    mean = np.mean(scores_l)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    \n",
    "    # confidence = 0.95\n",
    "    # n = len(scores_l)\n",
    "    # std_err = stats.sem(scores_l)  # Standard error of the mean\n",
    "    # h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)  # Margin of error\n",
    "    # ci = (mean - h, mean + h)\n",
    "\n",
    "    sample_std_dev = np.std(scores_l, ddof=1)\n",
    "    n = len(scores_l)\n",
    "    standard_error = sample_std_dev / np.sqrt(n)\n",
    "    margin_of_error = 1.96 * standard_error\n",
    "    ci = (mean - margin_of_error, mean + margin_of_error)\n",
    "\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d20bb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram for a list of scores and persist it\n",
    "def generate_histogram(scores_l, mean, ci, results_folder_name, experiment_name):\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(scores_l, bins=30, density=True, edgecolor='black', alpha=0.6, color = 'lightblue' ) # color='aquamarine')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.title(f\"F1-Scores for {experiment_name} - (Bootstraps: {BOOTSTRAP_CNT} - Sample Size: {SAMPLE_SIZE})\", fontsize=10)\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    # Add a vertical line for the mean\n",
    "    plt.axvline(mean, color='red', linestyle='dotted', linewidth=2, label=f'Mean F1: {round(mean, 2)}')\n",
    "\n",
    "    # Add vertical lines for the 95% confidence interval\n",
    "    plt.axvline(ci[0], color='orange', linestyle='dashdot', linewidth=1.5, label='95% CI Lower')\n",
    "    plt.axvline(ci[1], color='orange', linestyle='dashdot', linewidth=1.5, label='95% CI Upper')\n",
    "\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "    plt.savefig(os.path.join(results_folder_name, f\"{experiment_name}_{BOOTSTRAP_CNT}_{SAMPLE_SIZE}\"))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f96d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the metrics for an experiment in a CSV file\n",
    "def persist_metrics(results_folder_name, precision_l, recall_l, f1_l, experiment_name):\n",
    "    # Combine lists into rows\n",
    "    rows = zip(precision_l, recall_l, f1_l)\n",
    "\n",
    "    # Write to a CSV file\n",
    "    csv_file_path = f\"./{results_folder_name}/{experiment_name}_metrics_{BOOTSTRAP_CNT}_{SAMPLE_SIZE}.csv\"\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file) \n",
    "        writer.writerow([\"precision\", \"recall\", \"f1\"]) # Write header\n",
    "        writer.writerows(rows) # Write data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68a80d",
   "metadata": {},
   "source": [
    "##### Conduct all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9f59415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp\n",
    "start_timestamp = datetime.now()\n",
    "\n",
    "# Format the start timestamp as a string\n",
    "start_timestamp_str = start_timestamp.strftime('%Y%m%d_%H%M%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0394249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Conducting experiment: BASIC_RAG_HYDE\n",
      "results_folder_name: ../../../deh_data_results/results/results_20241125_231741/results_BASIC_RAG_HYDE\n",
      "\n",
      "current_query_prompt = \n",
      "                You are an assistant for question-answering tasks.\n",
      "                Use the following pieces of retrieved context to answer the question.\n",
      "                If you don't know the answer, just return 'DONT KNOW'. \n",
      "                If you know the answer keep it as short and concise as possible,\n",
      "                i.e. to a maximum of a couple of words.\n",
      "\n",
      "                Question: {question}\n",
      "                Context: {context}\n",
      "\n",
      "                Answer:\n",
      "                \n",
      "\n",
      "Number of questions with Hyde context in the dataset: 4183\n",
      "BASIC_RAG_HYDE - Bootstrap: 1 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 2 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 3 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 4 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 5 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 6 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 7 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 8 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 9 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 10 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 11 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 12 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 13 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 14 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 15 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 16 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 17 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 18 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 19 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 20 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 21 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 22 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 23 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 24 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 25 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 26 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 27 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 28 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 29 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 30 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 31 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 32 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 33 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 34 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 35 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 36 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 37 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 38 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 39 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 40 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 41 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 42 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 43 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 44 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 45 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 46 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 47 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 48 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 49 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 50 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 51 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 52 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 53 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 54 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 55 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 56 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 57 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 58 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 59 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 60 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 61 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 62 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 63 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 64 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 65 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 66 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 67 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 68 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 69 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 70 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 71 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 72 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 73 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 74 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 75 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 76 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 77 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 78 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 79 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 80 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 81 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 82 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 83 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 84 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 85 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 86 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 87 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 88 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 89 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 90 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 91 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 92 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 93 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 94 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 95 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 96 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 97 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 98 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 99 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 100 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 101 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 102 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 103 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 104 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 105 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 106 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 107 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 108 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 109 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 110 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 111 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 112 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 113 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 114 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 115 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 116 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 117 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 118 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 119 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 120 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 121 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 122 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 123 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 124 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 125 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 126 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 127 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 128 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 129 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 130 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 131 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 132 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 133 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 134 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 135 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 136 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 137 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 138 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 139 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 140 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 141 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 142 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 143 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 144 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 145 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 146 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 147 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 148 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 149 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 150 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 151 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 152 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 153 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 154 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 155 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 156 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 157 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 158 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 159 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 160 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 161 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 162 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 163 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 164 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 165 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 166 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 167 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 168 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 169 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 170 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 171 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 172 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 173 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 174 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 175 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 176 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 177 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 178 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 179 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 180 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 181 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 182 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 183 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 184 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 185 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 186 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 187 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 188 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 189 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 190 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 191 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 192 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 193 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 194 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 195 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 196 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 197 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 198 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 199 of 200...\n",
      "sample_size: 30\n",
      "BASIC_RAG_HYDE - Bootstrap: 200 of 200...\n",
      "sample_size: 30\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "\n",
    "for experiment in experiments:\n",
    "    if not experiment[\"conduct\"]:  # Skip experiments that should not be done for the moment\n",
    "        continue\n",
    "    \n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Conducting experiment: {experiment['name']}\")\n",
    "\n",
    "    results_folder_name = f\"{RESULTS_ROOT}/results_{start_timestamp_str}/results_{experiment['name']}\\n\"\n",
    "    os.makedirs(results_folder_name)\n",
    "    print(f\"results_folder_name: {results_folder_name}\")\n",
    "\n",
    "    precision_l, recall_l, f1_l = conduct_experiment(experiment,\n",
    "                                                     persist_answer_samples = PERSIST_ANSWER_SAMPLES,\n",
    "                                                     results_folder_name = results_folder_name)  \n",
    "\n",
    "    # Remove 2.5% from each tail of f1_l (precision_l and recall_l are not taken into account)\n",
    "    remove_cnt = math.ceil(BOOTSTRAP_CNT * 0.025)  # take ceiling for very low values of BOOTSTRAP_CNT\n",
    "    # print(f\"len of f1_l before: {len(f1_l)}\")\n",
    "    f1_l = sorted(f1_l)[remove_cnt:-(remove_cnt+1)]\n",
    "    # print(f\"len of f1_l after: {len(f1_l)}\")\n",
    "    mean, ci = calculate_mean_confidence_interval(f1_l)\n",
    "\n",
    "    plt = generate_histogram(f1_l, mean, ci, results_folder_name, experiment[\"name\"])\n",
    "    persist_metrics(results_folder_name, precision_l, recall_l, f1_l, experiment[\"name\"])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3373de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for conducting all experiments: 0:42:36.092618\n"
     ]
    }
   ],
   "source": [
    "# Calculate the time elapsed for conducting all experiments\n",
    "end_timestamp = datetime.now()\n",
    "\n",
    "print(f\"Elapsed time for conducting all experiments: {end_timestamp - start_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf7414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
