{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Change chunking strategy\n",
    "\n",
    "- change chunking --> chunk per article\n",
    "- recreate SQAUD database\n",
    "\n",
    "2.) Generate Hyde-based contexts\n",
    "\n",
    "- all files in one directory: hyde_based_contexts\n",
    "- each file called hyde_based_contexts.csv\n",
    "- each file contains:\n",
    "\n",
    "   - qid\n",
    "   - question\n",
    "   - hyde_article\n",
    "   - hyde_based_context\n",
    "\n",
    "Currently, files contain the following information:\n",
    "\n",
    "    - filename : hyde_contexts_<n>\n",
    "    - question, hyde_context\n",
    " \n",
    " ==> change strategy for hyde_generated_contexts\n",
    "\n",
    " a) read in complete dataset (\"title\", \"context\", \"qid\", \"question\", \"is_impossible\", \"answer\")\n",
    " b) read in all hyde_passages that already exist\n",
    " c) take a sample of size HYDE_SAMPLE_SIZE from the complete dataset\n",
    " d) \n",
    " create new list: hyde_based_contexts (dict elements:qid, question, hyde_article, hyde_based_context)\n",
    "\n",
    " for each member of the sample:\n",
    "\n",
    "   - get question qid and add it\n",
    "   - add question to dataset\n",
    "   - if not hyde_article exists:\n",
    "      generate hyde_article\n",
    "   - get hyde_based_context\n",
    "   - add new dict element with: qid, question, hyde_article, hyde_based_context)\n",
    "\n",
    "persist new dataset to file hyde_based_contexts_<n>\n",
    "\n",
    "ALTERNATIVE:\n",
    "\n",
    "use chain from video course\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "from csv import DictReader\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Prompts\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Runnables\n",
    "#from langchain.schema.runnable import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports for squad_scoring and prompts\n",
    "from pathlib import Path\n",
    "\n",
    "utils_folder = Path(\"..\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\"../src/deh\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "utils_folder = Path(\".\")\n",
    "sys.path.append(str(utils_folder))\n",
    "\n",
    "import squad_scoring\n",
    "import prompts\n",
    "import deh_hyde\n",
    "# from deh_hyde import generate_hyde_based_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders for storing data and the results\n",
    "DATA_ROOT = \"../../../deh_data_results/data\"         # Set to your own data folder\n",
    "RESULTS_ROOT = \"../../../deh_data_results/results\"   # Set to your own results folder\n",
    "HYDE_BASED_CONTEXTS_ROOT = F\"{DATA_ROOT}/hyde_based_contexts\"   # Set to your own hyde-based contexts folder\n",
    "\n",
    "# Vector Store Parameters\n",
    "ollama_embedding_model = \"avr/sfr-embedding-mistral\"\n",
    "embeddings = OllamaEmbeddings(model=ollama_embedding_model)\n",
    "persist_directory = f\"{DATA_ROOT}/chroma_deh_rag_db\"\n",
    "collection_name = \"deh_rag\"\n",
    "VECTORIZE_SQUAD_DATASET = False      # Set to True to vectorize the squad dataset. If False, \n",
    "                                    # then the documents and their embeddings should already\n",
    "                                    # exist in the vector store.\n",
    "CREATE_QUESTION_CONTEXTS = False     # Set to True to create question contexts from the vector store; \n",
    "                                    # if False, the question contexts are loaded from a csv file.\n",
    "CREATE_HYDE_CONTEXTS = True         # Set to True to create hyde contexts; if False,\n",
    "                                    # the hyde contexts are loaded from a csv file.                                    \n",
    "\n",
    "# LLM Parameters\n",
    "CHAT_MODEL_NAME = \"llama3.1\"\n",
    "MAX_TOKENS = 100\n",
    "TEMPERATURE = 0.5\n",
    "TOP_P = 0.95\n",
    "FREQUENCY_PENALTY = 0.0\n",
    "PRESENCE_PENALTY = 0.0\n",
    "\n",
    "CURRENT_QUERY_PROMPT_IDX = 0\n",
    "\n",
    "# Bootstrap Parameters\n",
    "SAMPLE_SIZE = 1000\n",
    "BOOTSTRAPS_N = 10000\n",
    "#TODO check if this code is ok for setting the seed\n",
    "# SEED = 42\n",
    "# set_seed = random.seed(SEED)\n",
    "\n",
    "# Experiment Parameters - define all the experiments to run\n",
    "experiments = [{\"name\": \"NO_RAG\", \"rag\": False, \"rag_model\": None, \"query_prompt_idx\": 0, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG\", \"rag\": True, \"rag_model\": \"basic\", \"query_prompt_idx\": 1, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_DONT_LIE\", \"rag\": True, \"rag_model\": \"basic_dont_lie\", \"query_prompt_idx\": 2, \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_HYDE\", \"rag\": True, \"rag_model\": \"basic_hyde\", \"query_prompt_idx\": 2, \"conduct\": True},\n",
    "               {\"name\": \"BASIC_RAG_MILVUS\", \"rag\": True, \"rag_model\": \"basic_milvus\", \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_SEMANTIC_CHUNKING\", \"rag\": True, \"rag_model\": \"basic_semantic_chunking\", \"conduct\": False},\n",
    "               {\"name\": \"BASIC_RAG_SUPPRESS_ANSWERS\", \"rag\": True, \"rag_model\": \"basic_suppress_answers\", \"query_prompt_idx\": 2, \"conduct\": False},\n",
    "               {\"name\": \"FULL_RAG\", \"rag\": True, \"rag_model\": \"full\", \"conduct\": False}]\n",
    "\n",
    "PERSIST_ANSWER_SAMPLES = False   # Set to True to persist the llm answers for each sample, for each experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#articles in the dataset:     35\n",
      "#contexts in the dataset:   1204\n",
      "#questions in the dataset: 26232\n",
      "#unique questions in the dataset: 11849\n",
      "#unique qas in the dataset: 16209\n",
      "Processing question 0...\n",
      "Question What working fluid is used in a mercury cycle? already processed. Skipping...\n",
      "Processing question 1...\n",
      "Question How many electorates does the State of Victoria have? already processed. Skipping...\n",
      "Processing question 2...\n",
      "Processing question 3...\n",
      "Processing question 4...\n",
      "Question What was the memoir entitled which was submitted to the American Philosophical Society? already processed. Skipping...\n",
      "Processing question 5...\n",
      "Processing question 6...\n",
      "Processing question 7...\n",
      "Processing question 8...\n",
      "Processing question 9...\n",
      "Question In the Rankine cycle, in what state is the working fluid received in the condenser? already processed. Skipping...\n",
      "Processing question 10...\n",
      "Question What is the first major city in the stream of the Rhine? already processed. Skipping...\n",
      "Processing question 11...\n",
      "Processing question 12...\n",
      "Question What style was he Paris Philharmony edifice built in? already processed. Skipping...\n",
      "Processing question 13...\n",
      "Processing question 14...\n",
      "Processing question 15...\n",
      "Question How can you protest against big companies in a non violent way? already processed. Skipping...\n",
      "Processing question 16...\n",
      "Processing question 17...\n",
      "Processing question 18...\n",
      "Processing question 19...\n",
      "Question What is key to getting the skills needed for high demand jobs? already processed. Skipping...\n",
      "Processing question 20...\n",
      "Processing question 21...\n",
      "Question How many miners died in the typhoid outbreak of 1854? already processed. Skipping...\n",
      "Processing question 22...\n",
      "Processing question 23...\n",
      "Processing question 24...\n",
      "Question What does oxygen form bonds with all other types of? already processed. Skipping...\n",
      "Processing question 25...\n",
      "Processing question 26...\n",
      "Processing question 27...\n",
      "Question Health problems were higher in places with higher levels of what? already processed. Skipping...\n",
      "Processing question 28...\n",
      "Processing question 29...\n",
      "Question 91% of what is used for farming? already processed. Skipping...\n",
      "Processing question 30...\n",
      "Question What life process produces oxygen in the presence of light? already processed. Skipping...\n",
      "Processing question 31...\n",
      "Processing question 32...\n",
      "Question When did Ribault first establish a settlement in South Carolina? already processed. Skipping...\n",
      "Processing question 33...\n",
      "Processing question 34...\n",
      "Processing question 35...\n",
      "Processing question 36...\n",
      "Processing question 37...\n",
      "Processing question 38...\n",
      "Processing question 39...\n",
      "Processing question 40...\n",
      "Processing question 41...\n",
      "Processing question 42...\n",
      "Question Pitt's plan called for what attacks? already processed. Skipping...\n",
      "Processing question 43...\n",
      "Processing question 44...\n",
      "Processing question 45...\n",
      "Processing question 46...\n",
      "Processing question 47...\n",
      "Question The Amazon region is home to how many species of insect? already processed. Skipping...\n",
      "Processing question 48...\n",
      "Question What is the delta delimited in the South by? already processed. Skipping...\n",
      "Processing question 49...\n",
      "Data successfully written to /home/spiro/Studium/Harvard/00_Capstone/deh_data_results/data/hyde_based_contexts/hyde_based_contexts.csv\n"
     ]
    }
   ],
   "source": [
    "deh_hyde.generate_hyde_based_contexts(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the prompts and a function to get the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompts = [\n",
    "    PromptTemplate(\n",
    "        template=prompts.rag_text_prompts[2],\n",
    "        input_variables = [\"question\"]\n",
    "    ),\n",
    "    # PromptTemplate(\n",
    "    #     template=prompts.rag_text_prompts[1],\n",
    "    #     input_variables = [\"context\", \"question\"]\n",
    "    # ),\n",
    "\n",
    "    PromptTemplate(\n",
    "        template = \"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Please only use the following pieces of retrieved context to answer the question.\n",
    "    Use ten words maximum and keep the answer concise.\n",
    "\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "        input_variables = [\"context\", \"question\"]\n",
    "    ),\n",
    "\n",
    "    PromptTemplate(\n",
    "        template=(\"\"\"\n",
    "                You are an assistant for question-answering tasks.\n",
    "                Use the following pieces of retrieved context to answer the question.\n",
    "                If you don't know the answer, just return 'DONT KNOW'. \n",
    "                If you know the answer, keep it as short and concise as possible,\n",
    "                i.e. to a maximum of a couple of words.\n",
    "\n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "        ),\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template = prompts.hyde_prompts[1],\n",
    "        input_variables = [\"question\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the llm instance, based on the current query prompt\n",
    "def get_llm(current_query_prompt):\n",
    "    llm = ChatOllama(\n",
    "        prompt_template = current_query_prompt,\n",
    "        model = CHAT_MODEL_NAME,\n",
    "        max_tokens = MAX_TOKENS,\n",
    "        temperature = TEMPERATURE,\n",
    "        top_p = TOP_P,\n",
    "        frequency_penalty = FREQUENCY_PENALTY,\n",
    "        presence_penalty = PRESENCE_PENALTY,\n",
    "        gpu_use = True\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intialize the Vector Store (Chroma; Milvus to be added later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize the Chroma vector store\n",
    "vector_store = Chroma(\n",
    "    #collection_name = collection_name,\n",
    "    collection_name = \"deh_rag_per_article\",\n",
    "    embedding_function = embeddings,\n",
    "    persist_directory = persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles, contexts, qas, unique_qas, unique_questions = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If configured, chunk the SQuAD dataset and add the chunks to the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VECTORIZE_SQUAD_DATASET:\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    article_contexts_lens = []\n",
    "    curr_id = 0\n",
    "\n",
    "    for article in dataset:\n",
    "        title = article[\"title\"]\n",
    "        article_contexts = []\n",
    "\n",
    "        for p in article['paragraphs']:\n",
    "            article_contexts.append(p[\"context\"])\n",
    "\n",
    "        all_article_contexts = \"\\n\\n\".join(article_contexts)\n",
    "        article_chunks = text_splitter.create_documents([all_article_contexts]) \n",
    "        for article_chunk in article_chunks:\n",
    "            article_chunk.metadata = {\"source\": \"squad\", \"article\": title}\n",
    "        ids = [str(i) for i in list(range(curr_id, curr_id + len(article_chunks)))]\n",
    "        curr_id += len(article_chunks)\n",
    "        vector_store.add_documents(documents=article_chunks, ids=ids)\n",
    "else:\n",
    "    print(\"Not vectorizing the squad dataset...\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyde-based Contexts\n",
    "\n",
    "Hyde:\n",
    "\n",
    "- if configured: generate Hyde-based contexts and then persist\n",
    "- else: read the Hyde-based contexts from a .csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyde_contexts_already_done(csv_file_path, suffixes_l):\n",
    "    # if not os.path.exists(csv_file_path):\n",
    "    #     return []\n",
    "\n",
    "    hyde_contexts_already_done = []\n",
    "    for suffix in suffixes_l:\n",
    "        csv_file_path_iter = csv_file_path[:-4] + suffix + \".csv\"\n",
    "        print(f\"Reading hyde contexts from a csv file: {csv_file_path_iter}\")\n",
    "\n",
    "        with open(csv_file_path_iter, mode='r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            \n",
    "            for i, row in enumerate(csv_reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                hyde_contexts_already_done.append({\"question\": row[0], \"hyde_context\": row[1]})\n",
    "    print(f\"len(hyde_contexts_already_done) --> {len(hyde_contexts_already_done)}\")\n",
    "    return hyde_contexts_already_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = f\"{DATA_ROOT}/hyde_contexts.csv\"\n",
    "\n",
    "hyde_contexts_already_done = get_hyde_contexts_already_done(csv_file_path, ['_1', '_2'])\n",
    "hyde_questions_already_done = [hc[\"question\"] for hc in hyde_contexts_already_done]\n",
    "print(hyde_questions_already_done[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE_HYDE_CONTEXTS = False\n",
    "# print(DATA_ROOT)\n",
    "# if CREATE_HYDE_CONTEXTS:\n",
    "#     print(f\"Creating Hyde contexts for the dataset and persisting these in a csv file...\")\n",
    "#     unique_questions = list(set([qa[\"question\"] for qa in qas]))\n",
    "#     print(f\"Number of unique questions: {len(unique_questions)}\")\n",
    "\n",
    "#     current_query_prompt = query_prompts[3]\n",
    "#     llm = get_llm(current_query_prompt)\n",
    "#     runnable_chain = RunnableSequence(current_query_prompt | llm)\n",
    "\n",
    "#     for i, question in enumerate(unique_questions[:3000]):\n",
    "#         #if i %10 == 0:\n",
    "#         print(\"---------------------------------------------------------------\")\n",
    "#         print(f\"Processing question {i}...\")\n",
    "#         print(f\"Question: {question}\")\n",
    "#         if question in hyde_questions_already_done:\n",
    "#             print(\"Question already done...\")\n",
    "#             continue\n",
    "#         response = runnable_chain.invoke({\"question\": question})\n",
    "#         hyde_contexts_already_done.append({\"question\": question, \"hyde_context\": response.content})\n",
    "#         hyde_questions_already_done.append(question)\n",
    "#         print(\"\")\n",
    "    \n",
    "#     csv_file_path = f\"{DATA_ROOT}/hyde_contexts_2.csv\"\n",
    "#     persist_hyde_contexts(csv_file_path, hyde_contexts_already_done)\n",
    "#     hyde_contexts = hyde_contexts_already_done\n",
    "\n",
    "# else:\n",
    "#     print(f\"Reading the Hyde contexts from a csv file...\")\n",
    "\n",
    "#     hyde_contexts = []\n",
    "#     #with open(csv_file_path, mode='r') as file:\n",
    "#     with open(f\"{DATA_ROOT}/hyde_contexts_2_MASTER_COPY_UNBEDINGT_BEHALTEN.csv\", mode='r') as file:\n",
    "#         csv_reader = csv.reader(file)\n",
    "        \n",
    "#         for i, row in enumerate(csv_reader):\n",
    "#             if i == 0:\n",
    "#                 continue\n",
    "#             hyde_contexts.append({\"question\": row[0], \"hyde_context\": row[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyde_s = set([hyde_context[\"question\"] for hyde_context in hyde_contexts])\n",
    "# len(hyde_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
