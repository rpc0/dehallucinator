{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:  Identify effectiveness of \"no answer\" prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**\n",
    "LLM prompt is told: 'If you don't know the answer, just say that you don't know.' to help prevent hallucinations when context does not provide an answer.  It would be helpful to understand how effective prompt is at preventing responses when no context answer.\n",
    "\n",
    "**Test Approach**\n",
    "A sample of questions will be selected from QA corpus where answer is not possible.  LLM will be asked question with most relevant possible context but with expectation that context does not provide actual answer.  Assessment will measure what % of responses accurately indicate the LLM doesn't know.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckhill/miniforge3/envs/deh_measure/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Common import\n",
    "from deh.assessment import QASetRetriever\n",
    "from deh.assessment import QASetType\n",
    "from deh import settings\n",
    "from deh.eval import generate_experiment_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples:int = 5\n",
    "experiment_folder:str = \"../../data/evaluation/no-context-prompt-experiment/\"\n",
    "qa_data_set_file:str = \"../../data/qas/squad_qas.tsv\"\n",
    "\n",
    "# Create experiment folder:\n",
    "if not os.path.exists(experiment_folder):\n",
    "    Path(experiment_folder).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 questions sampled from QA corpus (../../data/qas/squad_qas.tsv)\n"
     ]
    }
   ],
   "source": [
    "# Only get impossible to answer questions:\n",
    "qa_set = QASetRetriever.get_qasets(\n",
    "    file_path = qa_data_set_file,\n",
    "    sample_size= num_samples,\n",
    "    qa_type = QASetType.IMPOSSIBLE_ONLY\n",
    ")\n",
    "\n",
    "print(f\"{len(qa_set)} questions sampled from QA corpus ({qa_data_set_file})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Responses with default prompt (does not specify to say don't know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5 question/answer pairs.\n",
      "Processing 2 of 5 question/answer pairs.\n",
      "Processing 3 of 5 question/answer pairs.\n",
      "Processing 4 of 5 question/answer pairs.\n",
      "Processing 5 of 5 question/answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response.question</th>\n",
       "      <th>response.hyde</th>\n",
       "      <th>response.answer</th>\n",
       "      <th>response.context</th>\n",
       "      <th>response.evaluation.grade</th>\n",
       "      <th>response.evaluation.description</th>\n",
       "      <th>response.execution_time</th>\n",
       "      <th>system_settings.gpu_enabled</th>\n",
       "      <th>system_settings.llm_model</th>\n",
       "      <th>system_settings.llm_prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>system_settings.text_chunk_size</th>\n",
       "      <th>system_settings.text_chunk_overlap</th>\n",
       "      <th>system_settings.context_similarity_threshold</th>\n",
       "      <th>system_settings.context_docs_retrieved</th>\n",
       "      <th>system_settings.docs_loaded</th>\n",
       "      <th>reference.question</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>reference.is_impossible</th>\n",
       "      <th>reference.ref_context_id</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was sold to foreign PTTs?</td>\n",
       "      <td>False</td>\n",
       "      <td>Some of the income from foreign PTTs was used ...</td>\n",
       "      <td>[{'id': None, 'metadata': {'source': '../data/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>00:00:03</td>\n",
       "      <td>True</td>\n",
       "      <td>llama3.1:8b-instruct-q3_K_L</td>\n",
       "      <td>rlm/rag-prompt-llama</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>What was sold to foreign PTTs?</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>1096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                response.question  response.hyde  \\\n",
       "0  What was sold to foreign PTTs?          False   \n",
       "\n",
       "                                     response.answer  \\\n",
       "0  Some of the income from foreign PTTs was used ...   \n",
       "\n",
       "                                    response.context  \\\n",
       "0  [{'id': None, 'metadata': {'source': '../data/...   \n",
       "\n",
       "  response.evaluation.grade response.evaluation.description  \\\n",
       "0                                                             \n",
       "\n",
       "  response.execution_time  system_settings.gpu_enabled  \\\n",
       "0                00:00:03                         True   \n",
       "\n",
       "     system_settings.llm_model system_settings.llm_prompt  ...  \\\n",
       "0  llama3.1:8b-instruct-q3_K_L       rlm/rag-prompt-llama  ...   \n",
       "\n",
       "  system_settings.text_chunk_size system_settings.text_chunk_overlap  \\\n",
       "0                            1500                                100   \n",
       "\n",
       "   system_settings.context_similarity_threshold  \\\n",
       "0                                           1.0   \n",
       "\n",
       "   system_settings.context_docs_retrieved  system_settings.docs_loaded  \\\n",
       "0                                       6                         1256   \n",
       "\n",
       "               reference.question  reference.ground_truth  \\\n",
       "0  What was sold to foreign PTTs?                           \n",
       "\n",
       "  reference.is_impossible reference.ref_context_id  reference_id  \n",
       "0                    True                     1096             1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert(response) -> pd.DataFrame:\n",
    "    \"\"\"Converts retrieved JSON response to Pandas DataFrame\"\"\"\n",
    "    return pd.json_normalize(\n",
    "        data=response\n",
    "    )\n",
    "\n",
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for answer.\n",
    "    parameters:\n",
    "    - hyde (h) = False\n",
    "    - evaluation (e) = False\n",
    "    - lmm prompt selection (lp) = 1\n",
    "    \"\"\"\n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return f\"http://{settings.API_ANSWER_ENDPOINT}/answer?{query_params}&h=False&e=False&lp=1\"\n",
    "\n",
    "# Collect response:\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store dataframe:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/prompt_1.pkl\" )\n",
    "exp_df[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Responses with default prompt (specify to say don't know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5 question/answer pairs.\n",
      "Processing 2 of 5 question/answer pairs.\n",
      "Processing 3 of 5 question/answer pairs.\n",
      "Processing 4 of 5 question/answer pairs.\n",
      "Processing 5 of 5 question/answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response.question</th>\n",
       "      <th>response.hyde</th>\n",
       "      <th>response.answer</th>\n",
       "      <th>response.context</th>\n",
       "      <th>response.evaluation.grade</th>\n",
       "      <th>response.evaluation.description</th>\n",
       "      <th>response.execution_time</th>\n",
       "      <th>system_settings.gpu_enabled</th>\n",
       "      <th>system_settings.llm_model</th>\n",
       "      <th>system_settings.llm_prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>system_settings.text_chunk_size</th>\n",
       "      <th>system_settings.text_chunk_overlap</th>\n",
       "      <th>system_settings.context_similarity_threshold</th>\n",
       "      <th>system_settings.context_docs_retrieved</th>\n",
       "      <th>system_settings.docs_loaded</th>\n",
       "      <th>reference.question</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>reference.is_impossible</th>\n",
       "      <th>reference.ref_context_id</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was sold to foreign PTTs?</td>\n",
       "      <td>False</td>\n",
       "      <td>I don't know what was sold to foreign PTTs. Th...</td>\n",
       "      <td>[{'id': None, 'metadata': {'source': '../data/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>True</td>\n",
       "      <td>llama3.1:8b-instruct-q3_K_L</td>\n",
       "      <td>rlm/rag-prompt-llama</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>What was sold to foreign PTTs?</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>1096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                response.question  response.hyde  \\\n",
       "0  What was sold to foreign PTTs?          False   \n",
       "\n",
       "                                     response.answer  \\\n",
       "0  I don't know what was sold to foreign PTTs. Th...   \n",
       "\n",
       "                                    response.context  \\\n",
       "0  [{'id': None, 'metadata': {'source': '../data/...   \n",
       "\n",
       "  response.evaluation.grade response.evaluation.description  \\\n",
       "0                                                             \n",
       "\n",
       "  response.execution_time  system_settings.gpu_enabled  \\\n",
       "0                00:00:02                         True   \n",
       "\n",
       "     system_settings.llm_model system_settings.llm_prompt  ...  \\\n",
       "0  llama3.1:8b-instruct-q3_K_L       rlm/rag-prompt-llama  ...   \n",
       "\n",
       "  system_settings.text_chunk_size system_settings.text_chunk_overlap  \\\n",
       "0                            1500                                100   \n",
       "\n",
       "   system_settings.context_similarity_threshold  \\\n",
       "0                                           1.0   \n",
       "\n",
       "   system_settings.context_docs_retrieved  system_settings.docs_loaded  \\\n",
       "0                                       6                         1256   \n",
       "\n",
       "               reference.question  reference.ground_truth  \\\n",
       "0  What was sold to foreign PTTs?                           \n",
       "\n",
       "  reference.is_impossible reference.ref_context_id  reference_id  \n",
       "0                    True                     1096             1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert(response) -> pd.DataFrame:\n",
    "    \"\"\"Converts retrieved JSON response to Pandas DataFrame\"\"\"\n",
    "    return pd.json_normalize(\n",
    "        data=response\n",
    "    )\n",
    "\n",
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for answer.\n",
    "    parameters:\n",
    "    - hyde (h) = False\n",
    "    - evaluation (e) = False\n",
    "    - lmm prompt selection (lp) = 0\n",
    "    \"\"\"\n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return f\"http://{settings.API_ANSWER_ENDPOINT}/answer?{query_params}&h=False&e=False&lp=0\"\n",
    "\n",
    "# Collect response:\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store dataframe:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/prompt_0.pkl\" )\n",
    "exp_df[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and merge Experiment Datasets for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment results:\n",
    "p_0_retr_df = pd.read_pickle(f\"{experiment_folder}/prompt_0.pkl\")[[\"response.question\", \"response.answer\"]]\n",
    "p_0_retr_df = p_0_retr_df.reset_index(drop=True)\n",
    "\n",
    "p_1_retr_df = pd.read_pickle(f\"{experiment_folder}/prompt_1.pkl\")[[\"response.question\", \"response.answer\"]]\n",
    "p_1_retr_df = p_1_retr_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response.question_p_0</th>\n",
       "      <th>response.answer_p_0</th>\n",
       "      <th>response.question_p_1</th>\n",
       "      <th>response.answer_p_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was sold to foreign PTTs?</td>\n",
       "      <td>I don't know what was sold to foreign PTTs. Th...</td>\n",
       "      <td>What was sold to foreign PTTs?</td>\n",
       "      <td>Some of the income from foreign PTTs was used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The process of growing more trees in the fores...</td>\n",
       "      <td>I don't know what the process of growing more ...</td>\n",
       "      <td>The process of growing more trees in the fores...</td>\n",
       "      <td>The process of growing more trees in the fores...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               response.question_p_0  \\\n",
       "0                     What was sold to foreign PTTs?   \n",
       "1  The process of growing more trees in the fores...   \n",
       "\n",
       "                                 response.answer_p_0  \\\n",
       "0  I don't know what was sold to foreign PTTs. Th...   \n",
       "1  I don't know what the process of growing more ...   \n",
       "\n",
       "                               response.question_p_1  \\\n",
       "0                     What was sold to foreign PTTs?   \n",
       "1  The process of growing more trees in the fores...   \n",
       "\n",
       "                                 response.answer_p_1  \n",
       "0  Some of the income from foreign PTTs was used ...  \n",
       "1  The process of growing more trees in the fores...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate datasets together for comparison:\n",
    "combined_df = pd.merge( p_0_retr_df, p_1_retr_df, left_index=True, right_index=True, suffixes=[\"_p_0\", \"_p_1\"])\n",
    "combined_df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response.question_p_0</th>\n",
       "      <th>response.answer_p_0</th>\n",
       "      <th>response.question_p_1</th>\n",
       "      <th>response.answer_p_1</th>\n",
       "      <th>DNK_p_0</th>\n",
       "      <th>DNK_p_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was sold to foreign PTTs?</td>\n",
       "      <td>I don't know what was sold to foreign PTTs. Th...</td>\n",
       "      <td>What was sold to foreign PTTs?</td>\n",
       "      <td>Some of the income from foreign PTTs was used ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The process of growing more trees in the fores...</td>\n",
       "      <td>I don't know what the process of growing more ...</td>\n",
       "      <td>The process of growing more trees in the fores...</td>\n",
       "      <td>The process of growing more trees in the fores...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               response.question_p_0  \\\n",
       "0                     What was sold to foreign PTTs?   \n",
       "1  The process of growing more trees in the fores...   \n",
       "\n",
       "                                 response.answer_p_0  \\\n",
       "0  I don't know what was sold to foreign PTTs. Th...   \n",
       "1  I don't know what the process of growing more ...   \n",
       "\n",
       "                               response.question_p_1  \\\n",
       "0                     What was sold to foreign PTTs?   \n",
       "1  The process of growing more trees in the fores...   \n",
       "\n",
       "                                 response.answer_p_1  DNK_p_0  DNK_p_1  \n",
       "0  Some of the income from foreign PTTs was used ...     True    False  \n",
       "1  The process of growing more trees in the fores...     True    False  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicate if answer contains don't know:\n",
    "combined_df[\"DNK_p_0\"] = combined_df['response.answer_p_0'].str.contains(\"don't know\")\n",
    "combined_df[\"DNK_p_1\"] = combined_df['response.answer_p_1'].str.contains(\"don't know\")\n",
    "\n",
    "combined_df[0:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hallucinations prevented comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percent p_0:\n",
    "pcnt_p_0 = len( combined_df[ combined_df[\"DNK_p_0\"] == True ] ) / len (combined_df) * 100\n",
    "pcnt_p_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percent p_1:\n",
    "pcnt_p_1 = len( combined_df[ combined_df[\"DNK_p_1\"] == True ] ) / len (combined_df) * 100\n",
    "pcnt_p_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Telling the prompt to respond with \"I do not know\" if not available in context reduces hallucinations by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcnt_p_0 - pcnt_p_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deh_measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
