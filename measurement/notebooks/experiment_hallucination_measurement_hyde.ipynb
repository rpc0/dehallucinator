{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:  Measurement of model hallucination w/ HYDE enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**  There are several metrics which are indicative of hallucination.  \n",
    "Specifcically we will look at _Answer Faithfulness_ as it identifies deviations from provided context which would likely be hallucinated:\n",
    "\n",
    "* [Answer Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html) - measures the factual consistency of generated answer vs. given context.\n",
    "\n",
    "Secondarily the following provide indications of total RAG system effectiveness at generating a correct answer:\n",
    "\n",
    "* [Answer Similarity](https://docs.ragas.io/en/stable/concepts/metrics/semantic_similarity.html) - (aka Answer Semantic Similarity) which is the simple cosine similarity of the generated and ground-truth answer.\n",
    "* [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html) - measures the accuracy of the generated answer when compared to the ground truth answer.\n",
    "\n",
    "**Test Approach:** A sample of questions will be selected from QA corpus.  Answers to questions will be generated via \"v1\" RAG implementation __With HYDE document creation enabled__.  The above measures will be compared to [v0 Baseline measurement](./experiment_hallucination_measurement.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common import\n",
    "from deh import settings\n",
    "from deh.assessment import QASetRetriever\n",
    "from deh.eval import generate_experiment_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples:int = 5\n",
    "experiment_folder:str = \"../../data/evaluations/hallucination-measurement-v1-hyde/\"\n",
    "qa_data_set_file:str = \"../../data/qas/squad_qas.tsv\"\n",
    "\n",
    "# Create experiment folder:\n",
    "if not os.path.exists(experiment_folder):\n",
    "    Path(experiment_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QASetRetriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa_set \u001b[38;5;241m=\u001b[39m \u001b[43mQASetRetriever\u001b[49m\u001b[38;5;241m.\u001b[39mget_qasets(\n\u001b[1;32m      2\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m qa_data_set_file,\n\u001b[1;32m      3\u001b[0m     sample_size\u001b[38;5;241m=\u001b[39m num_samples\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(qa_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m questions sampled from QA corpus (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqa_data_set_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QASetRetriever' is not defined"
     ]
    }
   ],
   "source": [
    "qa_set = QASetRetriever.get_qasets(\n",
    "    file_path = qa_data_set_file,\n",
    "    sample_size= num_samples\n",
    ")\n",
    "\n",
    "print(f\"{len(qa_set)} questions sampled from QA corpus ({qa_data_set_file})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5 question/answer pairs.\n",
      "Processing 2 of 5 question/answer pairs.\n",
      "Processing 3 of 5 question/answer pairs.\n",
      "Processing 4 of 5 question/answer pairs.\n",
      "Processing 5 of 5 question/answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page_content</th>\n",
       "      <th>type</th>\n",
       "      <th>metadata.source</th>\n",
       "      <th>metadata.similarity_score</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>hyde</th>\n",
       "      <th>evaluation.grade</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>reference.is_impossible</th>\n",
       "      <th>json</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Other: Civil rights leader W. E. B. Du Bois; p...</td>\n",
       "      <td>Document</td>\n",
       "      <td>../data/contexts/context_650.context</td>\n",
       "      <td>0.486736</td>\n",
       "      <td>Conan O'Brien, a TV host and writer, attended ...</td>\n",
       "      <td>What tv host and writer went to Harvard?</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>Conan O'Brien</td>\n",
       "      <td>False</td>\n",
       "      <td>{\"response\": {\"question\": \"What tv host and wr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                       page_content      type  \\\n",
       "0  None  Other: Civil rights leader W. E. B. Du Bois; p...  Document   \n",
       "\n",
       "                        metadata.source  metadata.similarity_score  \\\n",
       "0  ../data/contexts/context_650.context                   0.486736   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Conan O'Brien, a TV host and writer, attended ...   \n",
       "\n",
       "                                   question  hyde evaluation.grade  \\\n",
       "0  What tv host and writer went to Harvard?  True                    \n",
       "\n",
       "  reference.ground_truth  reference.is_impossible  \\\n",
       "0          Conan O'Brien                    False   \n",
       "\n",
       "                                                json  reference_id  \n",
       "0  {\"response\": {\"question\": \"What tv host and wr...             1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for context retrieval.\"\"\"\n",
    "    hyde= True  # Enable HYDE for the context retrieval\n",
    "    evaluation = False\n",
    "    \n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return f\"http://{settings.API_ANSWER_ENDPOINT}/answer?h={hyde}&e={evaluation}&{query_params}\"\n",
    "\n",
    "def convert(response) -> pd.DataFrame:\n",
    "    \"\"\"Converts retrieved JSON response to Pandas DataFrame\"\"\"\n",
    "    response_df = pd.json_normalize(\n",
    "        data=response[\"response\"], record_path=\"context\", meta=[\"answer\",\"question\", \"hyde\", [\"evaluation\", \"grade\"]]\n",
    "    )\n",
    "\n",
    "    # Add reference/evaluation values:\n",
    "    response_df[\"reference.ground_truth\"] = response[\"reference\"][\"ground_truth\"]\n",
    "    response_df[\"reference.is_impossible\"] = response[\"reference\"][\"is_impossible\"]\n",
    "\n",
    "    # Add full JSON response incase needed:\n",
    "    response_df[\"json\"] = json.dumps(response)\n",
    "    return response_df\n",
    "\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store the generated response:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/response-v1.pkl\" )\n",
    "exp_df[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Measures for Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from enum import Enum\n",
    "class LLM_PLATFORMS (Enum):\n",
    "    OPENAI = 1\n",
    "    OLLAMA = 2\n",
    "\n",
    "# Either local (Ollama) or remote (OpenAI) evaluation models can be used:\n",
    "evaluation_model = LLM_PLATFORMS.OPENAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI platform.\n"
     ]
    }
   ],
   "source": [
    "if evaluation_model == LLM_PLATFORMS.OPENAI:\n",
    "    print (\"Using OpenAI platform.\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    print (\"Using OLLAMA platform\")\n",
    "    llm = Ollama(\n",
    "        base_url=settings.OLLAMA_URL,\n",
    "        model=settings.ASSESSMENT_LLM_MODEL,\n",
    "    )\n",
    "    embeddings = OllamaEmbeddings(\n",
    "        base_url=settings.OLLAMA_URL,\n",
    "        model=settings.ASSESSMENT_EMBEDDING_MODEL,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knightal/deh_m/lib/python3.12/site-packages/ragas/prompt/base.py:9: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.llms.prompt import PromptValue\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "import ragas.metrics as metrics\n",
    "from ragas.run_config import RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Other: Civil rights leader W. E. B. Du Bois; ...</td>\n",
       "      <td>What tv host and writer went to Harvard?</td>\n",
       "      <td>Conan O'Brien</td>\n",
       "      <td>Conan O'Brien, a TV host and writer, attended ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             retrieved_contexts  \\\n",
       "reference_id                                                      \n",
       "1             [Other: Civil rights leader W. E. B. Du Bois; ...   \n",
       "\n",
       "                                              question   ground_truth  \\\n",
       "reference_id                                                            \n",
       "1             What tv host and writer went to Harvard?  Conan O'Brien   \n",
       "\n",
       "                                                         answer  \n",
       "reference_id                                                     \n",
       "1             Conan O'Brien, a TV host and writer, attended ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to Dataset\n",
    "responses_df = pd.read_pickle(f\"{experiment_folder}/response-v1.pkl\")\n",
    "\n",
    "responses_df = responses_df.groupby(\"reference_id\").agg(\n",
    "    retrieved_contexts = ('page_content', lambda x: list(x)),\n",
    "    question = ('question','first'),\n",
    "    ground_truth = ('reference.ground_truth', 'first'),\n",
    "    answer = ('answer', 'first')\n",
    "    )\n",
    "\n",
    "responses_df[0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "responses_ds = Dataset.from_pandas( responses_df)\n",
    "\n",
    "evaluation_ds = evaluate(\n",
    "    dataset = responses_ds,\n",
    "    metrics = [metrics.answer_similarity, metrics.faithfulness, metrics.answer_correctness],\n",
    "    embeddings = embeddings,\n",
    "    llm = llm,\n",
    "    run_config=RunConfig(\n",
    "        max_workers=5\n",
    "    ),\n",
    "    raise_exceptions=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>evaluation.llm_model</th>\n",
       "      <th>evaluation.embedding_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What tv host and writer went to Harvard?</td>\n",
       "      <td>[Other: Civil rights leader W. E. B. Du Bois; ...</td>\n",
       "      <td>Conan O'Brien, a TV host and writer, attended ...</td>\n",
       "      <td>Conan O'Brien</td>\n",
       "      <td>0.881632</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.595408</td>\n",
       "      <td>openai.gpt-4o-mini</td>\n",
       "      <td>openai.Text-embedding-ada-002-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where were Persians more successful compared t...</td>\n",
       "      <td>[A rich cultural diversity developed during th...</td>\n",
       "      <td>I don't know where Persians were more successf...</td>\n",
       "      <td>reaching the highest-post in the government</td>\n",
       "      <td>0.736512</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.184128</td>\n",
       "      <td>openai.gpt-4o-mini</td>\n",
       "      <td>openai.Text-embedding-ada-002-v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0           What tv host and writer went to Harvard?   \n",
       "1  Where were Persians more successful compared t...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Other: Civil rights leader W. E. B. Du Bois; ...   \n",
       "1  [A rich cultural diversity developed during th...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Conan O'Brien, a TV host and writer, attended ...   \n",
       "1  I don't know where Persians were more successf...   \n",
       "\n",
       "                                  ground_truth  answer_similarity  \\\n",
       "0                                Conan O'Brien           0.881632   \n",
       "1  reaching the highest-post in the government           0.736512   \n",
       "\n",
       "   faithfulness  answer_correctness evaluation.llm_model  \\\n",
       "0      0.833333            0.595408   openai.gpt-4o-mini   \n",
       "1      0.500000            0.184128   openai.gpt-4o-mini   \n",
       "\n",
       "         evaluation.embedding_model  \n",
       "0  openai.Text-embedding-ada-002-v2  \n",
       "1  openai.Text-embedding-ada-002-v2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = evaluation_ds.to_pandas()\n",
    "\n",
    "# Evaluation metadata\n",
    "eval_df[\"evaluation.llm_model\"] = \"openai.gpt-4o-mini\"\n",
    "eval_df[\"evaluation.embedding_model\"] = \"openai.Text-embedding-ada-002-v2\"\n",
    "\n",
    "eval_df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_pickle( f\"{experiment_folder}/results-v0-openai.pkl\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithful vs. Non-Faithful Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_pickle( f\"{experiment_folder}/results-v0-openai.pkl\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of responses indicated as faithful: 20.0%\n"
     ]
    }
   ],
   "source": [
    "faithfulness_threshold = 0.75 #75% of claims are considered supported by context.\n",
    "ttl = len ( eval_df )\n",
    "faithful = len( eval_df[ eval_df[\"faithfulness\"] >= faithfulness_threshold ] )\n",
    "\n",
    "print (f\"% of responses indicated as faithful: {faithful/ttl*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deh_m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
