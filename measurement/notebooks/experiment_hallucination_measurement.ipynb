{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:  Establish a Base-line Measurement of model hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**  There are several metrics which are indicative of hallucination.  \n",
    "Specifcically we will look at _Answer Faithfulness_ as it identifies deviations from provided context which would likely be hallucinated:\n",
    "\n",
    "* [Answer Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html) - measures the factual consistency of generated answer vs. given context.\n",
    "\n",
    "Secondarily the following provide indications of total RAG system effectiveness at generating a correct answer:\n",
    "\n",
    "* [Answer Similarity](https://docs.ragas.io/en/stable/concepts/metrics/semantic_similarity.html) - (aka Answer Semantic Similarity) which is the simple cosine similarity of the generated and ground-truth answer.\n",
    "* [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html) - measures the accuracy of the generated answer when compared to the ground truth answer.\n",
    "\n",
    "**Test Approach:** A sample of questions will be selected from QA corpus.  Answers to questions will be generated via \"v0\" RAG implementation (excluding mitigation and advanced processing).  The above measures will be generated to establish a baseline of comparison for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.get(\"http_proxy\")\n",
    "os.environ[\"http_proxy\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckhill/miniforge3/envs/deh_measure/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Common import\n",
    "from deh.assessment import QASetRetriever\n",
    "from deh import settings\n",
    "from deh.eval import generate_experiment_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples:int = 2\n",
    "experiment_folder:str = \"../../data/evaluation/baseline-v0/\"\n",
    "qa_data_set_file:str = \"../../data/qas/squad_qas.tsv\"\n",
    "\n",
    "# Create experiment folder:\n",
    "if not os.path.exists(experiment_folder):\n",
    "    Path(experiment_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 questions sampled from QA corpus (../../data/qas/squad_qas.tsv)\n"
     ]
    }
   ],
   "source": [
    "qa_set = QASetRetriever.get_qasets(\n",
    "    file_path = qa_data_set_file,\n",
    "    sample_size= num_samples\n",
    ")\n",
    "\n",
    "print(f\"{len(qa_set)} questions sampled from QA corpus ({qa_data_set_file})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 2 question/answer pairs.\n",
      "Processing 2 of 2 question/answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page_content</th>\n",
       "      <th>type</th>\n",
       "      <th>metadata.source</th>\n",
       "      <th>metadata.similarity_score</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>hyde</th>\n",
       "      <th>evaluation.grade</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>reference.is_impossible</th>\n",
       "      <th>json</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Harvard's academic programs operate on a semes...</td>\n",
       "      <td>Document</td>\n",
       "      <td>/data/contexts/context_640.context</td>\n",
       "      <td>0.659308</td>\n",
       "      <td>Shortening the admission event is also referre...</td>\n",
       "      <td>What is another term for shortening the admiss...</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>shortening the cutoff</td>\n",
       "      <td>False</td>\n",
       "      <td>{\"response\": {\"question\": \"What is another ter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                       page_content      type  \\\n",
       "0  None  Harvard's academic programs operate on a semes...  Document   \n",
       "\n",
       "                      metadata.source  metadata.similarity_score  \\\n",
       "0  /data/contexts/context_640.context                   0.659308   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Shortening the admission event is also referre...   \n",
       "\n",
       "                                            question   hyde evaluation.grade  \\\n",
       "0  What is another term for shortening the admiss...  False                    \n",
       "\n",
       "  reference.ground_truth  reference.is_impossible  \\\n",
       "0  shortening the cutoff                    False   \n",
       "\n",
       "                                                json  reference_id  \n",
       "0  {\"response\": {\"question\": \"What is another ter...             1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for context retrieval.\"\"\"\n",
    "    hyde= False\n",
    "    evaluation = False\n",
    "    \n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return f\"http://{settings.API_ANSWER_ENDPOINT}/answer?h={hyde}&e={evaluation}&{query_params}\"\n",
    "\n",
    "def convert(response) -> pd.DataFrame:\n",
    "    \"\"\"Converts retrieved JSON response to Pandas DataFrame\"\"\"\n",
    "    response_df = pd.json_normalize(\n",
    "        data=response[\"response\"], record_path=\"context\", meta=[\"answer\",\"question\", \"hyde\", [\"evaluation\", \"grade\"]]\n",
    "    )\n",
    "\n",
    "    # Add reference/evaluation values:\n",
    "    response_df[\"reference.ground_truth\"] = response[\"reference\"][\"ground_truth\"]\n",
    "    response_df[\"reference.is_impossible\"] = response[\"reference\"][\"is_impossible\"]\n",
    "\n",
    "    # Add full JSON response incase needed:\n",
    "    response_df[\"json\"] = json.dumps(response)\n",
    "    return response_df\n",
    "\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store the generated response:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/baseline-response-v0.pkl\" )\n",
    "exp_df[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Measures for Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "embedding = OllamaEmbeddings(\n",
    "    base_url=settings.OLLAMA_URL,\n",
    "    model=settings.ASSESSMENT_EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=settings.OLLAMA_URL,\n",
    "    model=settings.ASSESSMENT_LLM_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "import ragas.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Harvard's academic programs operate on a seme...</td>\n",
       "      <td>What is another term for shortening the admiss...</td>\n",
       "      <td>shortening the cutoff</td>\n",
       "      <td>Shortening the admission event is also referre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             retrieved_contexts  \\\n",
       "reference_id                                                      \n",
       "1             [Harvard's academic programs operate on a seme...   \n",
       "\n",
       "                                                       question  \\\n",
       "reference_id                                                      \n",
       "1             What is another term for shortening the admiss...   \n",
       "\n",
       "                       ground_truth  \\\n",
       "reference_id                          \n",
       "1             shortening the cutoff   \n",
       "\n",
       "                                                         answer  \n",
       "reference_id                                                     \n",
       "1             Shortening the admission event is also referre...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to Dataset\n",
    "responses_df = pd.read_pickle(f\"{experiment_folder}/baseline-response-v0.pkl\")\n",
    "\n",
    "responses_df = responses_df.groupby(\"reference_id\").agg(\n",
    "    retrieved_contexts = ('page_content', lambda x: list(x)),\n",
    "    question = ('question','first'),\n",
    "    ground_truth = ('reference.ground_truth', 'first'),\n",
    "    answer = ('answer', 'first')\n",
    "    )\n",
    "\n",
    "responses_df[0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 1/2 [00:22<00:22, 22.28s/it]Failed to parse output. Returning None.\n",
      "Evaluating: 100%|██████████| 2/2 [00:49<00:00, 24.89s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "responses_ds = Dataset.from_pandas( responses_df)\n",
    "\n",
    "evaluation_ds = evaluate(\n",
    "    dataset = responses_ds,\n",
    "    metrics = [metrics.answer_similarity, metrics.faithfulness, metrics.answer_correctness],\n",
    "    embeddings = embedding,\n",
    "    llm = llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is another term for shortening the admiss...</td>\n",
       "      <td>[Harvard's academic programs operate on a seme...</td>\n",
       "      <td>Shortening the admission event is also referre...</td>\n",
       "      <td>shortening the cutoff</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many elements did Aristotle believe the te...</td>\n",
       "      <td>[Aristotle provided a philosophical discussion...</td>\n",
       "      <td>According to the provided context, Aristotle b...</td>\n",
       "      <td>four</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is another term for shortening the admiss...   \n",
       "1  How many elements did Aristotle believe the te...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Harvard's academic programs operate on a seme...   \n",
       "1  [Aristotle provided a philosophical discussion...   \n",
       "\n",
       "                                              answer           ground_truth  \\\n",
       "0  Shortening the admission event is also referre...  shortening the cutoff   \n",
       "1  According to the provided context, Aristotle b...                   four   \n",
       "\n",
       "   faithfulness  \n",
       "0           NaN  \n",
       "1           1.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deh_measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
