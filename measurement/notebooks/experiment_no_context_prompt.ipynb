{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:  Impact of context on RAG performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**\n",
    "To establish a base-line understanding and validate the thesis that context is a key element of hallucination reduction we can measure the change in RAGAS metrics for LLM that uses context or not.\n",
    "\n",
    "**Test Approach**\n",
    "Ask a question to LLM with and without a context.  We expect that RAGAS measures with a context should be significantly better than without.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckhill/miniforge3/envs/deh_measure/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Common import\n",
    "from deh.assessment import QASetRetriever\n",
    "from deh.assessment import QASetType\n",
    "from deh import settings\n",
    "from deh.eval import generate_experiment_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples:int = 5\n",
    "experiment_folder:str = \"../../data/evaluation/no-context-prompt-experiment/\"\n",
    "qa_data_set_file:str = \"../../data/qas/squad_qas.tsv\"\n",
    "\n",
    "# Create experiment folder:\n",
    "if not os.path.exists(experiment_folder):\n",
    "    Path(experiment_folder).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 questions sampled from QA corpus (../../data/qas/squad_qas.tsv)\n"
     ]
    }
   ],
   "source": [
    "# Only get impossible to answer questions:\n",
    "qa_set = QASetRetriever.get_qasets(\n",
    "    file_path = qa_data_set_file,\n",
    "    sample_size= num_samples,\n",
    "    qa_type = QASetType.POSSIBLE_ONLY\n",
    ")\n",
    "\n",
    "print(f\"{len(qa_set)} questions sampled from QA corpus ({qa_data_set_file})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Responses with default prompt (using context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5 question/answer pairs.\n",
      "Processing 2 of 5 question/answer pairs.\n",
      "Processing 3 of 5 question/answer pairs.\n",
      "Processing 4 of 5 question/answer pairs.\n",
      "Processing 5 of 5 question/answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response.question</th>\n",
       "      <th>response.hyde</th>\n",
       "      <th>response.answer</th>\n",
       "      <th>response.context</th>\n",
       "      <th>response.evaluation.grade</th>\n",
       "      <th>response.evaluation.description</th>\n",
       "      <th>response.execution_time</th>\n",
       "      <th>system_settings.gpu_enabled</th>\n",
       "      <th>system_settings.llm_model</th>\n",
       "      <th>system_settings.llm_prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>system_settings.text_chunk_size</th>\n",
       "      <th>system_settings.text_chunk_overlap</th>\n",
       "      <th>system_settings.context_similarity_threshold</th>\n",
       "      <th>system_settings.context_docs_retrieved</th>\n",
       "      <th>system_settings.docs_loaded</th>\n",
       "      <th>reference.question</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>reference.is_impossible</th>\n",
       "      <th>reference.ref_context_id</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>False</td>\n",
       "      <td>1st century BC.</td>\n",
       "      <td>[{'id': None, 'metadata': {'source': '../data/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>True</td>\n",
       "      <td>llama3.1:8b-instruct-q3_K_L</td>\n",
       "      <td>rlm/rag-prompt-llama</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>1st</td>\n",
       "      <td>False</td>\n",
       "      <td>1073</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   response.question  response.hyde  \\\n",
       "0  What century did the name of the Rhine come from?          False   \n",
       "\n",
       "   response.answer                                   response.context  \\\n",
       "0  1st century BC.  [{'id': None, 'metadata': {'source': '../data/...   \n",
       "\n",
       "  response.evaluation.grade response.evaluation.description  \\\n",
       "0                                                             \n",
       "\n",
       "  response.execution_time  system_settings.gpu_enabled  \\\n",
       "0                00:00:02                         True   \n",
       "\n",
       "     system_settings.llm_model system_settings.llm_prompt  ...  \\\n",
       "0  llama3.1:8b-instruct-q3_K_L       rlm/rag-prompt-llama  ...   \n",
       "\n",
       "  system_settings.text_chunk_size system_settings.text_chunk_overlap  \\\n",
       "0                            1500                                100   \n",
       "\n",
       "   system_settings.context_similarity_threshold  \\\n",
       "0                                           1.0   \n",
       "\n",
       "   system_settings.context_docs_retrieved  system_settings.docs_loaded  \\\n",
       "0                                       6                         1256   \n",
       "\n",
       "                                  reference.question  reference.ground_truth  \\\n",
       "0  What century did the name of the Rhine come from?                     1st   \n",
       "\n",
       "  reference.is_impossible reference.ref_context_id  reference_id  \n",
       "0                   False                     1073             1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert(response) -> pd.DataFrame:\n",
    "    \"\"\"Converts retrieved JSON response to Pandas DataFrame\"\"\"\n",
    "    response_df = pd.json_normalize(\n",
    "        data=response\n",
    "    )\n",
    "\n",
    "    response_df[\"reference.ground_truth\"] = response[\"reference\"][\"ground_truth\"]\n",
    "    response_df[\"reference.is_impossible\"] = response[\"reference\"][\"is_impossible\"]\n",
    "\n",
    "    return response_df\n",
    "\n",
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for answer.\n",
    "    parameters:\n",
    "    - hyde (h) = False\n",
    "    - evaluation (e) = False\n",
    "    - lmm prompt selection (lp) = 1\n",
    "    \"\"\"\n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return f\"http://{settings.API_ANSWER_ENDPOINT}/answer?{query_params}&h=False&e=False&lp=1\"\n",
    "\n",
    "# Collect response:\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store dataframe:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/prompt_1.pkl\" )\n",
    "exp_df[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Responses without context provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5 question/answer pairs.\n",
      "Processing 2 of 5 question/answer pairs.\n",
      "Processing 3 of 5 question/answer pairs.\n",
      "Processing 4 of 5 question/answer pairs.\n",
      "Processing 5 of 5 question/answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response.question</th>\n",
       "      <th>response.hyde</th>\n",
       "      <th>response.answer</th>\n",
       "      <th>response.context</th>\n",
       "      <th>response.evaluation.grade</th>\n",
       "      <th>response.evaluation.description</th>\n",
       "      <th>response.execution_time</th>\n",
       "      <th>system_settings.gpu_enabled</th>\n",
       "      <th>system_settings.llm_model</th>\n",
       "      <th>system_settings.llm_prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>system_settings.text_chunk_size</th>\n",
       "      <th>system_settings.text_chunk_overlap</th>\n",
       "      <th>system_settings.context_similarity_threshold</th>\n",
       "      <th>system_settings.context_docs_retrieved</th>\n",
       "      <th>system_settings.docs_loaded</th>\n",
       "      <th>reference.question</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>reference.is_impossible</th>\n",
       "      <th>reference.ref_context_id</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>False</td>\n",
       "      <td>The name \"Rhine\" comes from a Celtic tribe, th...</td>\n",
       "      <td>[{'id': None, 'metadata': {'source': '../data/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>00:00:01</td>\n",
       "      <td>True</td>\n",
       "      <td>llama3.1:8b-instruct-q3_K_L</td>\n",
       "      <td>rlm/rag-prompt-llama</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>1st</td>\n",
       "      <td>False</td>\n",
       "      <td>1073</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   response.question  response.hyde  \\\n",
       "0  What century did the name of the Rhine come from?          False   \n",
       "\n",
       "                                     response.answer  \\\n",
       "0  The name \"Rhine\" comes from a Celtic tribe, th...   \n",
       "\n",
       "                                    response.context  \\\n",
       "0  [{'id': None, 'metadata': {'source': '../data/...   \n",
       "\n",
       "  response.evaluation.grade response.evaluation.description  \\\n",
       "0                                                             \n",
       "\n",
       "  response.execution_time  system_settings.gpu_enabled  \\\n",
       "0                00:00:01                         True   \n",
       "\n",
       "     system_settings.llm_model system_settings.llm_prompt  ...  \\\n",
       "0  llama3.1:8b-instruct-q3_K_L       rlm/rag-prompt-llama  ...   \n",
       "\n",
       "  system_settings.text_chunk_size system_settings.text_chunk_overlap  \\\n",
       "0                            1500                                100   \n",
       "\n",
       "   system_settings.context_similarity_threshold  \\\n",
       "0                                           1.0   \n",
       "\n",
       "   system_settings.context_docs_retrieved  system_settings.docs_loaded  \\\n",
       "0                                       6                         1256   \n",
       "\n",
       "                                  reference.question  reference.ground_truth  \\\n",
       "0  What century did the name of the Rhine come from?                     1st   \n",
       "\n",
       "  reference.is_impossible reference.ref_context_id  reference_id  \n",
       "0                   False                     1073             1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for answer.\n",
    "    parameters:\n",
    "    - hyde (h) = False\n",
    "    - evaluation (e) = False\n",
    "    - lmm prompt selection (lp) = 2\n",
    "    \"\"\"\n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return f\"http://{settings.API_ANSWER_ENDPOINT}/answer?{query_params}&h=False&e=False&lp=2\"\n",
    "\n",
    "# Collect response:\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store dataframe:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/prompt_2.pkl\" )\n",
    "exp_df[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Measures for Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Either local (Ollama) or remote (OpenAI) evaluation models can be used:\n",
    "\n",
    "embedding = OllamaEmbeddings(\n",
    "    base_url=settings.OLLAMA_URL,\n",
    "    model=settings.ASSESSMENT_EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=settings.OLLAMA_URL,\n",
    "    model=settings.ASSESSMENT_LLM_MODEL,\n",
    ")\n",
    "\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "openai_embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "import ragas.metrics as metrics\n",
    "from ragas.run_config import RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = [\n",
    "    (\"prompt_1.pkl\",\"prompt_1_eval.pkl\"),\n",
    "    (\"prompt_2.pkl\", \"prompt_2_eval.pkl\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [01:22<00:00,  8.25s/it]\n",
      "Evaluating:  50%|█████     | 5/10 [00:03<00:03,  1.40it/s]Failed to parse output. Returning None.\n",
      "Evaluating: 100%|██████████| 10/10 [01:33<00:00,  9.32s/it]\n"
     ]
    }
   ],
   "source": [
    "for input_file, output_file in comp_data:\n",
    "\n",
    "    # Load Data and format\n",
    "    responses_df = pd.read_pickle(f\"{experiment_folder}/{input_file}\")\n",
    "    responses_df = responses_df.rename(columns={\n",
    "        \"response.question\" : \"question\",\n",
    "        \"response.answer\" : \"answer\",\n",
    "        \"reference.ground_truth\" : \"ground_truth\"\n",
    "    })[[\"question\", \"answer\", \"ground_truth\"]]\n",
    "\n",
    "    \n",
    "    # Convert to Dataset\n",
    "    responses_ds = Dataset.from_pandas( responses_df)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluation_ds = evaluate(\n",
    "        dataset = responses_ds,\n",
    "        metrics = [metrics.answer_similarity, metrics.answer_correctness],\n",
    "        embeddings = embedding,\n",
    "        llm = llm,\n",
    "        run_config=RunConfig(\n",
    "            max_workers=5\n",
    "        ),\n",
    "        raise_exceptions=False\n",
    "    )\n",
    "\n",
    "    eval_df = evaluation_ds.to_pandas()\n",
    "\n",
    "    # Evaluation metadata\n",
    "    eval_df[\"evaluation.llm_model\"] = \"ollama\"\n",
    "    eval_df[\"evaluation.embedding_model\"] = \"ollama\"\n",
    "\n",
    "    # Persist\n",
    "    eval_df.to_pickle( f\"{experiment_folder}/{output_file}\" )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>evaluation.llm_model</th>\n",
       "      <th>evaluation.embedding_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>The name \"Rhine\" comes from a Celtic tribe, th...</td>\n",
       "      <td>1st</td>\n",
       "      <td>0.646525</td>\n",
       "      <td>0.661631</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What type of flower is sought on Midsummer's Eve?</td>\n",
       "      <td>Lily or wildflowers, often associated with Sco...</td>\n",
       "      <td>fern</td>\n",
       "      <td>0.725134</td>\n",
       "      <td>0.181284</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What century did the name of the Rhine come from?   \n",
       "1  What type of flower is sought on Midsummer's Eve?   \n",
       "\n",
       "                                              answer ground_truth  \\\n",
       "0  The name \"Rhine\" comes from a Celtic tribe, th...          1st   \n",
       "1  Lily or wildflowers, often associated with Sco...         fern   \n",
       "\n",
       "   answer_similarity  answer_correctness evaluation.llm_model  \\\n",
       "0           0.646525            0.661631               ollama   \n",
       "1           0.725134            0.181284               ollama   \n",
       "\n",
       "  evaluation.embedding_model  \n",
       "0                     ollama  \n",
       "1                     ollama  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and merge Experiment Datasets for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment results:\n",
    "context_retr_df = pd.read_pickle(f\"{experiment_folder}/prompt_1_eval.pkl\")\n",
    "no_context_retr_df = pd.read_pickle(f\"{experiment_folder}/prompt_2_eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_context</th>\n",
       "      <th>answer_context</th>\n",
       "      <th>ground_truth_context</th>\n",
       "      <th>answer_similarity_context</th>\n",
       "      <th>answer_correctness_context</th>\n",
       "      <th>evaluation.llm_model_context</th>\n",
       "      <th>evaluation.embedding_model_context</th>\n",
       "      <th>question_no_context</th>\n",
       "      <th>answer_no_context</th>\n",
       "      <th>ground_truth_no_context</th>\n",
       "      <th>answer_similarity_no_context</th>\n",
       "      <th>answer_correctness_no_context</th>\n",
       "      <th>evaluation.llm_model_no_context</th>\n",
       "      <th>evaluation.embedding_model_no_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>1st century BC.</td>\n",
       "      <td>1st</td>\n",
       "      <td>0.772291</td>\n",
       "      <td>0.693073</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>The name \"Rhine\" comes from a Celtic tribe, th...</td>\n",
       "      <td>1st</td>\n",
       "      <td>0.646525</td>\n",
       "      <td>0.661631</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What type of flower is sought on Midsummer's Eve?</td>\n",
       "      <td>The sought flower is the fern flower.</td>\n",
       "      <td>fern</td>\n",
       "      <td>0.914749</td>\n",
       "      <td>0.728687</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What type of flower is sought on Midsummer's Eve?</td>\n",
       "      <td>Lily or wildflowers, often associated with Sco...</td>\n",
       "      <td>fern</td>\n",
       "      <td>0.725134</td>\n",
       "      <td>0.181284</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question_context  \\\n",
       "0  What century did the name of the Rhine come from?   \n",
       "1  What type of flower is sought on Midsummer's Eve?   \n",
       "\n",
       "                          answer_context ground_truth_context  \\\n",
       "0                        1st century BC.                  1st   \n",
       "1  The sought flower is the fern flower.                 fern   \n",
       "\n",
       "   answer_similarity_context  answer_correctness_context  \\\n",
       "0                   0.772291                    0.693073   \n",
       "1                   0.914749                    0.728687   \n",
       "\n",
       "  evaluation.llm_model_context evaluation.embedding_model_context  \\\n",
       "0                       ollama                             ollama   \n",
       "1                       ollama                             ollama   \n",
       "\n",
       "                                 question_no_context  \\\n",
       "0  What century did the name of the Rhine come from?   \n",
       "1  What type of flower is sought on Midsummer's Eve?   \n",
       "\n",
       "                                   answer_no_context ground_truth_no_context  \\\n",
       "0  The name \"Rhine\" comes from a Celtic tribe, th...                     1st   \n",
       "1  Lily or wildflowers, often associated with Sco...                    fern   \n",
       "\n",
       "   answer_similarity_no_context  answer_correctness_no_context  \\\n",
       "0                      0.646525                       0.661631   \n",
       "1                      0.725134                       0.181284   \n",
       "\n",
       "  evaluation.llm_model_no_context evaluation.embedding_model_no_context  \n",
       "0                          ollama                                ollama  \n",
       "1                          ollama                                ollama  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate datasets together for comparison:\n",
    "combined_df = pd.merge( context_retr_df, no_context_retr_df, left_index=True, right_index=True, suffixes=[\"_context\", \"_no_context\"])\n",
    "combined_df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAGAS comparison (context - no_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_similarity_diff</th>\n",
       "      <th>answer_correctness_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125765</td>\n",
       "      <td>0.031441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.189615</td>\n",
       "      <td>0.547404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_similarity_diff  answer_correctness_diff\n",
       "0                0.125765                 0.031441\n",
       "1                0.189615                 0.547404"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric comparison:\n",
    "combined_df[\"answer_similarity_diff\"] = combined_df[\"answer_similarity_context\"] - combined_df[\"answer_similarity_no_context\"]\n",
    "combined_df[\"answer_correctness_diff\"] = combined_df[\"answer_correctness_context\"] - combined_df[\"answer_correctness_no_context\"]\n",
    "\n",
    "\n",
    "combined_df[[\"answer_similarity_diff\", \"answer_correctness_diff\"]][0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer Similiarity Difference:\n",
      "Min: -0.16755055549431708\n",
      "Avg: 0.0537072351784353\n",
      "Max: 0.18961495380685867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Answer Similiarity:\n",
    "print(f\"\"\"\n",
    "Answer Similiarity Difference:\n",
    "Min: {combined_df[\"answer_similarity_diff\"].min()}\n",
    "Avg: {combined_df[\"answer_similarity_diff\"].mean()}\n",
    "Max: {combined_df[\"answer_similarity_diff\"].max()}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer Correctness Difference:\n",
      "Min: -0.14188763887357947\n",
      "Avg: 0.10414109450889449\n",
      "Max: 0.5474037384517146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Answer Correctness:\n",
    "print(f\"\"\"\n",
    "Answer Correctness Difference:\n",
    "Min: {combined_df[\"answer_correctness_diff\"].min()}\n",
    "Avg: {combined_df[\"answer_correctness_diff\"].mean()}\n",
    "Max: {combined_df[\"answer_correctness_diff\"].max()}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_context</th>\n",
       "      <th>answer_context</th>\n",
       "      <th>ground_truth_context</th>\n",
       "      <th>answer_similarity_context</th>\n",
       "      <th>answer_correctness_context</th>\n",
       "      <th>evaluation.llm_model_context</th>\n",
       "      <th>evaluation.embedding_model_context</th>\n",
       "      <th>question_no_context</th>\n",
       "      <th>answer_no_context</th>\n",
       "      <th>ground_truth_no_context</th>\n",
       "      <th>answer_similarity_no_context</th>\n",
       "      <th>answer_correctness_no_context</th>\n",
       "      <th>evaluation.llm_model_no_context</th>\n",
       "      <th>evaluation.embedding_model_no_context</th>\n",
       "      <th>answer_similarity_diff</th>\n",
       "      <th>answer_correctness_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>1st century BC.</td>\n",
       "      <td>1st</td>\n",
       "      <td>0.772291</td>\n",
       "      <td>0.693073</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What century did the name of the Rhine come from?</td>\n",
       "      <td>The name \"Rhine\" comes from a Celtic tribe, th...</td>\n",
       "      <td>1st</td>\n",
       "      <td>0.646525</td>\n",
       "      <td>0.661631</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.125765</td>\n",
       "      <td>0.031441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What type of flower is sought on Midsummer's Eve?</td>\n",
       "      <td>The sought flower is the fern flower.</td>\n",
       "      <td>fern</td>\n",
       "      <td>0.914749</td>\n",
       "      <td>0.728687</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What type of flower is sought on Midsummer's Eve?</td>\n",
       "      <td>Lily or wildflowers, often associated with Sco...</td>\n",
       "      <td>fern</td>\n",
       "      <td>0.725134</td>\n",
       "      <td>0.181284</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.189615</td>\n",
       "      <td>0.547404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of organization would need large qua...</td>\n",
       "      <td>Steel and aerospace industries.</td>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.688686</td>\n",
       "      <td>0.672172</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What type of organization would need large qua...</td>\n",
       "      <td>A hospital or steel manufacturing facility.</td>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.814059</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>-0.167551</td>\n",
       "      <td>-0.141888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What type of radar was used to classify trees ...</td>\n",
       "      <td>Synthetic Aperture Radar (SAR).</td>\n",
       "      <td>Synthetic aperture</td>\n",
       "      <td>0.817274</td>\n",
       "      <td>0.632890</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What type of radar was used to classify trees ...</td>\n",
       "      <td>Pulsed LIDAR (Light Detection and Ranging) tec...</td>\n",
       "      <td>Synthetic aperture</td>\n",
       "      <td>0.790144</td>\n",
       "      <td>0.626108</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.027130</td>\n",
       "      <td>0.006782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is missing a theory on quantum gravity?</td>\n",
       "      <td>A theory of quantum gravity is still missing.</td>\n",
       "      <td>General relativity</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.632376</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What is missing a theory on quantum gravity?</td>\n",
       "      <td>A complete, consistent, and experimentally ver...</td>\n",
       "      <td>General relativity</td>\n",
       "      <td>0.721640</td>\n",
       "      <td>0.555410</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.093577</td>\n",
       "      <td>0.076966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question_context  \\\n",
       "0  What century did the name of the Rhine come from?   \n",
       "1  What type of flower is sought on Midsummer's Eve?   \n",
       "2  What type of organization would need large qua...   \n",
       "3  What type of radar was used to classify trees ...   \n",
       "4       What is missing a theory on quantum gravity?   \n",
       "\n",
       "                                  answer_context ground_truth_context  \\\n",
       "0                                1st century BC.                  1st   \n",
       "1          The sought flower is the fern flower.                 fern   \n",
       "2                Steel and aerospace industries.            hospitals   \n",
       "3                Synthetic Aperture Radar (SAR).   Synthetic aperture   \n",
       "4  A theory of quantum gravity is still missing.   General relativity   \n",
       "\n",
       "   answer_similarity_context  answer_correctness_context  \\\n",
       "0                   0.772291                    0.693073   \n",
       "1                   0.914749                    0.728687   \n",
       "2                   0.688686                    0.672172   \n",
       "3                   0.817274                    0.632890   \n",
       "4                   0.815217                    0.632376   \n",
       "\n",
       "  evaluation.llm_model_context evaluation.embedding_model_context  \\\n",
       "0                       ollama                             ollama   \n",
       "1                       ollama                             ollama   \n",
       "2                       ollama                             ollama   \n",
       "3                       ollama                             ollama   \n",
       "4                       ollama                             ollama   \n",
       "\n",
       "                                 question_no_context  \\\n",
       "0  What century did the name of the Rhine come from?   \n",
       "1  What type of flower is sought on Midsummer's Eve?   \n",
       "2  What type of organization would need large qua...   \n",
       "3  What type of radar was used to classify trees ...   \n",
       "4       What is missing a theory on quantum gravity?   \n",
       "\n",
       "                                   answer_no_context ground_truth_no_context  \\\n",
       "0  The name \"Rhine\" comes from a Celtic tribe, th...                     1st   \n",
       "1  Lily or wildflowers, often associated with Sco...                    fern   \n",
       "2        A hospital or steel manufacturing facility.               hospitals   \n",
       "3  Pulsed LIDAR (Light Detection and Ranging) tec...      Synthetic aperture   \n",
       "4  A complete, consistent, and experimentally ver...      General relativity   \n",
       "\n",
       "   answer_similarity_no_context  answer_correctness_no_context  \\\n",
       "0                      0.646525                       0.661631   \n",
       "1                      0.725134                       0.181284   \n",
       "2                      0.856237                       0.814059   \n",
       "3                      0.790144                       0.626108   \n",
       "4                      0.721640                       0.555410   \n",
       "\n",
       "  evaluation.llm_model_no_context evaluation.embedding_model_no_context  \\\n",
       "0                          ollama                                ollama   \n",
       "1                          ollama                                ollama   \n",
       "2                          ollama                                ollama   \n",
       "3                          ollama                                ollama   \n",
       "4                          ollama                                ollama   \n",
       "\n",
       "   answer_similarity_diff  answer_correctness_diff  \n",
       "0                0.125765                 0.031441  \n",
       "1                0.189615                 0.547404  \n",
       "2               -0.167551                -0.141888  \n",
       "3                0.027130                 0.006782  \n",
       "4                0.093577                 0.076966  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "combined_df.to_csv(f\"{experiment_folder}/combined_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deh_measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
