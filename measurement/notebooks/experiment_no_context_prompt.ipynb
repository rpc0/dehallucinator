{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:  Impact of context on RAG performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**\n",
    "To establish a base-line understanding and validate the thesis that context is a key element of hallucination reduction we can measure the change in RAGAS metrics for LLM that uses context or not.\n",
    "\n",
    "**Test Approach**\n",
    "Ask a question to LLM with and without a context.  We expect that RAGAS measures with a context should be significantly better than without.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckhill/miniforge3/envs/deh_measure/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Common import\n",
    "from deh.assessment import QASetRetriever\n",
    "from deh.assessment import QASetType\n",
    "from deh import settings\n",
    "from deh.eval import generate_experiment_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples:int = 5\n",
    "experiment_folder:str = \"../../data/evaluation/no-context-prompt-experiment/\"\n",
    "qa_data_set_file:str = \"../../data/qas/squad_qas.tsv\"\n",
    "\n",
    "# Create experiment folder:\n",
    "if not os.path.exists(experiment_folder):\n",
    "    Path(experiment_folder).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 questions sampled from QA corpus (../../data/qas/squad_qas.tsv)\n"
     ]
    }
   ],
   "source": [
    "# Only get impossible to answer questions:\n",
    "qa_set = QASetRetriever.get_qasets(\n",
    "    file_path = qa_data_set_file,\n",
    "    sample_size= num_samples,\n",
    "    qa_type = QASetType.POSSIBLE_ONLY\n",
    ")\n",
    "\n",
    "print(f\"{len(qa_set)} questions sampled from QA corpus ({qa_data_set_file})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Responses with default prompt (using context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5 question/answer pairs.\n",
      "Processing 2 of 5 question/answer pairs.\n",
      "Processing 3 of 5 question/answer pairs.\n",
      "Processing 4 of 5 question/answer pairs.\n",
      "Processing 5 of 5 question/answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response.question</th>\n",
       "      <th>response.hyde</th>\n",
       "      <th>response.answer</th>\n",
       "      <th>response.context</th>\n",
       "      <th>response.evaluation.grade</th>\n",
       "      <th>response.evaluation.description</th>\n",
       "      <th>response.execution_time</th>\n",
       "      <th>system_settings.gpu_enabled</th>\n",
       "      <th>system_settings.llm_model</th>\n",
       "      <th>system_settings.llm_prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>system_settings.text_chunk_size</th>\n",
       "      <th>system_settings.text_chunk_overlap</th>\n",
       "      <th>system_settings.context_similarity_threshold</th>\n",
       "      <th>system_settings.context_docs_retrieved</th>\n",
       "      <th>system_settings.docs_loaded</th>\n",
       "      <th>reference.question</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>reference.is_impossible</th>\n",
       "      <th>reference.ref_context_id</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>False</td>\n",
       "      <td>The colonization of India occurred in the mid-...</td>\n",
       "      <td>[{'id': None, 'metadata': {'source': '../data/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>00:00:04</td>\n",
       "      <td>True</td>\n",
       "      <td>llama3.1:8b-instruct-q3_K_L</td>\n",
       "      <td>rlm/rag-prompt-llama</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>18th century</td>\n",
       "      <td>False</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           response.question  response.hyde  \\\n",
       "0  When did the colonization of India occur?          False   \n",
       "\n",
       "                                     response.answer  \\\n",
       "0  The colonization of India occurred in the mid-...   \n",
       "\n",
       "                                    response.context  \\\n",
       "0  [{'id': None, 'metadata': {'source': '../data/...   \n",
       "\n",
       "  response.evaluation.grade response.evaluation.description  \\\n",
       "0                                                             \n",
       "\n",
       "  response.execution_time  system_settings.gpu_enabled  \\\n",
       "0                00:00:04                         True   \n",
       "\n",
       "     system_settings.llm_model system_settings.llm_prompt  ...  \\\n",
       "0  llama3.1:8b-instruct-q3_K_L       rlm/rag-prompt-llama  ...   \n",
       "\n",
       "  system_settings.text_chunk_size system_settings.text_chunk_overlap  \\\n",
       "0                            1500                                100   \n",
       "\n",
       "   system_settings.context_similarity_threshold  \\\n",
       "0                                           1.0   \n",
       "\n",
       "   system_settings.context_docs_retrieved  system_settings.docs_loaded  \\\n",
       "0                                       6                         1256   \n",
       "\n",
       "                          reference.question  reference.ground_truth  \\\n",
       "0  When did the colonization of India occur?            18th century   \n",
       "\n",
       "  reference.is_impossible reference.ref_context_id  reference_id  \n",
       "0                   False                      235             1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert(response) -> pd.DataFrame:\n",
    "    \"\"\"Converts retrieved JSON response to Pandas DataFrame\"\"\"\n",
    "    response_df = pd.json_normalize(\n",
    "        data=response\n",
    "    )\n",
    "\n",
    "    response_df[\"reference.ground_truth\"] = response[\"reference\"][\"ground_truth\"]\n",
    "    response_df[\"reference.is_impossible\"] = response[\"reference\"][\"is_impossible\"]\n",
    "\n",
    "    return response_df\n",
    "\n",
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for answer.\n",
    "    parameters:\n",
    "    - hyde (h) = False\n",
    "    - evaluation (e) = False\n",
    "    - lmm prompt selection (lp) = 1\n",
    "    \"\"\"\n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return f\"http://{settings.API_ANSWER_ENDPOINT}/answer?{query_params}&h=False&e=False&lp=1\"\n",
    "\n",
    "# Collect response:\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store dataframe:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/prompt_1.pkl\" )\n",
    "exp_df[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Responses without context provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5 question/answer pairs.\n",
      "Processing 2 of 5 question/answer pairs.\n",
      "Processing 3 of 5 question/answer pairs.\n",
      "Processing 4 of 5 question/answer pairs.\n",
      "Processing 5 of 5 question/answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response.question</th>\n",
       "      <th>response.hyde</th>\n",
       "      <th>response.answer</th>\n",
       "      <th>response.context</th>\n",
       "      <th>response.evaluation.grade</th>\n",
       "      <th>response.evaluation.description</th>\n",
       "      <th>response.execution_time</th>\n",
       "      <th>system_settings.gpu_enabled</th>\n",
       "      <th>system_settings.llm_model</th>\n",
       "      <th>system_settings.llm_prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>system_settings.text_chunk_size</th>\n",
       "      <th>system_settings.text_chunk_overlap</th>\n",
       "      <th>system_settings.context_similarity_threshold</th>\n",
       "      <th>system_settings.context_docs_retrieved</th>\n",
       "      <th>system_settings.docs_loaded</th>\n",
       "      <th>reference.question</th>\n",
       "      <th>reference.ground_truth</th>\n",
       "      <th>reference.is_impossible</th>\n",
       "      <th>reference.ref_context_id</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>False</td>\n",
       "      <td>The British East India Company established its...</td>\n",
       "      <td>[{'id': None, 'metadata': {'source': '../data/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>00:00:03</td>\n",
       "      <td>True</td>\n",
       "      <td>llama3.1:8b-instruct-q3_K_L</td>\n",
       "      <td>rlm/rag-prompt-llama</td>\n",
       "      <td>...</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>18th century</td>\n",
       "      <td>False</td>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           response.question  response.hyde  \\\n",
       "0  When did the colonization of India occur?          False   \n",
       "\n",
       "                                     response.answer  \\\n",
       "0  The British East India Company established its...   \n",
       "\n",
       "                                    response.context  \\\n",
       "0  [{'id': None, 'metadata': {'source': '../data/...   \n",
       "\n",
       "  response.evaluation.grade response.evaluation.description  \\\n",
       "0                                                             \n",
       "\n",
       "  response.execution_time  system_settings.gpu_enabled  \\\n",
       "0                00:00:03                         True   \n",
       "\n",
       "     system_settings.llm_model system_settings.llm_prompt  ...  \\\n",
       "0  llama3.1:8b-instruct-q3_K_L       rlm/rag-prompt-llama  ...   \n",
       "\n",
       "  system_settings.text_chunk_size system_settings.text_chunk_overlap  \\\n",
       "0                            1500                                100   \n",
       "\n",
       "   system_settings.context_similarity_threshold  \\\n",
       "0                                           1.0   \n",
       "\n",
       "   system_settings.context_docs_retrieved  system_settings.docs_loaded  \\\n",
       "0                                       6                         1256   \n",
       "\n",
       "                          reference.question  reference.ground_truth  \\\n",
       "0  When did the colonization of India occur?            18th century   \n",
       "\n",
       "  reference.is_impossible reference.ref_context_id  reference_id  \n",
       "0                   False                      235             1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for answer.\n",
    "    parameters:\n",
    "    - hyde (h) = False\n",
    "    - evaluation (e) = False\n",
    "    - lmm prompt selection (lp) = 2\n",
    "    \"\"\"\n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return f\"http://{settings.API_ANSWER_ENDPOINT}/answer?{query_params}&h=False&e=False&lp=2\"\n",
    "\n",
    "# Collect response:\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store dataframe:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/prompt_2.pkl\" )\n",
    "exp_df[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Measures for Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Either local (Ollama) or remote (OpenAI) evaluation models can be used:\n",
    "\n",
    "embedding = OllamaEmbeddings(\n",
    "    base_url=settings.OLLAMA_URL,\n",
    "    model=settings.ASSESSMENT_EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=settings.OLLAMA_URL,\n",
    "    model=settings.ASSESSMENT_LLM_MODEL,\n",
    ")\n",
    "\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "openai_embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "import ragas.metrics as metrics\n",
    "from ragas.run_config import RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = [\n",
    "    (\"prompt_1.pkl\",\"prompt_1_eval.pkl\"),\n",
    "    (\"prompt_2.pkl\", \"prompt_2_eval.pkl\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:40<00:00, 10.03s/it]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:43<00:00, 10.38s/it]\n"
     ]
    }
   ],
   "source": [
    "for input_file, output_file in comp_data:\n",
    "\n",
    "    # Load Data and format\n",
    "    responses_df = pd.read_pickle(f\"{experiment_folder}/{input_file}\")\n",
    "    responses_df = responses_df.rename(columns={\n",
    "        \"response.question\" : \"question\",\n",
    "        \"response.answer\" : \"answer\",\n",
    "        \"reference.ground_truth\" : \"ground_truth\"\n",
    "    })[[\"question\", \"answer\", \"ground_truth\"]]\n",
    "\n",
    "    \n",
    "    # Convert to Dataset\n",
    "    responses_ds = Dataset.from_pandas( responses_df)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluation_ds = evaluate(\n",
    "        dataset = responses_ds,\n",
    "        metrics = [metrics.answer_similarity, metrics.answer_correctness],\n",
    "        embeddings = embedding,\n",
    "        llm = llm,\n",
    "        run_config=RunConfig(\n",
    "            max_workers=5\n",
    "        ),\n",
    "        raise_exceptions=False\n",
    "    )\n",
    "\n",
    "    eval_df = evaluation_ds.to_pandas()\n",
    "\n",
    "    # Evaluation metadata\n",
    "    eval_df[\"evaluation.llm_model\"] = \"ollama\"\n",
    "    eval_df[\"evaluation.embedding_model\"] = \"ollama\"\n",
    "\n",
    "    # Persist\n",
    "    eval_df.to_pickle( f\"{experiment_folder}/{output_file}\" )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>evaluation.llm_model</th>\n",
       "      <th>evaluation.embedding_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>The British East India Company established its...</td>\n",
       "      <td>18th century</td>\n",
       "      <td>0.768604</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What were requests made to British?</td>\n",
       "      <td>The British received several requests, includi...</td>\n",
       "      <td>continue worshiping in their Roman Catholic tr...</td>\n",
       "      <td>0.642737</td>\n",
       "      <td>0.433412</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question  \\\n",
       "0  When did the colonization of India occur?   \n",
       "1        What were requests made to British?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The British East India Company established its...   \n",
       "1  The British received several requests, includi...   \n",
       "\n",
       "                                        ground_truth  answer_similarity  \\\n",
       "0                                       18th century           0.768604   \n",
       "1  continue worshiping in their Roman Catholic tr...           0.642737   \n",
       "\n",
       "   answer_correctness evaluation.llm_model evaluation.embedding_model  \n",
       "0            0.567151               ollama                     ollama  \n",
       "1            0.433412               ollama                     ollama  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and merge Experiment Datasets for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment results:\n",
    "context_retr_df = pd.read_pickle(f\"{experiment_folder}/prompt_1_eval.pkl\")\n",
    "no_context_retr_df = pd.read_pickle(f\"{experiment_folder}/prompt_2_eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_context</th>\n",
       "      <th>answer_context</th>\n",
       "      <th>ground_truth_context</th>\n",
       "      <th>answer_similarity_context</th>\n",
       "      <th>answer_correctness_context</th>\n",
       "      <th>evaluation.llm_model_context</th>\n",
       "      <th>evaluation.embedding_model_context</th>\n",
       "      <th>question_no_context</th>\n",
       "      <th>answer_no_context</th>\n",
       "      <th>ground_truth_no_context</th>\n",
       "      <th>answer_similarity_no_context</th>\n",
       "      <th>answer_correctness_no_context</th>\n",
       "      <th>evaluation.llm_model_no_context</th>\n",
       "      <th>evaluation.embedding_model_no_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>The colonization of India occurred in the mid-...</td>\n",
       "      <td>18th century</td>\n",
       "      <td>0.807099</td>\n",
       "      <td>0.576775</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>The British East India Company established its...</td>\n",
       "      <td>18th century</td>\n",
       "      <td>0.768604</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What were requests made to British?</td>\n",
       "      <td>Requests were made to the British for medical ...</td>\n",
       "      <td>continue worshiping in their Roman Catholic tr...</td>\n",
       "      <td>0.651037</td>\n",
       "      <td>0.435486</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What were requests made to British?</td>\n",
       "      <td>The British received several requests, includi...</td>\n",
       "      <td>continue worshiping in their Roman Catholic tr...</td>\n",
       "      <td>0.642737</td>\n",
       "      <td>0.433412</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_context  \\\n",
       "0  When did the colonization of India occur?   \n",
       "1        What were requests made to British?   \n",
       "\n",
       "                                      answer_context  \\\n",
       "0  The colonization of India occurred in the mid-...   \n",
       "1  Requests were made to the British for medical ...   \n",
       "\n",
       "                                ground_truth_context  \\\n",
       "0                                       18th century   \n",
       "1  continue worshiping in their Roman Catholic tr...   \n",
       "\n",
       "   answer_similarity_context  answer_correctness_context  \\\n",
       "0                   0.807099                    0.576775   \n",
       "1                   0.651037                    0.435486   \n",
       "\n",
       "  evaluation.llm_model_context evaluation.embedding_model_context  \\\n",
       "0                       ollama                             ollama   \n",
       "1                       ollama                             ollama   \n",
       "\n",
       "                         question_no_context  \\\n",
       "0  When did the colonization of India occur?   \n",
       "1        What were requests made to British?   \n",
       "\n",
       "                                   answer_no_context  \\\n",
       "0  The British East India Company established its...   \n",
       "1  The British received several requests, includi...   \n",
       "\n",
       "                             ground_truth_no_context  \\\n",
       "0                                       18th century   \n",
       "1  continue worshiping in their Roman Catholic tr...   \n",
       "\n",
       "   answer_similarity_no_context  answer_correctness_no_context  \\\n",
       "0                      0.768604                       0.567151   \n",
       "1                      0.642737                       0.433412   \n",
       "\n",
       "  evaluation.llm_model_no_context evaluation.embedding_model_no_context  \n",
       "0                          ollama                                ollama  \n",
       "1                          ollama                                ollama  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate datasets together for comparison:\n",
    "combined_df = pd.merge( context_retr_df, no_context_retr_df, left_index=True, right_index=True, suffixes=[\"_context\", \"_no_context\"])\n",
    "combined_df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAGAS comparison (context - no_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_similarity_diff</th>\n",
       "      <th>answer_correctness_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.002075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_similarity_diff  answer_correctness_diff\n",
       "0                0.038495                 0.009624\n",
       "1                0.008300                 0.002075"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric comparison:\n",
    "combined_df[\"answer_similarity_diff\"] = combined_df[\"answer_similarity_context\"] - combined_df[\"answer_similarity_no_context\"]\n",
    "combined_df[\"answer_correctness_diff\"] = combined_df[\"answer_correctness_context\"] - combined_df[\"answer_correctness_no_context\"]\n",
    "\n",
    "\n",
    "combined_df[[\"answer_similarity_diff\", \"answer_correctness_diff\"]][0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer Similiarity Difference:\n",
      "Min: 0.00829950381471245\n",
      "Avg: 0.028568275375045738\n",
      "Max: 0.03859078489428036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Answer Similiarity:\n",
    "print(f\"\"\"\n",
    "Answer Similiarity Difference:\n",
    "Min: {combined_df[\"answer_similarity_diff\"].min()}\n",
    "Avg: {combined_df[\"answer_similarity_diff\"].mean()}\n",
    "Max: {combined_df[\"answer_similarity_diff\"].max()}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer Correctness Difference:\n",
      "Min: -0.03374665791811937\n",
      "Avg: -0.001191264489571886\n",
      "Max: 0.00964769622357009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Answer Correctness:\n",
    "print(f\"\"\"\n",
    "Answer Correctness Difference:\n",
    "Min: {combined_df[\"answer_correctness_diff\"].min()}\n",
    "Avg: {combined_df[\"answer_correctness_diff\"].mean()}\n",
    "Max: {combined_df[\"answer_correctness_diff\"].max()}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_context</th>\n",
       "      <th>answer_context</th>\n",
       "      <th>ground_truth_context</th>\n",
       "      <th>answer_similarity_context</th>\n",
       "      <th>answer_correctness_context</th>\n",
       "      <th>evaluation.llm_model_context</th>\n",
       "      <th>evaluation.embedding_model_context</th>\n",
       "      <th>question_no_context</th>\n",
       "      <th>answer_no_context</th>\n",
       "      <th>ground_truth_no_context</th>\n",
       "      <th>answer_similarity_no_context</th>\n",
       "      <th>answer_correctness_no_context</th>\n",
       "      <th>evaluation.llm_model_no_context</th>\n",
       "      <th>evaluation.embedding_model_no_context</th>\n",
       "      <th>answer_similarity_diff</th>\n",
       "      <th>answer_correctness_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>The colonization of India occurred in the mid-...</td>\n",
       "      <td>18th century</td>\n",
       "      <td>0.807099</td>\n",
       "      <td>0.576775</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>When did the colonization of India occur?</td>\n",
       "      <td>The British East India Company established its...</td>\n",
       "      <td>18th century</td>\n",
       "      <td>0.768604</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What were requests made to British?</td>\n",
       "      <td>Requests were made to the British for medical ...</td>\n",
       "      <td>continue worshiping in their Roman Catholic tr...</td>\n",
       "      <td>0.651037</td>\n",
       "      <td>0.435486</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What were requests made to British?</td>\n",
       "      <td>The British received several requests, includi...</td>\n",
       "      <td>continue worshiping in their Roman Catholic tr...</td>\n",
       "      <td>0.642737</td>\n",
       "      <td>0.433412</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.002075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many inhabitants did Betty Meggers believe...</td>\n",
       "      <td>Betty Meggers believed that a population densi...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.707551</td>\n",
       "      <td>0.551888</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>How many inhabitants did Betty Meggers believe...</td>\n",
       "      <td>Betty Meggers, a renowned archaeologist, estim...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.681775</td>\n",
       "      <td>0.545444</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.025776</td>\n",
       "      <td>0.006444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What scientific field's theory has received co...</td>\n",
       "      <td>Thermodynamics is the scientific field whose t...</td>\n",
       "      <td>thermodynamic</td>\n",
       "      <td>0.848720</td>\n",
       "      <td>0.545513</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>What scientific field's theory has received co...</td>\n",
       "      <td>The scientific field that has received contrib...</td>\n",
       "      <td>thermodynamic</td>\n",
       "      <td>0.817040</td>\n",
       "      <td>0.579260</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.031680</td>\n",
       "      <td>-0.033747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the final legislative proposals for a...</td>\n",
       "      <td>The final legislative proposals for a Scottish...</td>\n",
       "      <td>1978</td>\n",
       "      <td>0.727553</td>\n",
       "      <td>0.556888</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>When was the final legislative proposals for a...</td>\n",
       "      <td>The Final Legislative Proposals for a Scottish...</td>\n",
       "      <td>1978</td>\n",
       "      <td>0.688963</td>\n",
       "      <td>0.547241</td>\n",
       "      <td>ollama</td>\n",
       "      <td>ollama</td>\n",
       "      <td>0.038591</td>\n",
       "      <td>0.009648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question_context  \\\n",
       "0          When did the colonization of India occur?   \n",
       "1                What were requests made to British?   \n",
       "2  How many inhabitants did Betty Meggers believe...   \n",
       "3  What scientific field's theory has received co...   \n",
       "4  When was the final legislative proposals for a...   \n",
       "\n",
       "                                      answer_context  \\\n",
       "0  The colonization of India occurred in the mid-...   \n",
       "1  Requests were made to the British for medical ...   \n",
       "2  Betty Meggers believed that a population densi...   \n",
       "3  Thermodynamics is the scientific field whose t...   \n",
       "4  The final legislative proposals for a Scottish...   \n",
       "\n",
       "                                ground_truth_context  \\\n",
       "0                                       18th century   \n",
       "1  continue worshiping in their Roman Catholic tr...   \n",
       "2                                                0.2   \n",
       "3                                      thermodynamic   \n",
       "4                                               1978   \n",
       "\n",
       "   answer_similarity_context  answer_correctness_context  \\\n",
       "0                   0.807099                    0.576775   \n",
       "1                   0.651037                    0.435486   \n",
       "2                   0.707551                    0.551888   \n",
       "3                   0.848720                    0.545513   \n",
       "4                   0.727553                    0.556888   \n",
       "\n",
       "  evaluation.llm_model_context evaluation.embedding_model_context  \\\n",
       "0                       ollama                             ollama   \n",
       "1                       ollama                             ollama   \n",
       "2                       ollama                             ollama   \n",
       "3                       ollama                             ollama   \n",
       "4                       ollama                             ollama   \n",
       "\n",
       "                                 question_no_context  \\\n",
       "0          When did the colonization of India occur?   \n",
       "1                What were requests made to British?   \n",
       "2  How many inhabitants did Betty Meggers believe...   \n",
       "3  What scientific field's theory has received co...   \n",
       "4  When was the final legislative proposals for a...   \n",
       "\n",
       "                                   answer_no_context  \\\n",
       "0  The British East India Company established its...   \n",
       "1  The British received several requests, includi...   \n",
       "2  Betty Meggers, a renowned archaeologist, estim...   \n",
       "3  The scientific field that has received contrib...   \n",
       "4  The Final Legislative Proposals for a Scottish...   \n",
       "\n",
       "                             ground_truth_no_context  \\\n",
       "0                                       18th century   \n",
       "1  continue worshiping in their Roman Catholic tr...   \n",
       "2                                                0.2   \n",
       "3                                      thermodynamic   \n",
       "4                                               1978   \n",
       "\n",
       "   answer_similarity_no_context  answer_correctness_no_context  \\\n",
       "0                      0.768604                       0.567151   \n",
       "1                      0.642737                       0.433412   \n",
       "2                      0.681775                       0.545444   \n",
       "3                      0.817040                       0.579260   \n",
       "4                      0.688963                       0.547241   \n",
       "\n",
       "  evaluation.llm_model_no_context evaluation.embedding_model_no_context  \\\n",
       "0                          ollama                                ollama   \n",
       "1                          ollama                                ollama   \n",
       "2                          ollama                                ollama   \n",
       "3                          ollama                                ollama   \n",
       "4                          ollama                                ollama   \n",
       "\n",
       "   answer_similarity_diff  answer_correctness_diff  \n",
       "0                0.038495                 0.009624  \n",
       "1                0.008300                 0.002075  \n",
       "2                0.025776                 0.006444  \n",
       "3                0.031680                -0.033747  \n",
       "4                0.038591                 0.009648  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "combined_df.to_csv(f\"{experiment_folder}/combined_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deh_measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
