{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:  Feature Impact on Response Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of system response time for different permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deh_measurement/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Common import\n",
    "from deh.assessment import QASetRetriever\n",
    "from deh import settings\n",
    "from deh.eval import generate_experiment_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples:int = 10\n",
    "experiment_folder:str = \"../../data/evaluation/speed-experiment/\"\n",
    "qa_data_set_file:str = \"../../data/qas/squad_qas.tsv\"\n",
    "\n",
    "# Create experiment folder:\n",
    "if not os.path.exists(experiment_folder):\n",
    "    Path(experiment_folder).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 questions sampled from QA corpus (../../data/qas/squad_qas.tsv)\n"
     ]
    }
   ],
   "source": [
    "qa_set = QASetRetriever.get_qasets(\n",
    "    file_path = qa_data_set_file,\n",
    "    sample_size= num_samples\n",
    ")\n",
    "\n",
    "print(f\"{len(qa_set)} questions sampled from QA corpus ({qa_data_set_file})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Similiarity Scores based on original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 10 question/answer pairs.\n",
      "Error processing 1 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 2 of 10 question/answer pairs.\n",
      "Error processing 2 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 3 of 10 question/answer pairs.\n",
      "Error processing 3 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 4 of 10 question/answer pairs.\n",
      "Error processing 4 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 5 of 10 question/answer pairs.\n",
      "Error processing 5 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 6 of 10 question/answer pairs.\n",
      "Error processing 6 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 7 of 10 question/answer pairs.\n",
      "Error processing 7 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 8 of 10 question/answer pairs.\n",
      "Error processing 8 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 9 of 10 question/answer pairs.\n",
      "Error processing 9 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing 10 of 10 question/answer pairs.\n",
      "Error processing 10 of 10\n",
      "Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m exp_df \u001b[38;5;241m=\u001b[39m generate_experiment_dataset(qa_set, convert, api_endpoint)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Store dataframe:\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mexp_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pickle\u001b[49m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/no_rag.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m     26\u001b[0m exp_df[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_pickle'"
     ]
    }
   ],
   "source": [
    "def convert(response) -> pd.DataFrame:\n",
    "    \"\"\"Converts retrieved JSON response to Pandas DataFrame\"\"\"\n",
    "    return pd.json_normalize(\n",
    "        data=response[\"response\"],\n",
    "        meta=[\"execution_time\"],\n",
    "    )\n",
    "\n",
    "def api_endpoint(**kwargs) -> str:\n",
    "    \"\"\"Endpoint for context retrieval w/ hyde (h=true,p=0).\"\"\"\n",
    "\n",
    "    kwargs[\"h\"] = True\n",
    "    kwargs[\"e\"] = True\n",
    "    kwargs[\"lp\"] = 0\n",
    "    kwargs[\"k\"] = 20\n",
    "\n",
    "    query_params = \"&\".join([f\"{key}={kwargs[key]}\" for key in kwargs])\n",
    "    return (\n",
    "        f\"http://{settings.API_ANSWER_ENDPOINT}/answer?{query_params}\"\n",
    "    )\n",
    "\n",
    "# Collect response:\n",
    "exp_df = generate_experiment_dataset(qa_set, convert, api_endpoint)\n",
    "\n",
    "# Store dataframe:\n",
    "exp_df.to_pickle( f\"{experiment_folder}/no_rag.pkl\" )\n",
    "exp_df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_df[\"response_diff\"] = pd.to_timedelta(exp_df[\"execution_time\"]).dt.total_seconds()\n",
    "exp_df[\"response_diff\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deh_measurement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
